<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Non-Autoregressive Text Generation with Pre-trained Language Models | Sylvanas Forever</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="NAT" />
  
  
  
  
  <meta name="description" content="AG 模型因其链式条件概率建模而饱受高时延之痛；NAG 模型又因其强条件独立性假设而深陷低性能的泥沼。因此，作者提出使用预训练 BERT 模型来作为 NAG 模型的骨干从而提升其性能，并设计了一种简单却不失优雅的解码机制来缓解 NAG 模型的两个通病。为了进一步提升 NAG 模型在推理速度上的优势，作者额外引入了 $ratio$-$first$ 解码策略来加快模型解码速度。在三个文本生成任务($t">
<meta property="og:type" content="article">
<meta property="og:title" content="Non-Autoregressive Text Generation with Pre-trained Language Models">
<meta property="og:url" content="http://example.com/2021/12/10/Non-Autoregressive-Text-Generation-with-Pre-trained-Language-Models/index.html">
<meta property="og:site_name" content="Sylvanas Forever">
<meta property="og:description" content="AG 模型因其链式条件概率建模而饱受高时延之痛；NAG 模型又因其强条件独立性假设而深陷低性能的泥沼。因此，作者提出使用预训练 BERT 模型来作为 NAG 模型的骨干从而提升其性能，并设计了一种简单却不失优雅的解码机制来缓解 NAG 模型的两个通病。为了进一步提升 NAG 模型在推理速度上的优势，作者额外引入了 $ratio$-$first$ 解码策略来加快模型解码速度。在三个文本生成任务($t">
<meta property="og:locale">
<meta property="og:image" content="https://github.com/Rexbalaeniceps/pICtUReS/blob/main/nag-bert_2.png?raw=true">
<meta property="article:published_time" content="2021-12-10T05:31:31.000Z">
<meta property="article:modified_time" content="2021-12-10T05:33:30.098Z">
<meta property="article:author" content="wbxu">
<meta property="article:tag" content="NAT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/Rexbalaeniceps/pICtUReS/blob/main/nag-bert_2.png?raw=true">
  
    <link rel="alternate" href="/atom.xml" title="Sylvanas Forever" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">

  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Oswald%3A300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  
<link rel="stylesheet" href="/css/style.css">


  
<script src="/js/jquery-3.1.1.min.js"></script>


  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >
  <link rel="stylesheet" href="/css/fashion.css" >
  <link rel="stylesheet" href="/css/glyphs.css" >

<meta name="generator" content="Hexo 5.4.0"></head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  


<header id="allheader" class="site-header" role="banner" 
   >
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" rel="home" >
                <img style="margin-bottom: 10px;"  width="149px" height="200px" alt="Hike News" src=" /Syl.jpg">
              </a>
            
          </h1>
          
          
            <div class="site-description">wbxu&#39;s blog</div>
          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>

            <div class="clearfix sf-menu">
              <ul id="main-nav" class="menu sf-js-enabled sf-arrows"  style="touch-action: pan-y;">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663" linktext="/"> <a class="" href="/">Home</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663" linktext="archives"> <a class="" href="/archives">Archives</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663" linktext="categories"> <a class="" href="/categories">Categories</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663" linktext="tags"> <a class="" href="/tags">Tags</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663" linktext="about"> <a class="" href="/about">About</a> </li>
                    
              </ul>
            </div>
          </nav>

      </div>
  </div>
</header>


  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-Non-Autoregressive-Text-Generation-with-Pre-trained-Language-Models" style="width: 66%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
<div class="article-gallery">
  <div class="article-gallery-photos">
    
      <a class="article-gallery-img fancybox" target="_blank" href="https://github.com/Rexbalaeniceps/pICtUReS/blob/main/nag-bert_2.png?raw=true" rel="gallery_ckzcgbfp2001dccotf8yz97ta noopener">
        <img src="https://github.com/Rexbalaeniceps/pICtUReS/blob/main/nag-bert_2.png?raw=true" itemprop="image">
      </a>
    
  </div>
</div>

    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      Non-Autoregressive Text Generation with Pre-trained Language Models
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2021/12/10/Non-Autoregressive-Text-Generation-with-Pre-trained-Language-Models/" class="article-date">
	  <time datetime="2021-12-10T05:31:31.000Z" itemprop="datePublished">December 10, 2021</time>
	</a>

      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/Non-Autoregressive-Translation/">Non-Autoregressive Translation</a>
 
      
	<span id="busuanzi_container_page_pv">
	  本文总阅读量<span id="busuanzi_value_page_pv"></span>次
	</span>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>AG 模型因其链式条件概率建模而饱受高时延之痛；NAG 模型又因其强条件独立性假设而深陷低性能的泥沼。因此，作者提出<strong>使用预训练 BERT 模型来作为 NAG 模型的骨干从而提升其性能</strong>，并设计了一种简单却不失优雅的<strong>解码机制来缓解 NAG 模型的两个通病</strong>。为了进一步提升 NAG 模型在推理速度上的优势，作者额外<strong>引入了 $ratio$-$first$ 解码策略来加快模型解码速度</strong>。在三个文本生成任务($text\ \ summarization$、$sentence\ \ compression$、$machine\ \ translation$)上进行了实验，结果表明了作者的模型确实优于其他 NAG 模型。</p>
<span id="more"></span>



<h4 id="现存问题"><a href="#现存问题" class="headerlink" title="现存问题"></a>现存问题</h4><ol>
<li><p>NAG 模型<strong>具备高并行化能力因而能够更快地推理，但其生成质量常常落后于对应的 AG 模型</strong>；</p>
</li>
<li><p>之前的 NAG 模型需要<strong>在推理输出序列前就预测好目标序列的长度，这就使得其不得不额外增加一个长度预测模块</strong>；</p>
</li>
<li><p>而长度预测模块所预测到的最佳长度有很可能不是正确的目标长度，这使其又不得不<strong>采取长度束再评分方法，该方法大大削弱了 NAG 模型的快速推理优势</strong>；</p>
</li>
<li><p>现存的 NAG 模型都**需要强条件独立性以支持其并行解码范式，这就会导致其翻译出不符合语法规则的语段(重译问题)**；</p>
</li>
</ol>
<h4 id="NAG-BERT"><a href="#NAG-BERT" class="headerlink" title="NAG-BERT"></a>NAG-BERT</h4><ol>
<li>针对 <strong>NAG 模型性能问题</strong>，作者<strong>引入预训练 BERT 模型来提升模型性能</strong>；</li>
<li>针对<strong>长度预测问题</strong>，作者设计了一个基于模型决策的解码机制来<strong>动态地调整目标长度</strong>；</li>
<li>针对<strong>条件独立性假设</strong>，作者<strong>在损失函数中额外引入一项损失以缓解重译问题</strong>；</li>
<li>针对 <strong>NAG 的低延时优势</strong>，作者设计了一个<strong>解码策略 $ratio$-$first$ 来进一步提升解码速度</strong>。</li>
</ol>
<h5 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h5><p><img src="https://github.com/Rexbalaeniceps/pICtUReS/blob/main/nag-bert_1.png?raw=true" alt="MA"></p>
<p>整个模型就是在 BERT 架构上添加一个 CRF 网络，而其中的 BERT 模型部分都是用预训练的 BERT 参数来进行初始化。</p>
<p>参照 BERT 模型，作者首先将 <cls>、<sep> 这两个特殊标记添加到输入序列中去。然后使用 <pad> 标记来讲输入序列的长度填充到最长，这样可以<strong>保证输入序列的长度一定不小于目标长度</strong>。接下来就是简单的多头自注意力网络和位置级前馈神经网络的堆栈而已(和 BERT 或者说 Transformer encoder 是一样的)。</p>
<p>经过 BERT 编码后的隐藏表示 $H$ 会被喂给一个线性链 CRF 网络，长度为 $T’$ 目标序列 $Y$ 的似然函数如下所示：<br>$$<br>\large P_{CRF} (Y \mid X) = \frac{e^{S (X, Y)}}{\sum_{Y’} e^{S (X, Y’)}} = \frac{1}{Z (X)} exp (\sum_{i=1}^{T’} \Phi_{y_i} (h_i) + \sum_{i=2}^{T’} t(y_{i-1}, y_i))<br>$$</p>
<p>其中，$Z(X)$ 是归一化因子；$\Phi_{y_i} (h_i)$ 代表了 $y_i$ 在位置 $i$ 上的得分；神经网络参数 $\Phi$ 会将 BERT 输出的隐藏表示映射到目标词表空间中去；$t(y_{i-1}, y_i)$ 表示从标签 $y_{i-1}$ 转移到标签 $y_i$ 的转移得分；$T$ 就是转移矩阵</p>
<p>又因为目标词表很大(32k)，所以直接建模转移矩阵 $T$ 和归一化因子 $Z(X)$ 是不切实际的，故作者选择采用 Sun 等人的方法：</p>
<ul>
<li>对于 $T$ 矩阵，通过分解的方法，将 $T$ 分解为 $E_1$ 和 $E_2$ 两个较小的矩阵，其中 $T = E_1 × E_2^T$；</li>
<li>对于归一化因子 $Z(X)$，与其搜索所有可能的路径，倒不如使用预先定义的搜索束进行剪枝；</li>
</ul>
<h5 id="Length-Prediction"><a href="#Length-Prediction" class="headerlink" title="Length Prediction"></a>Length Prediction</h5><p>作者的基本想法就是希望<strong>模型能够通过生成一个特殊标记 (<eos>) 来动态地停止序列生成</strong>。为此，作者将两个连续的 <eos> 标记添加到目标序列的末尾，进而模型能够在两个 <eos> 之间学习到一个确定的转移行为，即 <eos> → <eos>。这是因为<strong>在训练期间，模型压根看不到任何一个从 <eos> 转移回回目标词表的转移行为</strong>。</p>
<p>推理期间，结果 $Y$ 需要能够最大化 CRF 的评分函数，该评分函数可分解为如下形式：<br>$$<br>\large S (X, Y’) = \sum_{i=1}^{T} \Phi_{y_i’} (h_i) + \sum_{i=2}^{T} t(y_{i-1}’, y_i’) = \Phi_{y_1’} (h_1) + \sum_{i=2}^{T} {\Phi_{y_i’} (h_i) + t(y_{i-1}’, y_i’)}<br>$$</p>
<p>一旦解码路径上出现了一个 <eos> 标记，那么接下来的所有转移行为都将在 <eos> 和 <eos> 中进行。简言之，翻译句子的后续子序列仅由 <eos> 构成，并且会被删除。</p>
<h5 id="Ratio-First-Decoding"><a href="#Ratio-First-Decoding" class="headerlink" title="Ratio-First Decoding"></a>Ratio-First Decoding</h5><p>BERT 的输出其实可以分为两段，第一段为第一个 <eos> 之前看的输出标记序列，第二段则为剩余部分。如上图所示，y1 y2 y3 y4 <eos> 为第一段，剩余为第二段。不难发现，第二段对于最终输出结果毫无作用，故而去除之。这足以说明<strong>只考虑 BERT 输出的前半段可以提升模型的解码速度</strong>。因此，对于那些已知目标序列短于源序列的任务，作者只使用 BERT 输出的前 $[\alpha \cdot T]$ 长度的子序列来进行推理。其中，$T$ 是源序列长度；$\alpha$ 是数据统计值；$[\cdot]$ 是整数舍入法。正式的公式如下所示：<br>$$<br>\large \hat{Y} = \underset{Y’}{\operatorname {argmax}} \mathcal{F} (X, Y’, \alpha) =  \underset{Y’}{\operatorname {argmax}} { \sum_{i=1}^{[\alpha \cdot T]} \Phi_{y_i’} (h_i) + \sum_{i=2}^{[\alpha \cdot T]} t(y_{i-1}’, y_i’) }<br>$$</p>
<p>值得注意的是：</p>
<ol>
<li>当 <strong>α = 1.0 时，该解码策略就退化为了标准的解码策略</strong>；</li>
<li>ratio-first 虽然<strong>会减短目标序列的最大长度，但是其实际输出长度仍然由模型动态地决定</strong>；</li>
<li>ratio-first 能够在<strong>保留生成质量的同时显著提高推理速度</strong>；</li>
</ol>
<h5 id="Learning-Objective"><a href="#Learning-Objective" class="headerlink" title="Learning Objective"></a>Learning Objective</h5><p>NAG 因其在输出标记上的条件独立性近似而偏爱重复单词，<strong>引入目标端明确的依赖关系可缓解之</strong>。作者提出在 NAG 的上下文中使用无可能表示(unliklihood formulation)，即：将否定的候选词集定义为在预设的上下文窗口 $c$ 内的邻近标记。具体的公式如下所示：<br>$$<br>\large \mathcal{L}<em>{CA} (Y \mid X) = - \sum</em>{i=1}^{T’} {\log p_{\theta} (y_i \mid h_i; X) + l_{CA} (i)}<br>$$<br>$$<br>\large l_{CA} (i) = \sum_{j = i - c,\ y_j \ne y_i}^{j = i + c} \log (1.0 -  p_{\theta} (y_j \mid h_i; X))<br>$$</p>
<p>其中，$l_{CA} (i)$ 被用来放大位置 $i$ 邻近重复标记的损失值；$\mathcal{L}_{CA}$ 则被用来最小化训练损失。于是，<strong>该策略会打消模型产生邻近的重复标记的积极性</strong>。</p>
<h4 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h4><img src="https://github.com/Rexbalaeniceps/pICtUReS/blob/main/nag-bert_rew.png?raw=true" alt="res" style="zoom: 67%;" />

<p>作者在机器翻译任务上得出的结论有：</p>
<ol>
<li>NAG-BERT 模型在性能和速度上都实现了有效的提升；</li>
<li>ratio-first 解码策略在损失一点性能的同时，将解码速度提高到 13.92 倍。</li>
</ol>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/Non-Autoregressive-Translation/">Non-Autoregressive Translation</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NAT/" rel="tag">NAT</a></li></ul>

            
      
    <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
    <div id="vcomments"></div>
    <script>
        var notify = 'false' == true ? true : false;
        var verify = 'false' == true ? true : false;
        window.onload = function() {
            new Valine({
                el: '#vcomments',
                notify: notify,
                verify: verify,
                app_id: "rRPDoEKWwnOvFa6kDEm3uFe7-gzGzoHsz",
                app_key: "hqI6QzzH8hYVIkKeVsMeWGOi",
                placeholder: "写下你的评论吧~",
                avatar:"retro"
            });
        }
    </script>


      
    </footer>
    <hr class="entry-footer-hr">
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/12/17/How-does-Length-Prediction-Influence-the-Performance-of-Non-Autoregressive-Translation/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          How does Length Prediction Influence the Performance of Non-Autoregressive Translation?
        
      </div>
    </a>
  
  
    <a href="/2021/12/09/ENGINE-Energy-Based-Inference-Networks-for-Non-Autoregressive-Machine-Translation/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
      <ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8E%B0%E5%AD%98%E9%97%AE%E9%A2%98"><span class="nav-number">1.</span> <span class="nav-text">现存问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NAG-BERT"><span class="nav-number">2.</span> <span class="nav-text">NAG-BERT</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Model-Architecture"><span class="nav-number">2.1.</span> <span class="nav-text">Model Architecture</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Length-Prediction"><span class="nav-number">2.2.</span> <span class="nav-text">Length Prediction</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Ratio-First-Decoding"><span class="nav-number">2.3.</span> <span class="nav-text">Ratio-First Decoding</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Learning-Objective"><span class="nav-number">2.4.</span> <span class="nav-text">Learning Objective</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Results"><span class="nav-number">3.</span> <span class="nav-text">Results</span></a></li></ol>
    
    </div>
  </aside>
</section>
        
      </div>

    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2022 Sylvanas Forever All Rights Reserved.
        
            <span id="busuanzi_container_site_pv">
            本站总访问量<span id="busuanzi_value_site_pv"></span>次
            </span>
          
      </div>
      <div class="site-credit">
        Theme by <a href="https://github.com/iTimeTraveler/hexo-theme-hipaper" target="_blank">hipaper</a>
      </div>
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");

    wrapdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";


    <!-- headerblur min height -->
    
    
</script>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
    
<div style="display: none;">
  <script src="https://s11.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
</div>

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


<script src="/js/bootstrap.js"></script>


<script src="/js/main.js"></script>








  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
</body>
</html>
