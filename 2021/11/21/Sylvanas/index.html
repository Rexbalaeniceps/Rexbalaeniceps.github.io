<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta name="theme-color" content="#33474d">
	<title>NON-AUTOREGRESSIVE NEURAL MACHINE TRANSLATION | wbxu&#39;s blog</title>
	<link rel="stylesheet" href="/css/style.css" />
	
      <link rel="alternate" href="/atom.xml" title="wbxu&#39;s blog" type="application/atom+xml">
    
<meta name="generator" content="Hexo 5.4.0"></head>

<body>

	<header class="header">
		<nav class="header__nav">
			
				<a href="/archives" class="header__link">Archive</a>
			
				<a href="/tags" class="header__link">Tags</a>
			
				<a href="/atom.xml" class="header__link">RSS</a>
			
		</nav>
		<h1 class="header__title"><a href="/">wbxu&#39;s blog</a></h1>
		<h2 class="header__subtitle">自然语言处理在读研究生</h2>
	</header>

	<main>
		<article>
	
		<h1>NON-AUTOREGRESSIVE NEURAL MACHINE TRANSLATION</h1>
	
	<div class="article__infos">
		<span class="article__date">2021-11-21</span><br />
		
			<span class="article__category">
				<a class="article-category-link" href="/categories/Natural-Language-Processing/">Natural Language Processing</a>
			</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/NLP-NMT-NAT/" rel="tag">NLP; NMT; NAT</a>
			</span>
		
	</div>

	

	
		<h3 id="mathbf-Abstract"><a href="#mathbf-Abstract" class="headerlink" title="$\mathbf{Abstract}$"></a>$\mathbf{Abstract}$</h3><ol>
<li><p><strong>背景</strong></p>
<ul>
<li>现存的神经机器翻译系统都是以之前预测的偏翻译作为条件来预测当前的输出单词。</li>
</ul>
</li>
<li><p><strong>问题</strong></p>
<ul>
<li>但是，随着目标句子的长度增加，自回归解码器的翻译时延也会随之线性增长。</li>
</ul>
</li>
<li><p><strong>创新</strong></p>
<ul>
<li>NAT模型能够并行地预测每个单词，因而大大减少了模型翻译的推理时延。</li>
<li>作者使用了以下3个训练策略：<ul>
<li>知识蒸馏($\mathfrak{Knowledge\ \ Distillation}$)；</li>
<li>繁殖力($\mathfrak{fertilities}$)；</li>
<li>策略梯度($\mathfrak{policy\ \ gradient}$)微调方法；</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>结论</strong></p>
<ul>
<li>在三个数据集上进行评估：<ul>
<li><a target="_blank" rel="noopener" href="https://wit3.fbk.eu/">IWSLT16 En-De</a>；</li>
<li><a target="_blank" rel="noopener" href="http://www.statmt.org/wmt14/translation-task">WMT14 En-De</a>；</li>
<li><a target="_blank" rel="noopener" href="http://www.statmt.org/wmt16/translation-task">WMT16 En-Ro</a>；</li>
</ul>
</li>
<li>NAT模型性能只比其AT教师模型Transformer下降了2.0个BLEU点。</li>
<li>通过在推理时并行地采样繁殖力，作者的NAT模型在 <a target="_blank" rel="noopener" href="http://www.statmt.org/wmt16/translation-task">WMT16 En-Ro</a> 数据集上取得了接近于SOTA的成绩（29.8 BLEU）。</li>
</ul>
</li>
</ol>
<h3 id="mathbf-Background"><a href="#mathbf-Background" class="headerlink" title="$\mathbf{Background}$"></a>$\mathbf{Background}$</h3><h4 id="1-Autoregressive-Decoding"><a href="#1-Autoregressive-Decoding" class="headerlink" title="1. Autoregressive Decoding"></a>1. Autoregressive Decoding</h4><ol>
<li><strong>链式条件概率</strong></li>
</ol>
<p>给定输入序列 $X = {x_1, x_2, \dots, x_{T’}}$，AT模型会将可能的输出序列 $Y = {y_1, y_2, \dots, y_{T}}$ 上的分布分解为自左向右的线性链式条件概率：<br>$$<br>\large p_{\mathcal AR} (Y|X;\theta) = \prod_{t=1}^{T+1} p(y_t|y_{0:t-1}, x_{1:T’}; \theta)<br>$$<br>其中，两个特殊标记会被添加以标示句子的开始和结束，$y_0 = $&lt;bos&gt;表示句子的开始，$y_{T+1} = $&lt;eos&gt;表示句子的结束。</p>
<ol start="2">
<li><strong>最大似然训练</strong></li>
</ol>
<p>自回归地分解输出分布使得模型可以直接通过在每一解码步上施加交叉熵损失来进行最大似然训练，而该损失直接监督了每个条件概率的预测：<br>$$<br>\large \mathcal L_{ML} = \log p_{\mathcal AR} (Y|X;\theta) = \sum_{t=1}^{T+1} \log p(y_t|y_{0:t-1}, x_{1:T’}; \theta)<br>$$<br>3. <strong>自回归模型训练</strong></p>
<p>在模型训练期间，由于我们已经知道了目标翻译是什么，所以后续的条件概率和交叉熵损失的计算并不是真的依赖于当前解码器所预测的单词。<br>虽然上述方法可以提高模型训练速度，但是在推理时，模型解码必须保持完全线性才能与训练时的解码方式保持一致，因此模型无法脱离自回归的特性。</p>
<ol start="4">
<li><strong>自回归的优点</strong></li>
</ol>
<p>自回归地分解输出分布这种方式不仅与人类的语言生产过程保持一致，而且能够有效地捕捉到真实翻译的分布。</p>
<ol start="5">
<li><strong>自回归模型</strong><ul>
<li>AT模型的优点在于<ul>
<li>在大规模语料上能够取得SOTA成绩；</li>
<li>容易被训练；</li>
<li>束搜索算法能为其提供一个寻找近似最优解的有效局部搜索方法；</li>
</ul>
</li>
<li>AT模型存在的缺陷有<ul>
<li>自左向右地预测输出单词造成了推理的高时延；</li>
<li>束大小会使得束搜索算法遭受收益递减问题，从而限制了搜索的并行性；</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="2-Non-Autoregressive-Decoding"><a href="#2-Non-Autoregressive-Decoding" class="headerlink" title="2. Non-Autoregressive Decoding"></a>2. Non-Autoregressive Decoding</h4><ol>
<li><strong>非自回归构思</strong></li>
</ol>
<p>为了解决这种高时延问题，我们自然而然地就会想去移除掉解码器的自回归属性。于是，我们先假设可以通过一个单独的条件分布 $p_L$ 来建模目标序列的长度 $T$，那么就会有：<br>$$<br>\Large p_{\mathcal NA}(Y|X;\theta) = p_L(T|x_{1:T’};\theta)\cdot\prod_{t=1}^Tp(y_t|x_{1:T’};\theta)<br>$$<br>这样的模型仍然有其明确的似然函数，而且我们也还可以使用各个输出分布上的独立交叉熵损失来训练它，但它已经实现了非自回归($\mathfrak{non-autoregressive}$)。</p>
<ol start="2">
<li><strong>多模态问题</strong></li>
</ol>
<p>由于NAT模型基于完全的条件独立性假设，即每个单词的分布仅依赖于输入序列，而不依赖于翻译句子中的其他单词。于是就会出现多模态($\mathfrak{Multi-Modality}$)问题。那么什么是多模态问题呢，简单来说就是模型将多个可能的翻译杂糅在一起。具体如下例：</p>
<p>假设原句输入为汉语的：“知道了”，<br>那么正确的日语翻译有如下几种：<br>① “わかりました”；<br>② “承知しました”；<br>③ “かしこまりました”；<br>而一个NAT模型则很可能将该输入句子翻译成 “わかしました”。</p>
<p>NAT模型的条件独立性假设($\mathfrak{conditional\ \ independence\ \ assumption}$)阻碍了其正确捕捉到输出翻译上的多模态分布，从而导致了该问题。作者将繁殖力引入Transformer模型以期能够缓解多模态问题。</p>
<h3 id="mathbf-Non-Autoregressive-Translation-NAT"><a href="#mathbf-Non-Autoregressive-Translation-NAT" class="headerlink" title="$\mathbf{Non-Autoregressive Translation (NAT)}$"></a>$\mathbf{Non-Autoregressive Translation (NAT)}$</h3><h4 id="1-总结构"><a href="#1-总结构" class="headerlink" title="1. 总结构"></a>1. 总结构</h4><p>作者是基于Transformer模型架构实现的Non-Autoregressive Translation模型，该NAT模型由编码器堆栈、解码器堆栈、繁殖力预测器、翻译预测器构成。具体如下图所示：<br><figure class="figure"><img src="C:\Users\Sylvanas\Desktop\NAT\NAT.png" alt="Non-Autoregressive Translation"><figcaption class="figure__caption">Non-Autoregressive Translation</figcaption></figure></p>
<h4 id="2-编码器堆栈"><a href="#2-编码器堆栈" class="headerlink" title="2. 编码器堆栈"></a>2. 编码器堆栈</h4><p>由于NAT模型的编码器与Vanilla Transformer模型的编码器所承担的工作完全相同，因此作者选择不对编码器做任何修改，也就是直接使用Vanilla Transformer模型的编码器作为NAT模型的编码器。</p>
<h4 id="3-解码器堆栈"><a href="#3-解码器堆栈" class="headerlink" title="3. 解码器堆栈"></a>3. 解码器堆栈</h4><ol>
<li><strong>解码器输入</strong></li>
</ol>
<p>由于NAT模型的解码器需要一下子预测所有单词，所以解码器必须在解码之前就知道预测句子的长度。除此之外，我们也不能像AT模型那样直接将正确的目标序列输入到解码器中去，所以解码器该接收什么样的输入也是一个问题。如果只把位置编码序列输入到解码器中去呢，作者通过实验证明该方法并不可取。因此，作者提出了复制输入序列的方法来初始化解码器输入：</p>
<ul>
<li><strong>均匀复制法</strong></li>
</ul>
<p>每个解码器输入 $t$ 都是复制自编码器输入序列中的第 $Round(T’t/T)$ 个单词，这相当于以一个恒定的速度自左向右地扫描编码器输入序列。这种方法使得模型的解码处理对于给定的目标序列长度 $T$ 是确定的。</p>
<ul>
<li><strong>基于繁殖力的复制方法</strong></li>
</ul>
<p>输入序列中的每个单词都会有自己对应的繁殖力，而这个繁殖力就是模型需要复制该单词的次数，这相当于以变化的速度自左向右地扫描输入序列。此时，模型的解码处理会基于繁殖力序列 $f = {f<br><em>1, f_2, \dots, f</em>{T’}}$ 而不是整个目标句子的长度 $T$，而且显然有 $T = \sum_{t=1}^{T’} f_i$。</p>
<ol start="2">
<li><strong>无前后因果关系的自注意力网络</strong></li>
</ol>
<p>由于NAT模型摆脱了自回归属性的限制，所以它就不再需要mask掉当前解码单词的后续单词了。因此，作者抛弃了Vanilla Transformer的Masked Multi-head Attention机制，并期望解码器中的每个单词都能注意到其双向上下文信息。但是，作者并没有采用unmasked自注意力网络，而是构造了一种mask每个query位置信息的机制，这是因为该方法实证有效。</p>
<ol start="3">
<li><strong>位置注意力网络</strong></li>
</ol>
<p>这个注意力网络类似于多头自注意力网络，只是它的Query和Key来自于位置编码($\mathfrak{Positional\ \ Encodings}$)，而Value则来自于解码器状态($\mathfrak{Decoder\ \ States}$)。该模块将位置信息直接并入到注意力处理中去，并提供了比单独嵌入层更强的位置信号。作者是基于额外的位置信息能够提升解码器局部重排序的能力这一假设而做出的改进，但该假设在后来实验中并没有得到有效的验证。</p>
<h4 id="4-建模繁殖力"><a href="#4-建模繁殖力" class="headerlink" title="4. 建模繁殖力"></a>4. 建模繁殖力</h4><ol>
<li><strong>繁殖力</strong></li>
</ol>
<p>作者引入了一个隐变量($\mathfrak{latent\ \ variable}$) $\mathcal z$ 来直接建模翻译处理中的不确定性($\mathfrak{nondeterminism}$)：首先，从一个先验分布($\mathfrak{prior\ \ distribution}$)中采样 $\mathcal z$ ；然后，基于隐变量 $\mathcal z$ 来非自回归地生成一个翻译句子。</p>
<p>而这样的一个隐变量 $\mathcal z$ 应该具有以下3个理想的属性：</p>
<ul>
<li>对于给定的输入输出对可以简单地为 $\mathcal z$ 推理出一个值，这样就有益于训练一个端到端的模型；</li>
<li>将 $\mathcal z$ 添加到条件上下文中去应该尽可能多的构成不同输出间跨时间的关联性，这样就能使得每个输出位置上的剩余边际概率($\mathfrak{remaining\ \ marginal\ \ probabilities}$)尽可能地接近于满足条件独立性；</li>
<li>$\mathcal z$ 不应该促成输出翻译的变体从而直接导致 $p(y|x, z)$ 学习起来十分琐碎，因为那是解码器神经网络将要近似的函数；</li>
</ul>
<p>之前作者给出的 Eq.3 是由长度来进行分解的，这只能提供一个非常差的隐变量模型样例，而且该模型只能满足以上的第1和第3条属性。为此，作者提出了使用繁殖力作为替代。输入序列中的每一个单词都会有一个对应的整数，该数字等于目标句子中与这个单词对齐的单词的数量，这种对齐关系则可以使用类似于IBM Model 2那样的硬对齐算法来建模。</p>
<p>对于作者提出的NAT模型而言，当选择基于预测的繁殖力去复制编码器输入时，它自然地引入了一个信息化的隐变量。更准确来说，给定输入序列 $X$，一个目标翻译 $Y$ 的条件概率如下：<br>$$<br>\Large p_{\mathcal NA}(Y|X;\theta) = \sum_{f_1,\dots,f_{T’}\in\mathcal F}\bigg(\prod_{t’=1}^{T’}p_F\big(f_{t’}|x_{1:T’};\theta\big)\cdot\prod_{t=1}^{T}p\big(y_t|x_1{f_1},\dots,x_{T’}{f_{T’}};\theta\big)\bigg)<br>$$<br>其中，$\mathcal F = {f_1, \dots, f_{T’}|\sum_{t’=1}^{T’}f_{t’}=T, f_{t’}\ \in\ \mathbb Z^*}$ 是所有繁殖力序列的集合；$x{f}$ 则代表了标记 $x$ 被重复了 $f$ 次。</p>
<ol start="2">
<li><strong>繁殖力预测</strong></li>
</ol>
<p>通过在编码器上构造一个带有Softmax分类器的单层神经网络，作者实现了独立建模每个位置上的繁殖力 $p_F(f_{t’}|x_{1:T’})$。这种结构的模型能够让繁殖力的数值依赖于整个输入序列的信息和上下文，并使其成为输入单词的一个属性。</p>
<ol start="3">
<li><strong>繁殖力的好处</strong></li>
</ol>
<p>繁殖力拥有作者之前所述非自回归机器翻译隐变量的全部3个理想属性：</p>
<ul>
<li>一个外部对齐器能提供一个简单且快速的近似推理模型将无监督训练问题简化为两个有监督训练问题；</li>
<li>使用繁殖力来作为隐变量能够显著地缓解多模态问题，这是因为它能够提供输出空间上的一个自然分解。对于一个给定的输入序列而言，将输出分布限定在那些与特定繁殖力序列保持一致的目标句子上，这会极大地缩减模式空间($\mathfrak{mode\ \ space}$)。而且，全局的模式选择被分解成了一系列局部的模式选择，即从如何翻译整个输入序列转变成了如何翻译每个输入单词。因为繁殖力提供了一个固定的“脚手架”，这种局部的模式选择可以被有效的监督；</li>
<li>如果将繁殖力和重排序一起并入到隐变量中去的话，这将会提供完全的对齐统计数据，但却使得编码器需要承担所有的复杂建模任务而解码函数则能够轻而易举地近似给定的隐变量。仅使用繁殖力将会让解码器也承担起一部分繁杂的建模任务；</li>
</ul>
<h4 id="5-翻译预测器和解码策略"><a href="#5-翻译预测器和解码策略" class="headerlink" title="5. 翻译预测器和解码策略"></a>5. 翻译预测器和解码策略</h4><p>在推理时，模型可以通过边缘化所有可能的隐变量序列来识别最高条件概率的翻译结果。给定一个繁殖力序列，识别最佳翻译只需要独立地最大化每个输出位置上的局部概率($\mathfrak{local\ \ probability}$)即可。但是，搜索并边缘化整个繁殖力空间仍然是无法实现的，因此作者提出了以下3种启发式搜索算法来减少搜索空间。首先，定义 $Y = G(x_{1:T’}, f_{1:T’}; \theta)$ 来表示给定输入序列 $x_{1:T’}$ 和繁殖力序列 $f_{1:T’}$ 条件下的最佳翻译：</p>
<ul>
<li><p><strong>Argmax Decoding</strong>     因为繁殖力序列也是条件独立地分解建模的，所以通过为每个输入单词选择其概率最高的繁殖力来组成一个概率最高的繁殖力序列，并以此来估计最佳翻译：<br>$$<br>\Large \hat{Y}<em>{argmax} = G(x</em>{1:T’},\hat{f}<em>{1:T’};\theta),<br>\ where\ \  \hat{f}</em>{t’} = \underset{f} {\operatorname {argmax}} p_F(f_{t’}|x_{1:T’}; \theta)<br>$$</p>
</li>
<li><p><strong>Average Decoding</strong>     估计每个繁殖力作为其相应Softmax分布的期望值：<br>$$<br>\Large \hat{Y}<em>{average} = G(x</em>{1:T’},\hat{f}<em>{1:T’};\theta),<br>\ where\ \  \hat{f}</em>{t’} = Round\bigg(\sum_{f_{t’}=1}^Lp_F\big(f_{t’}|x_{1:T’};\theta\big)f_{t’}\bigg)<br>$$</p>
</li>
<li><p><strong>Noisy Parallel Decoding (NPD)</strong>     从繁殖力空间中随机采样一些繁殖力序列并为每个繁殖力序列计算其最佳翻译，然后交由AT教师模型为这些翻译打分，从而挑选出整体最佳翻译：<br>$$<br>\Large \hat{Y}<em>{NPD} = G\bigg(x</em>{1:T’}, \underset{f_{t’}\ \sim\ p_F}{\operatorname {argmax}}p_{\mathcal AR}\big(G(x_{1:T’},f_{1:T’};\theta)|X;\theta\big);\theta\bigg)<br>$$<br>值得注意的是，虽然AT模型在推理时具有高时延的特性，但是在为给定翻译打分时可以像训练时那样做到并行计算。NPD是一个随机的搜索方法，随着采样大小的增加，其对计算资源的需求也呈线性增长。总而言之，所有的搜索样本都可以被完全并行地计算和打分，因此该过程只会让时延增加一倍。</p>
</li>
</ul>
<h3 id="mathbf-Training"><a href="#mathbf-Training" class="headerlink" title="$\mathbf{Training}$"></a>$\mathbf{Training}$</h3><p>NAT模型包含了一个离散的顺序隐变量 $f_{1:T’}$，它的后验分布($\mathfrak{posterior\ \ distribution}$) $p(f_{1:T’}|x_{1:T’}, y_{1:T}; \theta)$ 可以使用一个建议分布($\mathfrak{proposal\ \ distribution}$) $q(f_{1:T’}|x_{1:T’}, y_{1:T})$ 来近似。这为总体最大似然损失提供了一个变量范围：<br>$$<br>\Large \mathcal L_{ML} = \log p_{\mathcal NA}(Y|X; \theta) = \log \sum_{f_{1:T’}\in\mathcal F} p_F(f_{1:T’}|x_{1:T’}; \theta)\cdot p(y_{1:T}|x_{1:T’}, f_{1:T’}; \theta) \<br>\large \geqslant \underset{f_{1:T’}\ \sim\ q} {\operatorname {\mathbb E}} \bigg(\sum_{t=1}^{T}\log p(y_{t}|x_1{f_1}, \dots, x_{T’}{f_{T’}}; \theta)\ +\ \sum_{t’=1}^{T’}\log p_F(f_{t’}|x_{1:T’}; \theta)\bigg)\ + \ \mathcal H(q)<br>$$<br>作者使用一个独立的、固定的繁殖力模型来定义建议分布 $q$，它可能是一个能够为训练语料库中的每个输入输出对生成一个确定的整数繁殖力序列的外部对齐器的输出。繁殖力也可能是由固定的AT教师模型的注意力权重计算而来。这能够大大地简化推理过程，因为 $q$ 上的期望值是确定的。</p>
<p>上述的损失函数使得作者可以以有监督方式来训练整个模型，通过使用预测的繁殖力来同时训练翻译模型 $p$ 和有监督的繁殖力神网络模型 $p_F$。</p>
<h4 id="序列级知识蒸馏"><a href="#序列级知识蒸馏" class="headerlink" title="序列级知识蒸馏"></a>序列级知识蒸馏</h4><p>虽然繁殖力模型能够显著地提升NAT模型将输出分布近似多模态目标分布的能力，但它却并不能完全解决训练数据中的不确定性问题。这是因为即使是同一个繁殖力序列，训练数据中也会有多个正确翻译与之对应。</p>
<p>因此，作者额外地将序列级知识蒸馏应用到了训练语料上：首先，在使用的训练语料库上训练一个AT教师模型并取其贪婪输出作为蒸馏数据集的目标序列；然后，作者会使用得到的蒸馏训练数据集来训练NAT学生模型。因为被训练出来的AT模型始终会把某英语序列翻译成另一固定的德语序列，从而缓解了原始语料库中的多模态翻译问题。而且AT模型的翻译结果是更少噪声化和更多确定性的，这也能够缓解多模态问题。最后，AT模型的翻译质量会低于原始的数据集，这将减少NAT学生模型的学习难度。</p>
<h4 id="强化学习的微调策略"><a href="#强化学习的微调策略" class="headerlink" title="强化学习的微调策略"></a>强化学习的微调策略</h4><p>我们的有监督繁殖力模型能够让整体最大似然损失分解为翻译损失和繁殖力损失两项，但它相比于变分训练($\mathfrak{varitional\ \ training}$)而言也有一些缺陷。它严重依赖于由外部对齐系统所提供的确定的近似推理模型，尽管这样对于训练整个模型端到端是理想的。</p>
<p>因此，作者提出在NAT模型训练收敛之后增加一个微调阶段。他们引入了一个额外的损失项，该损失由教师模型输出分布的反向K-L散度组成，这是一种词级知识蒸馏的形式：<br>$$<br>\Large \mathcal L_{RKL}(f_{1:T’}; \theta) = \sum_{t=1}^T\sum_{y_t} \big[\log p_{\mathcal{AR}}(y_t|\hat y_{1:t-1}, x_{1:T’})\cdot p_{\mathcal{NA}}(y_t|x_{1:T’}, f_{1:T’}; \theta)\big]<br>$$<br>其中，$\hat y_{1:T} = G(x_{1:T’}, f_{1:T’}; \theta)$。这种损失将会比标准的交叉熵损失更适合于高峰值的学生模型输出分布。（使模型收敛得更快）</p>
<p>在这之后，作者通过将原始蒸馏损失和两个反向K-L散度损失项求带权和来联合训练整个模型。其中一项是预测的繁殖力分布上通过baseline归一化后的期望值，另一项则是基于外部繁殖力推理模型的期望值：<br>$$<br>\Large \mathcal L_{FT} = \lambda\bigg(\underset{f_{1:T’}\ \sim\ p_F} {\operatorname {\mathbb E}}\big(\mathcal L_{RKL}(f_{1:T’}) - \mathcal L_{RKL}(\bar f_{1:T’})\big) + \underset{f_{1:T’}\ \sim\ q} {\operatorname {\mathbb E}}\big(\mathcal L_{RKL}(f_{1:T’})\big)\bigg) + (1 - \lambda)\mathcal L_{KD}<br>$$</p>
<p>其中，$\bar f_{1:T’}$ 是由Average Decoding计算出来的平均繁殖力。不可微的 $\mathcal L_{RL}$ 项的梯度可以使用REINFORCE算法估计得到，而 $\mathcal L_{BP}$ 项则可以使用普通的反向传播来训练。</p>

	

	
		<span class="different-posts"><a href="/2021/11/21/Sylvanas/" onclick="window.history.go(-1); return false;">⬅️ Go back </a></span>

	

</article>

	</main>

	<footer class="footer">
	<div class="footer-content">
		
	      <div class="footer__element">
	<p>Hi there, <br />welcome to my Blog glad you found it. Have a look around, will you?</p>
</div>

	    
	      <div class="footer__element">
	<h5>Check out</h5>
	<ul class="footer-links">
		<li class="footer-links__link"><a href="/archives">Archive</a></li>
		
		  <li class="footer-links__link"><a href="/atom.xml">RSS</a></li>
	    
		<li class="footer-links__link"><a href="/about">about page</a></li>
		<li class="footer-links__link"><a href="/tags">Tags</a></li>
		<li class="footer-links__link"><a href="/categories">Categories</a></li>
	</ul>
</div>

	    

		<div class="footer-credit">
			<span>© 2021 wbxu | Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> | Theme <a target="_blank" rel="noopener" href="https://github.com/HoverBaum/meilidu-hexo">MeiliDu</a></span>
		</div>

	</div>


</footer>



</body>

</html>
