{"meta":{"title":"Sylvanas Forever","subtitle":"自然语言处理在读研究生","description":"wbxu's blog","author":"wbxu","url":"http://example.com","root":"/"},"pages":[{"title":"","date":"2015-08-16T06:58:08.000Z","updated":"2021-11-24T08:26:17.551Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"Hello, everyone.大家好，我是一名苏州大学在读的硕士研究生，我选的方向是自然语言处理。 这个博客主要是用于发表一些我的实践经历和论文阅读，才疏学浅，若有错误还请指正。"},{"title":"Tags","date":"2016-08-11T04:12:45.000Z","updated":"2021-11-21T07:40:51.587Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","date":"2021-12-26T09:30:32.000Z","updated":"2021-12-26T09:34:17.234Z","comments":true,"path":"2021/12/26/Automated-Essay-Scoring/","link":"","permalink":"http://example.com/2021/12/26/Automated-Essay-Scoring/","excerpt":"","text":"1. Background1.1 Problems基于人工评分的方法具有以下的缺点 耗时耗力 主观性强 误差性大 过程漫长 1.2 Corresponding Information AES Systems $\\Delta$ e-rater Intelligent Essay Assessor Competition $\\Delta$ Kaggle 的 The Hewlett Foundation: Automated Essay Scoring Metrics $\\Delta$ Quadratic Weighted Kappa (QWK) “ASAP competition adopted QWK as the official evaluation metric“ Mean Absolute Error (MAE) Mean Square Error (MSE) Pearson’s Correlation Coefficient (PCC) Spearman’s Correlation Coefficient (SCC) Evaluation Schemas in-domain : 在同一提词上训练和评估，并且模型的总体性能由所有提词的平均性能所衡量 cross-domain : 在不同的提词上训练和评估，尤其适用于那些使用迁移学习技术的系统 2. Essay Quality Dimensions Difficulty Dimension Description 1 Grammaticality 文章语法 2 Usage 实词与虚词的使用 3 Mechanics 单词拼写、标点符号以及大小写的正误 4 Style 选词与句子结构的多样性 5 Relevance 内容与提词的相关性 6 Organization 文章结构的组织性 7 Development 以例子印证文章观点 8 Cohesion 过渡短语的使用恰当与否 9 Coherence 文章观点间的衔接恰当与否 10 Thesis Clarity 文章主题的阐述如何 11 Persuasiveness 文章中心论点的信服力如何 3. Features3.1 Length-based features因为长度已被证明和文章的整体评分呈现高度正相关的关系，所以基于长度的特征是 AES 系统中最重要的特征类型之一。这类特征**将文章的长度信息进行编码(句子/单词/字符的数量)**。 3.2 Lexical features n-grams 这类特征包含了词的一元文法、二元文法以及三元文法信息。这些单词 n-grams 信息十分有用，因为它们所编码的语法、语义、篇章信息对于 AES 系统都是大有裨益的。n-grams 作为特征的关键优势在于它们是语言独立的，其缺点就是通常需要大量训练数据来学习哪些单词 n-grams 是有用的。 statistics 这类特征包含了基于词 n-grams 而计算出来的统计数据(尤其是一元文法)。 3.3 Embeddings嵌入可以被视为一种 n-grams 特征的变体，可以说是一种语义上比 n-grams 更佳的(词/短语)表示，主要分为以下三类 第一类包含了基于在大语料上预训练词嵌入计算而来的特征 第二类包含了基于 AES 特定的嵌入表示计算而来的特征 第三类包含了可被训练的 one-hot 初始词向量 3.4 Word category features词类特征是基于包含了属于某一特定词汇、句法、语义类别的所有单词构成的词表或词典计算而来的。文章中某些类别的选词可以揭示作者组织思想、对提词作出连贯且一致的反应、掌握标准英语的能力。直观的来说，较高的单词级别表明词汇的使用更为复杂。词类特征有助于概括词的 n-grams 特征，但只在少量训练数据的情况下特别有用。 3.5 Prompt-relevant features提词相关性特征会编码文章内容与其提词的相关性，而一篇不符合提词的文章是不能得到高分的。不同的相似性衡量方法被用来计算一篇文章与其提词的相关性，如：单词重叠率及其变体、单词的主题性、通过随机索引测量的语义相似性等方法。 3.6 Readability features可读性特征需要编码一篇文章有多难读，该特征主要依赖于文章的选词。简单来说，一篇好的文章，需要使用广泛的词汇和多样的句子结构，但又不能晦涩难懂。该特征既可以使用可读性指标(如: Flesch-Kindcaid Reading Ease)来衡量，也可以使用简单的指标(如: type-token ratio)来衡量。 3.7 Syntactic features句法特征编码文章的句法信息，主要可以分为以下三类 Part-of-speech (POS) tag sequences 这类特征需要提供单词 n-grams 的句法概括并编码文章得非语法性和风格 Parse Trees 解析树的深度编码句子中句法结构的复杂性，短语结构规则编码不同语法结构的存在，依存关系计算标题与其从属关系之间的句法距离 Grammatical error rates 这类特征编码文章中语法错误出现的频率，可以使用语言模型计算或者直接从标注的语法错误类型中获得 3.8 Argumentation features论证特征基于文章的论证结构计算而来，因此该特征仅适用于那些具有论证结构的文章。一篇文章的论证结构是一颗树，其节点为论证成分(如: 断言、假设)，而边则表示对应论证成分间的关系(如: 支持、反对)。具体来说，论证特征基于论证结构及其具体内容来计算得出。 3.9 Semantic features语义特征编码文章中不同单词间的词汇语义关联性，主要分为以下两种语义特征 Histogram-based features 计算文章中的每对词之间的点状互信息(PMI)，它会根据共现性来衡量两个词之间的关联程度 通过对PMI值进行分档构建直方图，其中每个档对应的值表示具有该档内PMI值的词对所占的百分比 基于直方图计算出语义特征 直观地，高关联的配对比例越高，可能表明话题的延申越好，而低关联的配对比例越高，可能表明对语言的使用越有新意。 Frame-based features 该特征基于 FrameNet 中的语义框架计算而来。简言之，一个框架可以描述一句话中发生的事件，而框架的事件元素可以是参与相应事件的人或物。而知道文章中的某个论点是由作者还是他人表达的，这对文章论题的清晰度打分有益，因为论题的清晰度应该根据作者自己的论点来衡量。 3.10 Discourse features语篇特征编码文章的语篇结构，主要可以分为以下四种来源 Entity grids 实体网格是一种语篇表征，用于捕捉基于Centering Theory的文本局部连贯性 Discourse parse trees 基于Rhetorical Structure Theory构造的语篇解析树可以对文本的层次结构进行了编码(如: 一个语段是对另一个语段的阐述，还是与另一个语段有对比关系)，它可以用来捕捉文章的局部和整体的一致性特征 Lexical chains 词链是语篇中相关词汇的序列，可用于评估文本的内聚力。一篇包含了很多词链的文章，尤其是那些词链的开头和结尾覆盖了很大跨度的文章，往往具有更强的内聚力 discourse function label 语篇功能标签是定义在一个句子或段落上的，它表明了该句子或段落在特定文章中的语段功能(如: 一个段落是引言还是结论，一个句子是否是文章的论题) 这些标签可用于推导出文章组织评分所需的特征。 4. Corpora Corpora Essay Types Writer’s Language Level Essays Numbers Prompts Numbers CLC-FCE 议论文+记叙文+评论文+建议文+书信 非英语母语者+ESOL考生 1244 篇文章 10 个题词 ASAP 议论文+回应文+记叙文 7 到 10 年级的英国学生 17450 篇文章 8 个题词 TOEFL11 议论文 非英语母语者+托福考生 1100 篇文章 8 个题词 ICLE 议论文 非英语母语者+大学生 1003 篇文章 12 个题词 ICLE 议论文 非英语母语者+大学生 830 篇文章 13 个题词 ICLE 议论文 非英语母语者+大学生 830 篇文章 13 个题词 ICLE 议论文 非英语母语者+大学生 1000 篇文章 10 个题词 AAE 议论文 线上社区 essayforum2 102 篇论文 101 个题词 Corpora Scoring Task Score Range Additional Annotations CLC-FCE 整体评分 分数在 1 到 40 内 额外标注了语言学错误 (约 80 种错误类型) ASAP 整体评分 小则 0 到 3 之间，大则 0 到 60 之间 无额外标注 TOEFL11 整体评分 三个等级：Low，Medium，High 无额外标注 ICLE 文章组织评分 1 到 4 之间以 0.5 增长 无额外标注 ICLE 主题阐述评分 1 到 4 之间以 0.5 增长 无额外标注 ICLE 提词关联性评分 1 到 4 之间以 0.5 增长 无额外标注 ICLE 论点信服力评分 1 到 4 之间以 0.5 增长s 无额外标注 AAE 论点信服力评分 1 到 6 分之间 额外标注了影响信服力的因素 5. Tasks Task Description Additions Holistic scoring 基于文章的整体内容为其进行分数裁定的任务 AES 方向的主要任务 Dimension-specific scoring 针对文章质量的某个维度进行分数裁定 $11$ 种维度见 2 6. Approaches6.1 Neural NetworksA Neural Approach to Automated Essay Scoring “EMNLP 2016” 该论文是第一个使用神经网络进行自动文章评分的，它有效地缓解了模型对繁复的特征工程的需求，其大致处理流程如下 将 one-hot 向量作为输入 convolution 层来捕捉局部文本依赖 recurrent 层则用于捕捉文章中的长程依赖关系 将不同时间步的隐藏表示向量拼接起来以预测文章得分 6.2 Score-Specific Word EmbeddingsAutomatic Text Scoring Using Neural Networks “ACL 2016” 由于低信息词相较于高信息词而言，对文章评分的影响更小。出于此，该论文提出训练一个得分特定的词嵌入表示作为输入，而不是使用第一篇论文的 one-hot 向量。即，该模型能够依据词嵌入辨别高低信息化的用词。 6.3 Document StructureAutomatic Features for Essay Scoring – An Empirical Study “EMNLP 2016” 前面说的两篇论文都是将文章看作为单词的线性序列，因此该论文提出通过建模文章的等级结构来提升 AES 性能，其大致流程如下 词级卷积层接收独热词向量并独立抽取每个句子内的 n-gram 级特征信息 池化层将这些 n-gram 特征抽取出来并压缩为一个句子向量 句子级卷积层接收句子向量并抽取不同句子间的 n-gram 级特征信息 简言之: 将 words 融合以表示 sentences + 将 sentences 融合以表示 document。 6.4 Attention PoolingAttention-based Recurrent Convolutional Neural Network for Automatic Essay Scoring “CoNLL 2017” 为了能够自动地识别文章中重要的字符、单词、句子，该论文将注意力机制引入其中。即，使用一个注意力池化层来代替简单池化。具体地来讲就是，每个注意力池化层接收相应的卷积层输出作为输入，并充分利用可训练的权重矩阵来为输入向量进行加权组合。 6.5 CoherenceSkipFlow: Incorporating neural coherence features for end-to-end automatic text scoring “arXiv 2017” 因为连贯性使文章质量的一个重要维度，因此该论文假设通过计算和利用文章的连贯性得分可以提升整体性评分的性能，其大致流程如下 首先，LSTM 层负责建模文章内依赖关系 其次，额外的层来接收 LSTM 两个位置上的输出并为其计算相似度 (作者将其称为神经连贯性特征，因为直觉上连贯性是和相似度呈正相关的) 然后，神经连贯性特征会增强 LSTM 的输出向量 最后，模型基于神经连贯性特征增强的表示向量来进行整体评分 6.6 Transfer Learning理论上，训练提词特定的 AES 系统可以获得更准确的评分，因为测试相同的题词时，模型可以充分利用训练时学习到的该题词所特定的所有知识。然而，几乎对于任意特定的目标提词而言，其所对应的充足的训练数据都是不存在的。所以，许多 AES 系统都是以一种提词独立的方式训练出来的，这也就意味着少量的目标提词文章和大量的无目标提词文章被用于模型训练。于是，源提词词表和目标提词词表之间潜在的不一致就很可能会导致这些系统的性能下降。 Frustratingly Easy Domain Adaptation “ACL 2007” EasyAdapt 是一种简单而有效的迁移学习算法，它基于假设：”输入数据只可能是源提词或者目标提词的“。首先，我们知道不使用迁移学习的模型会采用一个被源提词和目标提词共享的特征空间，而 EasyAdapt 通过对这个特征空间中的特征进行三次复制来增强这种特征集。 第一次复制会存储两个领域共享的信息 第二次复制会存储源提词的信息 第三次复制会存储目标提词的信息 在这样的特征空间中，目标提词信息的重要性将会是源提词信息的 $2$ 倍，故模型能够更好地适应目标提词的信息。 Flexible Domain Adaptation for Automated Essay Scoring Using Correlated Linear Regression “EMNLP 2015” 该论文将 EasyAdapt 扩展到相关贝叶斯线性岭回归(Correlated Bayesian Linear Ridge Regression)，这使得**目标提词信息的权重可以被学习，而不是固定为 $2$**。 Constrained Multi-Task Learning for Automated Essay Scoring “ACL 2016” 该论文应用 EasyAdapt 来增强特征空间，此外还训练一个对级评级器(pairwise ranker)来评级同一提词的文章对。 TDNN: A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring “ACL 2018” 上述的几个系统还认为有少量的目标提词存在于训练数据中，而该论文则基于训练数据中不存在目标提词的假设来使用迁移学习技术，具体可分为以下两阶段 Stage 1 - 旨在识别测试集中质量极端的目标提词文章(极差或极佳)。作者使用提词无关特征(如语法错误、拼写错误等)在源提词文章上训练一个模型，之后使用该模型来为测试文章进行评分。这一步基于的假设是：质量极差或极佳的文章仅通过通用的特征就可以识别出来了。 Stage 2 - 旨在为剩余的测试文章评分。首先，作者将在 Stage 1 阶段选出的极端质量文章进行自动化标注，极佳为 $1$，极差为 $0$。然后，作者使用提词特定的特征在这些自动标注的文章上训练一个回归器，这一步基于的假设是：这些特定的特征需要正确地捕捉那些质量不极端文章的意义。最后，作者使用这个回归器来为剩余的测试文章进行评分，其分值区间为 $(0, 1)$。 7. State Of The Art Corpus System Scoring Task Approach CLC-FCE Modeling coherence in ESOL learner texts Holistic Ranking ASAP Automated essay scoring with string kernels and word embeddings “in-domain” Holistic Regression ASAP Automated essay scoring with string kernels and word embeddings “cross-domain” Holistic Regression TOEFL11 Investigating the Role of Linguistic Features Holistic Regression ICLE Using Argument Mining to Assess the Argumentation Quality of Essays Organization Regression ICLE Modeling Thesis Clarity in Student Essays Thesis Clarity Regression ICLE Modeling Prompt Adherence in Student Essays Prompt Adherence Regression ICLE Using Argument Mining to Assess the Argumentation Quality of Essays Persuasiveness Regression AAE Modeling Attributes Affecting Argument Persuasiveness in Student Essays Persuasiveness Regression (Neural) Corpus Features QWK PCC MAE CLC-FCE 5 - $0.749$ - ASAP 1 $0.785$ - - ASAP 1 $0.661$ ; $0.779$ ; $0.788$ ; $0.649$ - - TOEFL11 5 - $0.800$ $0.400$ ICLE 6 - - $0.315$ ICLE 4 - - $0.483$ ICLE 4 - $0.360$ $0.348$ ICLE 6 - - $0.378$ AAE 4 - $0.236$ $1.035$ 注: Kaggle Leaderboard 上的最新 SOTA 成绩为 $0.81407$。 Attachments Cambridge Learner Corpus-First Certificate in English exam “CLC-FCE” Automated Student Assessment Prize “ASAP” Automated Essay Scoring for Swedish “Corpora in Swedish” Automated Essay Scoring: A Survey of the State of the Art “IJCAI 2019” Code for “A Neural Approach to Automated Essay Scoring” “Based on Python 2.7 &amp; TensorFlow 1.X” Code for “Automatic Text Scoring Using Neural Networks” “Based on TensorFlow 1.9 &amp; Keras 2.2.2” Code for “SkipFlow: Incorporating neural coherence features for end-to-end automatic text scoring” “No Environment Information”","categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"}]},{"title":"Math 1-3","slug":"Math-1-3","date":"2021-12-24T15:42:33.000Z","updated":"2021-12-24T15:54:14.362Z","comments":true,"path":"2021/12/24/Math-1-3/","link":"","permalink":"http://example.com/2021/12/24/Math-1-3/","excerpt":"组合数学前三章知识点的大致整理","text":"组合数学前三章知识点的大致整理 § 第一章 排列、组合、二项式定理$Ⅰ$ 分配问题 将 $n$ 个 不同的的小球 放入 $r$ 个 不同的盒子 中，且 允许 存在空盒 $$\\large r^n$$ 解: 对于第 $i$ 个小球来说, 它都有 $r$ 个盒子可以放 $(i = 1, 2, \\dots)$所以一共有 $r^n$ 种分配方案。 将 $n$ 个 不同的的小球 放入 $r$ 个 不同的盒子 中，且 不允许 存在空盒 $$\\large r! S(n, r)$$ 解: 因为小球各不相同且每个盒子又不能为空所以, 可以看作是对小球的集合 $M$ 进行 $r$ 分划, 即 $S(n, r)$又因为盒子也各不相同所以还需要对分划后的 $r$ 个小球子集进行排列, 即 $r!$综上所述, 一共有 $r! S(n, r)$ 种分配方案。 将 $n$ 个 不同的的小球 放入 $r$ 个 相同的盒子 中，且 允许 存在空盒 $$\\large \\sum_{i=1}^{r} S(n, i)$$ 解: 因为允许盒子为空且盒子都是一样的, 那么空盒的个数就可能为 $r-1, r-2, \\dots, 0$对于非空盒为 $i\\ \\ (i = 1, 2, \\dots)$ 个的情况, 那么我们只需要对不同小球集合 $M$ 进行 $i$ 分划即可又因为盒子完全相同, 所以无须排列综上所述, 一共有 $\\sum_{i=1}^{r} S(n, i)$ 种分配方案 将 $n$ 个 不同的的小球 放入 $r$ 个 相同的盒子 中，且 不允许 存在空盒 $$\\large S(n, r)$$ 解: 因为盒子完全相同且不允许存在空盒所以直接将不同小球集合 $M$ 进行 $r$ 分划即可因此, 一共有 $S(n, r)$ 种分配方案 将 $n$ 个 相同的的小球 放入 $r$ 个 不同的盒子 中，且 允许 存在空盒 $$\\large \\binom{n+r-1}{r-1} $$ 解: 相当于求 $x_1 + x_2 + \\dots + x_r = n$ 的非负整数解个数此时将 $r-1$ 个 $0$ 作为分隔符插入到 $n$ 个 $1$ 里面, 那个结果字串就可以表示一种非负整数解因此原题又相当于多重集合 $M = {n \\times 1, (r-1) \\times 0}$ 的全排列数由书上定理可知, 其全排列数为 $\\frac{(n+r-1)!}{n! (r-1)!} = \\binom{n+r-1}{r-1} $因此, 一共有 $\\binom{n+r-1}{r-1}$ 种分配方案 将 $n$ 个 相同的的小球 放入 $r$ 个 不同的盒子 中，且 不允许 存在空盒 $$\\large \\binom{n-1}{r-1} $$ 解: 相当于求 $x_1 + x_2 + \\dots + x_r = n$ 的正整数解个数令 $ y_i = x_i - 1$, 则有 $y_1 + y_2 + \\dots + y_r = n - r$ 且 $y_i \\nless 0$ $(i = 1, 2, \\dots, r)$因此相当于求 $y_1 + y_2 + \\dots + y_r = n - r$ 的非负整数解个数此时将 $r-1$ 个 $0$ 作为分隔符插入到 $n-r$ 个 $1$ 里面, 那个结果字串就可以表示一种非负整数解因此原题又相当于多重集合 $M = {(n-r) \\times 1, (r-1) \\times 0}$ 的全排列数由书上定理可知, 其全排列数为 $\\frac{[(n-r)+(r-1)]!}{(n-r)! (r-1)!} = \\binom{n-1}{r-1} $因此, 一共有 $\\binom{n-1}{r-1}$ 种分配方案 将 $n$ 个 相同的的小球 放入 $r$ 个 相同的盒子 中，且 允许 存在空盒 $$\\large \\sum_{i=1}^{r} B(n, i)$$ 解: 因为允许盒子为空且盒子都是一样的, 那么空盒的个数就可能为 $r-1, r-2, \\dots, 0$对于非空盒为 $i\\ \\ (i = 1, 2, \\dots)$ 个的情况, 那么我们只需要对 $n$ 进行 $i$ 无序分拆即可综上所述, 一共有 $\\sum_{i=1}^{r} B(n, i)$ 种分配方案 将 $n$ 个 相同的的小球 放入 $r$ 个 相同的盒子 中，且 不允许 存在空盒 $$\\large B(n, r)$$ 解: 因为盒子完全相同且不允许存在空盒所以直接将 $n$ 进行 $r$ 无序分拆即可因此, 一共有 $B(n, r)$ 种分配方案 $Ⅱ$ 多重集合排列和组合的证明 多重集合 $M = {k_1 \\cdot a_1, k_2 \\cdot a_2, \\dots, k_n \\cdot a_n}$ 的全排列数 $$\\Large \\frac{(k_1 + k_2 + \\dots + k_n)!}{k_1! k_2! \\cdots k_n!}$$ 证: $M$ 的全排列中, $a_1$ 占了 $k_1$ 个又因为所有的 $a_1$ 都完全相同, 所以不需要排列于是, 对 $a_1$ 有 $\\binom{k_1 + k_2 + \\dots + k_n}{k_1}$ 种方案类似地, 在 $M$ 剩余的 $k_2 + k_3 + \\dots + k_n$ 位置中, $a_2$ 占了 $k_2$ 个又因为 $a_2$ 完全相同, 所以不用排列于是, 对 $a_2$ 有 $\\binom{ k_2 + k_3 + \\dots + k_n}{k_1}$ 种方案以此类推并由乘法原则可知一共有$$\\large \\binom{k_1 + k_2 + \\dots + k_n}{k_1} \\cdot \\binom{ k_2 + k_3 + \\dots + k_n}{k_1} \\cdots \\binom{k_n}{k_n}$$ $= $ $$\\large \\frac{(k_1 + k_2 + \\dots + k_n)!}{k_1!(k_2 + k_3 + \\dots + k_n)!} \\cdot \\frac{(k_2 + k_3 + \\dots + k_n)!}{k_2!(k_3 + k_4 + \\dots + k_n)!} \\cdots \\frac{k_n!}{k_n!} $$ $= $ $$\\Large \\frac{(k_1 + k_2 + \\dots + k_n)!}{k_1! k_2! \\cdots k_n!}$$ 多重集合 $M = {\\infty \\cdot a_1, \\infty \\cdot a_2, \\dots, \\infty \\cdot a_n}$ 的 $r$ 组合数 $$\\Large \\binom{r + n - 1}{r}$$ 证: 假设 $x_i$ 表示 $a_i$ 被选取的个数 ($i = 1, 2, \\dots, n$)那么就相当于求 $x_1 + x_2 + \\dots + x_n = r$ 的非负整数解个数我们将 $n-1$ 个 $0$ 插入到 $r$ 个 $1$ 串中去, 那么就会被划分出 $n$ 个 $1$ 的字串, 而第 $i$ 的字串中 $1$ 的个数就表示 $x_i$ 的取值大小先定义多重集合 $N = {r \\times 1, (n-1) \\times 0}$因此, 我们可以知道一种上述的 $10$ 串就可以唯一对应上述方程的一个非负整数解即, 多重集合 $N$ 的 $r$ 组合数与方程的非负整数解一一对应故原题又相当于求多重集合 $N = {r \\times 1, (n-1) \\times 0}$ 的全排列数由书上定理可知, 其全排列数为 $\\frac{[r+(n-1)]!}{r! (n-1)!} = \\binom{r+n-1}{r} $因此, 一共有 $\\binom{r+n-1}{r}$ 种分配方案 $Ⅲ$ 二项式定理$$\\large (x+y)^n = \\binom{n}{0} y^n + \\binom{n}{1} x^1 y^{n-1} + \\dots + \\binom{n}{n-1} x^{n-1} y^1 + \\binom{n}{n} x^n$$ 即 $$\\large (x+y)^n = \\sum_{i=0}^{n} \\binom{n}{i} x^i y^{n-i}$$ $Ⅳ$ 分划数的性质 $$\\large S(n, 1) = 1$$ $$\\large S(n, 2) = 2^{n-1} - 1$$ $$\\large S(n, r) = S(n-1, r-1) + r S(n-1, r)$$ $$\\large S(n, n-1) = \\binom{n}{2}$$ $$\\large S(n, n) = 1$$ $Ⅴ$ 无序分拆数的性质 $$\\large B(n, 1) = 1$$ $$\\large B(n, 2) = \\lfloor \\frac{n}{2} \\rfloor$$ $$\\large B(n) = \\sum_{k=1}^{n} B(n, k)$$ $$\\large B(n+k, k) = B(n, 1) + B(n, 2) + \\dots + B(n, k)$$ $$\\large B(n, n) = 1$$ § 第二章 容斥原理与鸽巢原理$Ⅰ$ 容斥原理的三个表示 设 $S$ 是一有限集合, $P_1, P_2, \\dots, P_m$ 是同集合 $S$ 有关的 $m$ 个性质, 设 $A_i$ 为 $S$ 中具有性质 $P_i$ 的元素构成的集合 ($1 \\leqslant i \\leqslant m$), $\\bar{A_i}$ 是 $S$ 中不具有性质 $P_i$ 的元素构成的集合 ($1 \\leqslant i \\leqslant m$), 则 $S$ 中不具有全部 $m$ 个性质的元素个数为 $$\\large |\\bar{A_1} \\cap \\bar{A_2} \\cap \\dots \\cap \\bar{A_m}| = |S| - \\sum_{i=1}^{m}|A_i| + \\sum_{1 \\leqslant i \\ne j \\leqslant m}^{m} |A_i \\cap A_j| - \\dots + (-1)^{m} |A_1 \\cap A_2 \\cap \\dots \\cap A_m|$$ 设 $S$ 是一有限集合, $P_1, P_2, \\dots, P_m$ 是同集合 $S$ 有关的 $m$ 个性质, 设 $A_i$ 为 $S$ 中具有性质 $P_i$ 的元素构成的集合 ($1 \\leqslant i \\leqslant m$), $\\bar{A_i}$ 是 $S$ 中不具有性质 $P_i$ 的元素构成的集合 ($1 \\leqslant i \\leqslant m$), 则 $S$ 中至少具有一个性质的元素个数为 $$\\large |A_1 \\cup A_2 \\cup \\dots \\cup A_m| = \\sum_{i=1}^{m}|A_i| - \\sum_{1 \\leqslant i \\ne j \\leqslant m}^{m} |A_i \\cap A_j| + \\dots + (-1)^{m-1} |A_1 \\cap A_2 \\cap \\dots \\cap A_m|$$ 设集合 $S$ 中具有性质集合 $P = {P_1, P_2, \\dots, P_m}$ 中恰有 $r$ 个性质的元素个数为 $N(r)$, 则 $$\\large N(r) = w(r) - \\binom{r+1}{r} w(r+1) + \\binom{r+2}{r} w(r+2) - \\dots + (-1)^{m-r} \\binom{m}{r} w(m)$$ 其中, $w(0) = |S|$, $w(r) = \\sum_{1 \\leqslant i_1 \\leqslant \\dots \\leqslant i_r \\leqslant m} N(P_{i_1}, P_{i_2}, \\dots, P_{i_r})$, 而 $N(P_{i_1}, P_{i_2}, \\dots, P_{i_r})$ 表示 $S$ 中具有性质 $P_{i_1}, P_{i_2}, \\dots, P_{i_r}$ 的元素个数 $Ⅱ$ 容斥原理应用 求任意多重集合 $M$ 的 $r$ 组合数 设 $S_{\\infty} = { \\infty \\cdot a_i }$ 取集合 $A$ 为 $S_{\\infty}$ 的 $r$ 组合全体 定义性质集合 $P$ 假设 $a_1$ 元素在 $M$ 中的个数为 x, 那么 $P_1$ 应该指 $r$ 组合中 $a_1$ 的个数大于等于 $x+1$ 取集合 $A_i$ 为满足性质 $P_i$ 的 $r$ 组合全体 计算容斥原理中剩余的各个项 使用容斥原理求解答案 错排问题 $$\\large D_n = n! (1 - \\frac{1}{1!} + \\frac{1}{2!} - \\dots + (-1)^n \\frac{1}{n!})$$ 有禁止模式的错排问题 $$\\large Q_n = n! - \\binom{n-1}{1} (n-1)! + \\binom{n-1}{2} (n-2)! - \\dots + (-1)^{n-1} \\binom{n-1}{n-1} 1! = D_n + D_{n-1}$$ $Ⅲ$ 鸽巢原理 如果把 $n+1$ 个物体放入 $n$ 个盒子里, 那么至少有一个盒子里有两个或更多的的物体 设 $a_1, a_2, \\dots, a_n$ 都是正整数, 如果把 $a_1 + a_2 + \\dots + a_n - n + 1$ 个物体放入 $n$ 个盒子中, 那么至少存在一个 $i \\in [1, n]$, 使得第 $i$ 个盒子至少包含 $a_i$ 个物体 $Ⅳ$ 鸽巢原理应用 寻找和题目条件相关的数值组合、区间等 对数据进行分组 因为须要从 $m$ 个组中选择 $m+1$ 个数 所以, 由鸽巢原理可证原题关系 § 第三章 递推关系$Ⅰ$ 非齐次特解 非齐次项 是否为特征根 非齐次特解 $\\beta^{n}$ $\\beta$ 为 $m$ 重特征根 $\\alpha \\cdot n^m \\cdot \\beta^n$ $\\beta^{n}$ $\\beta$ 不为特征根 $\\alpha \\cdot \\beta^n$ $n^s$ $1$ 为 $m$ 重特征根 $n^m \\cdot (b_s \\cdot n^s + b_{s-1} \\cdot n^{s-1} + \\dots + b_1 \\cdot n + b_0)$ $n^s$ $1$ 不为特征根 $b_s \\cdot n^s + b_{s-1} \\cdot n^{s-1} + \\dots + b_1 \\cdot n + b_0$ $n^s \\beta^n$ $\\beta$ 为 $m$ 重特征根 $n^m \\cdot (b_s \\cdot n^s + b_{s-1} \\cdot n^{s-1} + \\dots + b_1 \\cdot n + b_0) \\beta^n$ $n^s \\beta^n$ $\\beta$ 不为特征根 $(b_s \\cdot n^s + b_{s-1} \\cdot n^{s-1} + \\dots + b_1 \\cdot n + b_0) \\beta^n$ $Ⅱ$ 迭代法 解题步骤 求解 $f(n) = F(f(\\le n))$ 的迭代关系: $f(n) = \\dots = F(?) + f(1)$ 数学归纳法证明之 证明 $k=1$ 时, 上式成立 假设 $n=k$ 时, 上式成立 证明 $n=k+1$ 时, 上式成立 转换递推关系 将变系数的一阶线性递推关系转换为常系数的线性递推关系 将一阶高次递推关系通过变量代换为一阶线性递推关系式 $Ⅲ$ Fibonacci 数和 Catalan 数 Fibonacci 数的性质 $$\\large f(n) = f(n-1) + f(n-2)\\ \\ and\\ \\ f(1)=1, f(0)=1$$ $$\\large f(n) = \\binom{n}{0} + \\binom{n-1}{1} + \\dots + \\binom{n-k}{k}\\ \\ and\\ \\ k = \\lfloor \\frac{n}{2} \\rfloor$$ Catalan 数 $n$ 个 $+1$ 和 $n$ 个 $-1$ 构成的 $2n$ 项 $a_1, a_2, \\dots, a_{2n}$, 其部分和满足 $a_1 + a_2 + \\dots + a_k \\geqslant 0$ ($k = 1, 2, \\dots, 2n$) 的数列的个数等于第 $n$ 个 Catalan 数 $$\\large C_n = \\frac{1}{n+1} \\binom{2n}{n}$$","categories":[{"name":"Combinatorial Mathematices","slug":"Combinatorial-Mathematices","permalink":"http://example.com/categories/Combinatorial-Mathematices/"}],"tags":[{"name":"Combinatorial Mathematices","slug":"Combinatorial-Mathematices","permalink":"http://example.com/tags/Combinatorial-Mathematices/"}]},{"title":"How does Length Prediction Influence the Performance of Non-Autoregressive Translation?","slug":"How-does-Length-Prediction-Influence-the-Performance-of-Non-Autoregressive-Translation","date":"2021-12-17T15:29:54.000Z","updated":"2021-12-17T15:36:21.944Z","comments":true,"path":"2021/12/17/How-does-Length-Prediction-Influence-the-Performance-of-Non-Autoregressive-Translation/","link":"","permalink":"http://example.com/2021/12/17/How-does-Length-Prediction-Influence-the-Performance-of-Non-Autoregressive-Translation/","excerpt":"EMNLP 2021 : How Length Prediction Influence the Performance of Non-Autoregressive Translation? 长度预测(length prediction)是 NAT 模型的一项特殊任务，旨在(预先/动态)决定目标序列的长度。然而长度预测性能由何决定又如何与翻译质量相联系这个问题却鲜有人关注，因此作者基于 CMLM 模型进行了大量的实验以研究如下两个问题： 影响长度预测性能的因素是什么？ 长度预测是如何影响翻译质量的？","text":"EMNLP 2021 : How Length Prediction Influence the Performance of Non-Autoregressive Translation? 长度预测(length prediction)是 NAT 模型的一项特殊任务，旨在(预先/动态)决定目标序列的长度。然而长度预测性能由何决定又如何与翻译质量相联系这个问题却鲜有人关注，因此作者基于 CMLM 模型进行了大量的实验以研究如下两个问题： 影响长度预测性能的因素是什么？ 长度预测是如何影响翻译质量的？ 引言Transformer 模型实现了训练时的并行化，而 Vanilla NAT 模型则实现了推理时的并行化。但 NAT 基于的条件独立性假设使其翻译质量远不如对应的 AT 模型，因而各种方法被提出以提升 NAT 模型性能，包括了基于插入的方法、基于迭代的方法、基于隐变量的方法等等。这些方法都希望将目标序列依赖关系建模施加到 NAT 模型推理阶段，但除此之外，NAT 模型的长度预测问题也是很重要的。之前的学者所提出的方法大致可以分为以下四类： 基于对齐 Gu 等人通过繁殖力来隐式地预测目标长度；但这可能会引入噪声，因为我们并没有完全正确的对齐关系；当然 Gu 等人还提出了 NPD 来提升模型性能； 基于统计 模型在训练集上学习一个比例 α 来讲源句长度映射为目标长度；显然，这种频次统计 α 值无法兼顾各条数据样本的差异性； 基于回归 让分类器基于源端表示来预测出目标长度；这个方法直观上比之前的好，但是这种固定长度仍然会抑制推理的灵活性； 基于插入 通过动态地执行 tokens 插入操作，模型就可以在推理期间动态地调整目标序列的长度；其缺陷就是大大降低了 NAT 模型的推理速度； 实验分析基于统计的长度预测分析 De 和 Ro 的句子长度普遍要长于 En 的句子; 对于所有的语言对，随着源端长度的增长，长度比率 $\\alpha$ 也会随之变化 (通常都是不断衰减)，这说明单一的线性模型是不充分的； 训练数据集上的 $\\alpha$ 与测试数据集上的 $\\alpha$ 值存在着明显的差值(gap)，这可能会导致长度预测的错误发生； 对于 En-de 语言对，原始数据上的 $\\alpha$ 的标准差和均值范围始终大于蒸馏数据集，这说明蒸馏数据集更加的干净和简单；对于 En-Ro 语言对，这种差距没有那么明显。 总的来说，对于基于统计的方法 $L_y = \\alpha L_x$ 而言，其优点是简单且时间效率较高；其缺陷在于统计值 $\\alpha$ 过于笼统而忽视了各条数据样本间的差异；进一步来讲，目标长度并不是仅仅依赖于源端长度的，它还依赖于各种语言属性(如：句法、语义等) 长度预测错误分析 NAT 模型，尤其是训练在蒸馏数据集上的，明显优于 AT 教师模型，这说明了 NAT 模型能够明确地建模目标长度分布，可能的原因为：相较于 AT 模型的隐式预测，NAT 模型的明确预测能够获得更好的性能； 虽然直觉上来说，因为原始数据集中的训练数据集和测试数据集同分布，所以训练在原始数据集上的 NAT 模型应该会优于在蒸馏数据集上的模型；但事实却不然；这可能是因为：蒸馏数据集更加的干净和单调，这就使得 NAT 模型更容易符合目标长度分布，尽管它的长度错误是更大的； 大部分长度错误都在 5 个 tokens 以内，并且在原始数据集上训练的 NAT 模型($R$)、在蒸馏数据集上训练的 NAT 模型($D$)以及在原始数据集上训练的 AT 模型($KD$)所对应的长度预测错误都有着明显不同的分布； 长度预测与翻译质量的关联分析① 源句长度源句长度的影响不大，可以不将其考虑在内。 ② 长度错误随着长度错误的增加，模型的翻译质量几乎呈线性衰减，这表明 AT 和 NAT 模型都存在着这样的关系：长度错误与翻译质量成负相关。 ③ 翻译上界除了 Ro-En 结果，真实长度能够明确地提升翻译质量，这是因为预测的长度可以被视为一种离散的隐变量，所以目标长度建模和标记预测之间存在着强联系；而在 Ro→En 上，只有原始数据集上出现了相反的现象，这可能是因为过拟合训练集；如果模型能够在长度候选中推断出其最佳的目标长度，那么翻译质量还能够得到潜在的提升。 ④ 迭代改进 较少的迭代会将真实长度 $L$ 打上错误的得分*； 较少的几轮迭代所产生的翻译句子中通常都存在着许多重复标记； 首轮迭代中，真实长度 L* 反而产生了更多的重复标记，表明了即使给定了真实目标长度 L*，模型也未必能够善加利用； 经过更多的迭代改进之后，真实长度下的翻译句子质量会更好，说明了更多的迭代起了十分重要的作用。 总结 目标长度是 NAT 模型中的一个重要隐变量，且其与翻译质量之间存在着强关联，因此应该得到重视； 准确地预测到真实的目标长度虽然有益于提升模型性能，但是如果模型在长度束中能够推理出其最佳目标长度时，模型的翻译质量还可以获得进一步提升以达到性能上界； 在改善 NAT 翻译质量方面，灵活的解码策略比竭力追求准确的长度预测更有效，这是因为语言具有复杂性和多样性，基本上不存在 gold 目标长度；","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"Non-Autoregressive Text Generation with Pre-trained Language Models","slug":"Non-Autoregressive-Text-Generation-with-Pre-trained-Language-Models","date":"2021-12-10T05:31:31.000Z","updated":"2021-12-10T05:33:30.098Z","comments":true,"path":"2021/12/10/Non-Autoregressive-Text-Generation-with-Pre-trained-Language-Models/","link":"","permalink":"http://example.com/2021/12/10/Non-Autoregressive-Text-Generation-with-Pre-trained-Language-Models/","excerpt":"AG 模型因其链式条件概率建模而饱受高时延之痛；NAG 模型又因其强条件独立性假设而深陷低性能的泥沼。因此，作者提出使用预训练 BERT 模型来作为 NAG 模型的骨干从而提升其性能，并设计了一种简单却不失优雅的解码机制来缓解 NAG 模型的两个通病。为了进一步提升 NAG 模型在推理速度上的优势，作者额外引入了 $ratio$-$first$ 解码策略来加快模型解码速度。在三个文本生成任务($text\\ \\ summarization$、$sentence\\ \\ compression$、$machine\\ \\ translation$)上进行了实验，结果表明了作者的模型确实优于其他 NAG 模型。","text":"AG 模型因其链式条件概率建模而饱受高时延之痛；NAG 模型又因其强条件独立性假设而深陷低性能的泥沼。因此，作者提出使用预训练 BERT 模型来作为 NAG 模型的骨干从而提升其性能，并设计了一种简单却不失优雅的解码机制来缓解 NAG 模型的两个通病。为了进一步提升 NAG 模型在推理速度上的优势，作者额外引入了 $ratio$-$first$ 解码策略来加快模型解码速度。在三个文本生成任务($text\\ \\ summarization$、$sentence\\ \\ compression$、$machine\\ \\ translation$)上进行了实验，结果表明了作者的模型确实优于其他 NAG 模型。 现存问题 NAG 模型具备高并行化能力因而能够更快地推理，但其生成质量常常落后于对应的 AG 模型； 之前的 NAG 模型需要在推理输出序列前就预测好目标序列的长度，这就使得其不得不额外增加一个长度预测模块； 而长度预测模块所预测到的最佳长度有很可能不是正确的目标长度，这使其又不得不采取长度束再评分方法，该方法大大削弱了 NAG 模型的快速推理优势； 现存的 NAG 模型都**需要强条件独立性以支持其并行解码范式，这就会导致其翻译出不符合语法规则的语段(重译问题)**； NAG-BERT 针对 NAG 模型性能问题，作者引入预训练 BERT 模型来提升模型性能； 针对长度预测问题，作者设计了一个基于模型决策的解码机制来动态地调整目标长度； 针对条件独立性假设，作者在损失函数中额外引入一项损失以缓解重译问题； 针对 NAG 的低延时优势，作者设计了一个解码策略 $ratio$-$first$ 来进一步提升解码速度。 Model Architecture 整个模型就是在 BERT 架构上添加一个 CRF 网络，而其中的 BERT 模型部分都是用预训练的 BERT 参数来进行初始化。 参照 BERT 模型，作者首先将 、 这两个特殊标记添加到输入序列中去。然后使用 标记来讲输入序列的长度填充到最长，这样可以保证输入序列的长度一定不小于目标长度。接下来就是简单的多头自注意力网络和位置级前馈神经网络的堆栈而已(和 BERT 或者说 Transformer encoder 是一样的)。 经过 BERT 编码后的隐藏表示 $H$ 会被喂给一个线性链 CRF 网络，长度为 $T’$ 目标序列 $Y$ 的似然函数如下所示：$$\\large P_{CRF} (Y \\mid X) = \\frac{e^{S (X, Y)}}{\\sum_{Y’} e^{S (X, Y’)}} = \\frac{1}{Z (X)} exp (\\sum_{i=1}^{T’} \\Phi_{y_i} (h_i) + \\sum_{i=2}^{T’} t(y_{i-1}, y_i))$$ 其中，$Z(X)$ 是归一化因子；$\\Phi_{y_i} (h_i)$ 代表了 $y_i$ 在位置 $i$ 上的得分；神经网络参数 $\\Phi$ 会将 BERT 输出的隐藏表示映射到目标词表空间中去；$t(y_{i-1}, y_i)$ 表示从标签 $y_{i-1}$ 转移到标签 $y_i$ 的转移得分；$T$ 就是转移矩阵 又因为目标词表很大(32k)，所以直接建模转移矩阵 $T$ 和归一化因子 $Z(X)$ 是不切实际的，故作者选择采用 Sun 等人的方法： 对于 $T$ 矩阵，通过分解的方法，将 $T$ 分解为 $E_1$ 和 $E_2$ 两个较小的矩阵，其中 $T = E_1 × E_2^T$； 对于归一化因子 $Z(X)$，与其搜索所有可能的路径，倒不如使用预先定义的搜索束进行剪枝； Length Prediction作者的基本想法就是希望模型能够通过生成一个特殊标记 () 来动态地停止序列生成。为此，作者将两个连续的 标记添加到目标序列的末尾，进而模型能够在两个 之间学习到一个确定的转移行为，即 → 。这是因为在训练期间，模型压根看不到任何一个从 转移回回目标词表的转移行为。 推理期间，结果 $Y$ 需要能够最大化 CRF 的评分函数，该评分函数可分解为如下形式：$$\\large S (X, Y’) = \\sum_{i=1}^{T} \\Phi_{y_i’} (h_i) + \\sum_{i=2}^{T} t(y_{i-1}’, y_i’) = \\Phi_{y_1’} (h_1) + \\sum_{i=2}^{T} {\\Phi_{y_i’} (h_i) + t(y_{i-1}’, y_i’)}$$ 一旦解码路径上出现了一个 标记，那么接下来的所有转移行为都将在 和 中进行。简言之，翻译句子的后续子序列仅由 构成，并且会被删除。 Ratio-First DecodingBERT 的输出其实可以分为两段，第一段为第一个 之前看的输出标记序列，第二段则为剩余部分。如上图所示，y1 y2 y3 y4 为第一段，剩余为第二段。不难发现，第二段对于最终输出结果毫无作用，故而去除之。这足以说明只考虑 BERT 输出的前半段可以提升模型的解码速度。因此，对于那些已知目标序列短于源序列的任务，作者只使用 BERT 输出的前 $[\\alpha \\cdot T]$ 长度的子序列来进行推理。其中，$T$ 是源序列长度；$\\alpha$ 是数据统计值；$[\\cdot]$ 是整数舍入法。正式的公式如下所示：$$\\large \\hat{Y} = \\underset{Y’}{\\operatorname {argmax}} \\mathcal{F} (X, Y’, \\alpha) = \\underset{Y’}{\\operatorname {argmax}} { \\sum_{i=1}^{[\\alpha \\cdot T]} \\Phi_{y_i’} (h_i) + \\sum_{i=2}^{[\\alpha \\cdot T]} t(y_{i-1}’, y_i’) }$$ 值得注意的是： 当 α = 1.0 时，该解码策略就退化为了标准的解码策略； ratio-first 虽然会减短目标序列的最大长度，但是其实际输出长度仍然由模型动态地决定； ratio-first 能够在保留生成质量的同时显著提高推理速度； Learning ObjectiveNAG 因其在输出标记上的条件独立性近似而偏爱重复单词，引入目标端明确的依赖关系可缓解之。作者提出在 NAG 的上下文中使用无可能表示(unliklihood formulation)，即：将否定的候选词集定义为在预设的上下文窗口 $c$ 内的邻近标记。具体的公式如下所示：$$\\large \\mathcal{L}{CA} (Y \\mid X) = - \\sum{i=1}^{T’} {\\log p_{\\theta} (y_i \\mid h_i; X) + l_{CA} (i)}$$$$\\large l_{CA} (i) = \\sum_{j = i - c,\\ y_j \\ne y_i}^{j = i + c} \\log (1.0 - p_{\\theta} (y_j \\mid h_i; X))$$ 其中，$l_{CA} (i)$ 被用来放大位置 $i$ 邻近重复标记的损失值；$\\mathcal{L}_{CA}$ 则被用来最小化训练损失。于是，该策略会打消模型产生邻近的重复标记的积极性。 Results 作者在机器翻译任务上得出的结论有： NAG-BERT 模型在性能和速度上都实现了有效的提升； ratio-first 解码策略在损失一点性能的同时，将解码速度提高到 13.92 倍。","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation","slug":"ENGINE-Energy-Based-Inference-Networks-for-Non-Autoregressive-Machine-Translation","date":"2021-12-09T06:40:14.000Z","updated":"2021-12-09T06:41:30.409Z","comments":true,"path":"2021/12/09/ENGINE-Energy-Based-Inference-Networks-for-Non-Autoregressive-Machine-Translation/","link":"","permalink":"http://example.com/2021/12/09/ENGINE-Energy-Based-Inference-Networks-for-Non-Autoregressive-Machine-Translation/","excerpt":"知识蒸馏是一种用于将 AT 模型知识迁移到 NAT 模型的方法，它能够有效地帮助 NAT 模型缓解多模问题(multi-modality problem)。准确的来说，通过一个已训练的 AT 模型来翻译原始语料中的源语言句子，然后将 AT 模型预测的翻译句子取代源预料中的 ground true，从而构造出蒸馏数据集(distill dataset)，最后 NAT 模型将在蒸馏数据集上进行训练。","text":"知识蒸馏是一种用于将 AT 模型知识迁移到 NAT 模型的方法，它能够有效地帮助 NAT 模型缓解多模问题(multi-modality problem)。准确的来说，通过一个已训练的 AT 模型来翻译原始语料中的源语言句子，然后将 AT 模型预测的翻译句子取代源预料中的 ground true，从而构造出蒸馏数据集(distill dataset)，最后 NAT 模型将在蒸馏数据集上进行训练。 但是，这种方法将会导致 NAT 模型只能够学习到 AT 模型在固定的数据集上的知识，从而导致 NAT 模型的性能落后于 AT 模型。为此，作者将 NAT 模型视作一个推理网络(inference network)， 该网络被训练以最小化 AT 教师模型的能量(energy)。作者认为这能让 NAT 模型学习到额外的能量信息。为了能够让推理网络最小化 AT 能量，这个能量就必须关于推理网络的输出是可微的，这样就可以进行基于梯度的优化。 Energy对于一个自回归的神经机器翻译系统而言，由链式条件分解有：$$\\large \\log p_{\\Theta} (y \\mid x) = \\sum_{t=1}^{\\mid y \\mid} \\log p_{\\Theta} (y_t \\mid y_{0:t-1}, x)$$ 这个模型就可以被视作一个基于能量的模型(energy-based model)，而其**能量函数(Energy Function)**为：$$\\large E_{\\Theta} (x, y) = - \\log p_{\\Theta} (y \\mid x)$$ 给定训练的参数 $\\Theta$，模型进行推理时就是需要找到一个能够最小化能量(energy)的翻译句子：$$\\large \\hat{y} = \\underset{y}{\\operatorname {argmin}} E_{\\Theta} (x, y)$$ 想要找到最小化能量的翻译句子就会涉及到组合搜索(combinatorial search)，作者训练一个推理网络来近似这种组合搜索。其想法就是：用一个被训练来生成近似最优预测的网络的输出来代替测试时的组合搜索，尤其是被应用在结构化预测中的组合搜索。而这个推理网络 $A_{\\Psi}$ 需要将输入序列 $x$ 映射为翻译序列 $y$，其训练目标函数为 $A_{\\Psi} \\approx \\underset{y}{\\operatorname {argmin}} E_{\\Psi} (x, y)$，而推理网络参数 $\\Psi$ 的训练方法如下：$$\\large \\hat{\\Psi} = \\underset{\\Psi}{\\operatorname {argmin}} \\sum_{\\left\\langle x, y \\right\\rangle \\in \\mathcal{D}} E_{\\Theta} (x, A_{\\Psi} (x))$$ 值得注意的是，推理网络 $A_{\\Psi}$ 的模型结构可以不同于 Energy Function 的模型结构，因此，作者将 NAT 模型作为推理网络，而 AT 模型作为功能函数。其目标就是：兼取 AT 模型的高质量和 NAT 模型的低时延。 Energy Function为了能够对推理网络的参数 $\\Psi$ 进行基于梯度的优化，作者将 $y$ 定义为翻译句子上的词分布序列，具体形式如下：$$\\large E_{\\Theta} (x, y) = \\sum_{t=1}^{\\mid y \\mid} e_t (x, y)$$$$\\large e_t (x, y) = - y_t^T \\log p_{\\Theta} (\\ \\dot\\ \\mid y_0, y_1, \\dots, y_{t-1}, x)$$上式中的 $p_{\\Theta} (\\ \\dot\\ \\mid y_0, y_1, \\dots, y_{t-1}, x)$ 表示词上的完整分布。 通过使用独热分布来代替单词，作者又恢复了原始能量；而为了训练一个推理网络以最小化这种能量，作者所需要的一个能够生成词分布序列的网络结构，NAT 模型则恰好满足。因为原始能量所涉及的分布是独热的，对推理网络来说，输出独热或近似独热的分布也是有利的。作者将推理网络视作生成一个长度为 $T$ 的预测向量 $z_t$ 序列的模块，然后再考虑两个操作 $O_1$ 和 $O_2$，它们会被用于将这些预测向量映射成词分布以用于计算能量。模型总览如下图所示： Operators如上所述，作者需要两个操作 $O_1$ 和 $O_2$ 来管理推理网络和能量函数之间的接口，其中：$O_1$ 需要调节推理网络输出的预测向量 $z_t$，然后将其喂给能量函数中的解码器；$O_2$ 需要决定如何使用词分布 $p_{\\Theta}$ 来计算单词 $y$ 的对数概率。最终，能量函数的局部被改写为如下形式：$$\\large e_t (x, y) = - O_2 (z_t)^T \\log p_{\\Theta} (\\ \\dot\\ \\mid O_1 (z_0), O_1 (z_1), \\dots, O_1 (z_{t-1}), x)$$ 通过实验比较，作者最终将 $Softmax$ 作为 $O_1$ 操作，而将 $Straight-Through$ 作为 $O_2$ 操作。 Results 通过实验，作者得出一下结论： 能量函数加上推理网络始终优于使用蒸馏数据集的 NAT 模型； 10 次迭代后，ENGINE 在 WMT16 Ro-En 数据集上优于 CMLM； 1 次迭代的情况下，ENGINE 优于很多其他的迭代式 NAT 模型。","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation","slug":"Rejuvenating-Low-Frequency-Words-Making-the-Most-of-Parallel-Data-in-Non-Autoregressive-Translation","date":"2021-12-05T06:13:23.000Z","updated":"2021-12-05T06:14:44.980Z","comments":true,"path":"2021/12/05/Rejuvenating-Low-Frequency-Words-Making-the-Most-of-Parallel-Data-in-Non-Autoregressive-Translation/","link":"","permalink":"http://example.com/2021/12/05/Rejuvenating-Low-Frequency-Words-Making-the-Most-of-Parallel-Data-in-Non-Autoregressive-Translation/","excerpt":"虽然，数据蒸馏(Knowledge Distillation)被广泛地用于构造合成数据来训练 NAT 模型，但是蒸馏数据集和原始数据集在低频词上存在着不一致，进而导致了 NAT 模型预测低频词时会出现更多的错误。 换言之，知识蒸馏可能会丢失原始数据中的某种重要信息，从而导致了预测低频词时出现更多错误。针对此，Ding 等人提出的方法严重依赖于外部资源和人工先验知识，这就限制了其应用的范围。","text":"虽然，数据蒸馏(Knowledge Distillation)被广泛地用于构造合成数据来训练 NAT 模型，但是蒸馏数据集和原始数据集在低频词上存在着不一致，进而导致了 NAT 模型预测低频词时会出现更多的错误。 换言之，知识蒸馏可能会丢失原始数据中的某种重要信息，从而导致了预测低频词时出现更多错误。针对此，Ding 等人提出的方法严重依赖于外部资源和人工先验知识，这就限制了其应用的范围。 作者利用预训练将原始数据直接暴露给 NAT 模型而不需要大量修改模型体系结构。此外，作者还根据两个对齐方向来分析蒸馏数据中的双向连接(对齐关系)，进而发现知识蒸馏使得源低频词词更确定地对齐到目标词，而目标低频词则难以对齐源端词语。作者将其归结于知识蒸馏所造成的信息缺失，并基于上述问题提出了逆知识蒸馏(reverse KD)来召回目标低频词的对齐，之后又将两种蒸馏数据拼接起来以同时让模型拥有确定性知识和低频词信息。为了充分利用原始数据和合成数据，作者最终将原始预训练、双语蒸馏训练、知识蒸馏微调这三个互补的方法结合起来以进一步提升 NAT 模型的性能表现。 Pretraining with Raw Data $\\overrightarrow{KD}$ 能够减少训练数据上的模式从而减低内在不确定性和学习难度，这就使得 NAT 模型学习起来更加简单； $\\overrightarrow{KD}$ 加剧了训练数据中高频词和低频词间的不平衡并且还会导致某种重要信息的缺失。Ding 等人也揭示了 $\\overrightarrow{KD}$ 的副作用：蒸馏数据集引起了 NAT 模型在低频词上的词汇选择错误。 预训练可以迁移知识和数据分布从而提升模型的鲁棒性，作者希望能够将损失信息的分布(低频词)迁移到翻译模型中去，因此提出了如下的预训练方法： raw data + pretraining 在原始数据集上预训练 NAT 模型，这是因为原始数据能够保持数据的原始分布，尤其是针对那些低频词。尽管对于 NAT 来说是困难的，但是模型通过预训练能够获得真实数据的通用知识，这或许将有助于模型更好更快地去学习后续任务。作者选择在模型获得了原始数据最佳 BLEU 得分的 90% 时就早停预训练。 distill data + continuous training 在蒸馏数据集上持续训练 NAT 模型，这是为了保持低模(low-modes)的优势。 Bidirectional Distillation Training$\\overrightarrow{KD}$ 通过将高频词代替低频词简化了训练数据，这虽然促成了更简单的源端到目标端对齐，从而导致了双语覆盖率高。但作者认为 $\\overrightarrow{KD}$ 使得目标低频词没什么机会去对齐到源单词。为了验证上述想法，他们提出了一个定量分析双向双语连接的方法。 实验结果表明：1. 在 source 到 target 方向上，$\\overrightarrow{KD}$ 数据召回了更多 LFW 对齐关系，并且其对齐准确率较高。因此从双语对齐来看，$\\overrightarrow{KD}$ 对于 NAT 模型是有效的。2. 在 target 到 source 方向上，$\\overrightarrow{KD}$ 数据召回了更少的 LFW 对齐关系，而且对齐准确率较低。因此 $\\overrightarrow{KD}$ 会因目标低频词的缺失而损害 NAT 模型。 基于上述的实验结果，使用逆知识蒸馏是一个自然而然的想法；因此，作者提出在双语蒸馏数据集(将两种不同方向的蒸馏数据集拼接起来)上训练 NAT 模型。 Knowledge Distillation $\\overrightarrow{KD}$ 同之前的方法一样，将 AT 模型翻译的目标句作为 ground true。 Reverse Knowledge Distillation $\\overrightarrow{KD}$ 使用一个反向 AT 教师模型来根据目标句子翻译出源句，并将其取代原始的源句。 具体流程如下： 同时用两个 AT 模型来生成正逆蒸馏数据集 拼接这两个数据集为一个蒸馏数据集以互补 在新的蒸馏数据集上训练 NAT 模型 通过上述方法，作者希望：蒸馏数据能够保持低模优势的同时；双语蒸馏能够找回更多的 LFW 对齐连接并且伴随着更高质量的对齐，从而为模型带来总体的性能提升。 Low-Frequency Rejuvenation (LFR)作者将上述的两种方法结合起来以进一步提升模型性能，又考虑到预训练效果和干净微调，故而将结合的流水线作为最佳训练策略，即 **$Raw \\Rightarrow \\overrightarrow{KD} + \\overleftarrow{KD} \\Rightarrow \\overrightarrow{KD}$**。 Results 作者通过实验得出的结论大概有： 显著且普遍地提升了 NAT 模型的翻译性能 该性能提升主要依赖于低频词翻译准确率的提高 在不同语系的数据集上，依旧大大地提升了 NAT 模型的性能 在跨领域测试时，显示出了 LFR 方法能够增强 NAT 模型的领域鲁棒性 在不同规模的数据集上，始终提升 NAT 模型性能的同时，在大规模数据集上显示出了更好的性能 能够与 Ding et al. 的数据级方法互补","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"Context-Aware Cross-Attention for Non-Autoregressive Translation","slug":"Context-Aware-Cross-Attention-for-Non-Autoregressive-Translation","date":"2021-12-01T14:38:43.000Z","updated":"2021-12-01T14:40:15.007Z","comments":true,"path":"2021/12/01/Context-Aware-Cross-Attention-for-Non-Autoregressive-Translation/","link":"","permalink":"http://example.com/2021/12/01/Context-Aware-Cross-Attention-for-Non-Autoregressive-Translation/","excerpt":"因为 NAT 模型缺失了对目标序列中依赖关系建模的能力，所以其十分依赖于交叉注意力网络。但作者发现 NAT 模型的交叉注意力网络存在着一种局部认知缺陷(localness perception problem)，即该交叉注意力网络难以充分地捕捉到源句上下文中的依赖信息。","text":"因为 NAT 模型缺失了对目标序列中依赖关系建模的能力，所以其十分依赖于交叉注意力网络。但作者发现 NAT 模型的交叉注意力网络存在着一种局部认知缺陷(localness perception problem)，即该交叉注意力网络难以充分地捕捉到源句上下文中的依赖信息。 Localness Perception Problem作者分析得到：缺失自回归分解的缘故，NAT 解码器难以充分地捕捉到源端上下文信息。而 Li 等人则发现：相较于 AT 模型，NAT 模型的交叉注意力分布更加的模糊。这两个发现不谋而合，于是作者构造了实验以证明上述的发现。简言之，$LE$ 表示语料级局部交叉熵损失(locality entropy)，该值越高则表明注意力分布越分散；反之则表示注意力分布越集中。该实验结果如下，不难发现： 模型性能和 $LE$ 呈现负相关； NAT 模型的 $LE$ 明显高于 AT 模型，而作者提出的方法的确能够缓解这种问题； 作者的方法不能将 NAT 模型的 $LE$ 拉低到与 AT 模型接近的水平。 CCAN为了缓解 NAT 的局部认知缺陷，作者提出了一种结合局部和全局交叉注意力的方法。下图为一个例子： Global Cross-Attention 不难发现，原始 NAT 或者 AT 的交叉注意力就是全局的，也就是说解码器端的查询 Query 能够关注到源端的所有位置 如上图则表示，”socializing” 能够关注到 【”弗兰克” “在” “跟” “一个” “女孩” “交往”】。 Local Cross-Attention 在源端对齐的单词 $s_j$ 周围构造一个窗口，那么该窗口就是局部注意力能够关注到的范围。而对齐的话，作者将原始注意力权重最高的单词作为对齐单词，这样就无需引入外部对齐信息。 在此之上，作者进一步提出了一个插值门机制以将全局注意力和局部注意力结合起来。通过一个权重 $g = \\sigma (W Q_i)$ 来控制全局和局部的比例。 值得注意的是，唯一引入的额外参数只有上述的 $W$，它会基于解码器端查询 Query 来估计全局交叉注意力的重要性，从而将两个注意力结合起来。作者让每个注意力头都共享这个参数，这样就又能缩减模型参数量了。 如上图则表示，”dating” 只能关注到 【”一个” “女孩” “交往”】。 Results 作者通过实验得到了如下结论： CCAN 能够有效地提升模型性能； CCAN 通过局部和全局注意力更好地利用了源端上下文信息； CCAN 强化了 NAT 模型学习句法和语义信息的能力。","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"LAT-NAT","slug":"LAT-NAT","date":"2021-12-01T05:16:03.000Z","updated":"2021-12-01T05:17:29.252Z","comments":true,"path":"2021/12/01/LAT-NAT/","link":"","permalink":"http://example.com/2021/12/01/LAT-NAT/","excerpt":"NAT 模型能够完全并行化，但是其翻译质量却下跌地厉害，为此之前的学者提出了迭代式解码策略来提高模型性能。本文作者通过将局部自回归翻译(local autoregressive translation)机制融入到非自回归翻译模型中去从而实现了目标句子中局部依赖关系的捕捉能力，因而提高了 NAT 模型的性能。","text":"NAT 模型能够完全并行化，但是其翻译质量却下跌地厉害，为此之前的学者提出了迭代式解码策略来提高模型性能。本文作者通过将局部自回归翻译(local autoregressive translation)机制融入到非自回归翻译模型中去从而实现了目标句子中局部依赖关系的捕捉能力，因而提高了 NAT 模型的性能。 **对于每一个解码位置 $i$ 而言，作者不再预测单个 token $t_i$，而是通过局部翻译器来翻译出长度为 $K$ 的翻译片段(translation piece)**，如下图所示(K=3)： 不难发现，这样就会导致后续位置上出现多个预测 tokens 的情况。如上图，$i=2$ 位置上的预测 tokens 包括了 ${like, like}$，而 $i=3$ 的位置上预测 tokens 包括了 ${birds, birds, birds}$。作者正是利用这一特性，采取了一种对齐归并算法来将所有的翻译片段归并成为完全的翻译输出。简言之，归并算法通过逐步地对齐和归并邻接翻译片段来构造输出结果，最终的结果则是由归并算法动态地决定，这也就意味着模型对于翻译长度具有更高的灵活性。作者将这种局部自回归翻译机制搭建在 CMLM 模型之上，并且也采用了迭代式解码策略来提升模型性能。 Model在 CMLM 解码器输出之上，作者搭建了一个轻量级的 LSTM 顺序解码器作为局部翻译器(local translator)来自回归地生成翻译片段。具体地，对于某个目标端位置 $i$，CMLM 解码器会生成一个隐藏表示 $pos_i$，随后局部翻译器基于该表示自回归地预测出一个翻译片段 $[t_i^1, t_i^2, \\dots, t_i^K]$。此处的 $K$ 是一个超参数以控制翻译片段的长度，而出于推理速度的考虑，作者将其设置为 3。 推理期间，特殊标记 被喂给局部翻译器作为自回归翻译的初始标记，当所有翻译片段生成结束后，作者再使用归并算法将其归并成一个完全的翻译输出。上文说到本模型也采取了迭代式解码，而作者完全遵循 Mask-Predict 的迭代方式。但除此之外，作者将特殊标记 喂给编码器以预测目标端的长度 $N$，但值得注意的是归并算法能够动态地规划长度，因此该模型对长度并没有那么敏感。 模型训练则主要分为如下几步： 从 $[1, N]$ 上的均匀分布中采样 mask 的数量； 随机地选择翻译单词进行 mask； 对于每个目标位置，作者都采用 teacher-forcing styled 训练方法来手机预测翻译片段的交叉熵损失 (作者简单地通过顺延方法来构造翻译片段的 ground true)。 最终作者将所有位置都考虑在损失内，只不过那些 unmasked 位置上的损失权重较小，这样模型还是能够主要处理 masked 单词预测。其 token 预测损失如下 (其中，unmasked 单词的损失权重 $\\alpha$ 被设置为了 0.1)：$$\\Large \\mathcal{L} = - \\sum_{i=1}^N \\sum_{j=1}^K \\mathbb{1} \\big{ t_i^j \\in T_{mask} \\big} \\log \\big( p(t_i^j) \\big) - \\sum_{i=1}^N \\sum_{j=1}^K \\mathbb{1} \\big{ t_i^j \\not\\in T_{mask} \\big} \\alpha \\log \\big( p(t_i^j) \\big)$$ 除此之外，作者还通过随机地删除输入序列中的某些位置来使模型学习如何进行插入式操作。模型训练的最终损失为 token 预测损失和 length 预测损失之和：$$\\large \\mathcal{L}{model} = \\mathcal{L}{tokens} + \\mathcal{L}_{length}$$ Merging Algorithm如上所述，作者采用一个归并算法来逐段渐进地构造输出。该算法基于如下假设： 如果局部翻译器是 well-trained 的话，那么 其自回归翻译片段也应该是 well-translated 并且具备流畅性； 各个翻译片段之间存在着重叠的单词，而这些词可以作为归并时的对齐点。 首先，通过下图的例子来说明归并算法是如何归并两个邻接翻译片段 $s1$ 和 $s2$ 的： 使用最长共同子序列(LCS)算法找出两个翻译片段之间的对齐点(如上假设，所找到的最长共同子序列应该承担着作为归并对齐点的任务)，也就是上图中的 study； 如果没有匹配到对齐点，那么就简单地将两个翻译片段拼接起来； 否则，找到的对齐点至多划分出两个冲突段中段，即上图的 going to &amp; will 和 here &amp; in the； 比较两个段中段的置信分，将高者作为冲突结果复制到归并翻译中去，即上图中的 going to 和 in the； 当处理完所有的冲突段中段后，归并翻译就构造完成了。 上述过程中，冲突段中段的置信分该如何定义呢？作者将每个预测 token 的对数概率作为其模型得分，然后简单地将段中段内所有 tokens 得分的均值作为段中段的置信分。还有一个问题，那就是如果冲突段中段的一个段中段为空时，其置信分是否直接为0呢，作者将该值设置为一个恒定值 $\\log 0.25$，因为如此就能让模型(归并算法)学习到删除式操作了。 对于所有的翻译片段，作者自左至右地逐段归并翻译片段；而在每个归并操作中，作者也只对 $2K$ 个 tokens 进行归并($s1$ 有 $K$ 个，$s2$ 也有 $K$ 个)，这样能够保证归并的局部性从而减轻错误对齐点的负面影响。 这样的归并算法是能够直接应用在迭代解码过程中的，但因为归并算法可以动态地调节翻译长度，所以作者进一步采用了一种中间迭代长度调整策略来缓解上述问题。简言之，通过增加或删除 标记的数量就可以保证归并后的翻译长度不会偏离预测长度太多。 值得注意的是，归并算法整体是类似于自回归的方式，但其并不涉及神经计算，所以并不会带来很多速度损耗，但该算法的动态长度特性(选择空段中段，即删除；选择更长的段中段，即插入)还能使得模型输出更具灵活性。 Results作者在 WMT14 En-De、WMT16 En-Ro 以及 IWSLT14 De-En 数据集上进行了实验，其主要结果如下图所示： 作者得出的结论有： LAT-NAT 模型能够显著地提升 CMLM 基线模型的性能； LAT-NAT 模型能够显著地减少 CMLM 基线模型的迭代； LAT-NAT 模型能够更好的泛化到更长的句子上去。","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"RewriteNAT","slug":"RewriteNAT","date":"2021-11-30T08:16:23.000Z","updated":"2021-11-30T08:18:14.155Z","comments":true,"path":"2021/11/30/RewriteNAT/","link":"","permalink":"http://example.com/2021/11/30/RewriteNAT/","excerpt":"NAT 模型因其性能的衰减而无法与 AT 模型同日而语，CMLM 提出了一种 conditional mask language model 来缓解 NAT 模型的性能衰减问题。实际上，CMLM 将 Heuristic Mask 和 Iterative Predict 相结合以实现模型性能的提升，但是这其中最大的问题就在于所谓的启发式规则是否真的能够有效地定位到错误单词。简言之，如果迭代时纠正的错误单词并不错误，那么整个模型的性能不仅无法提高而且会造成推理速度的衰减。而之前所提出的一些迭代式 NAT 模型已经暴露出了其识别错误单词能力不足的问题，因此作者设计了 RewriteNAT 模型以克服这个问题。","text":"NAT 模型因其性能的衰减而无法与 AT 模型同日而语，CMLM 提出了一种 conditional mask language model 来缓解 NAT 模型的性能衰减问题。实际上，CMLM 将 Heuristic Mask 和 Iterative Predict 相结合以实现模型性能的提升，但是这其中最大的问题就在于所谓的启发式规则是否真的能够有效地定位到错误单词。简言之，如果迭代时纠正的错误单词并不错误，那么整个模型的性能不仅无法提高而且会造成推理速度的衰减。而之前所提出的一些迭代式 NAT 模型已经暴露出了其识别错误单词能力不足的问题，因此作者设计了 RewriteNAT 模型以克服这个问题。 对标 CMLM 模型首先，作者列举了 CMLM 模型的三宗罪： 识别错误单词的能力不足：CMLM 模型可能会保留错误单词而重写正确单词； 迭代造成了推理速度下降：在上述条件下的迭代会损害 NAT 与生俱来的高推理速度而没有什么性能提升； 训练与推理之间的不一致：训练时的目标端能够偷看到很大一部分 ground true 单词，而推理时却不能。 基于上述的缺点，作者采用了以下措施来进一步改进 NAT 模型，从而提出了 RewriteNAT 模型： 使用 revisor 模块和 locator 模块来代替 CMLM 的 Mask-Predict 策略； 使用动态止停策略来动态地结束迭代而不是永远恒定迭代次数； 使用 ground true 来监督上述的 revisor 模块和 locator 模块。 我们可以先对比一下 RewriteNAT 模型与 CMLM 模型在大体结构上的差异，如下图： 不难发现，主要的区别就是启发式规则被一个 locator 模块给代替了，而这个 locator 似乎能够更准确地捕捉到错误单词。在两个模型的右下角，我们又可以知道 locator 模块会基于编码器输出，或者是源句上下文来进行判断，这在直觉上是要比启发式规则强很多。 RewriteNATRewriteNAT 模型总体上由三部分组成： encoder：编码器和其他的 MT 模型编码器一样，负责将输入序列编码成上下文化的隐藏表示； revisor：重写器则类似于 CMLM 的解码器，负责将 masked 单词重新预测以生成正确的单词； locator：定位器则取代了 CMLM 的启发式规则，负责定位出 revisor 已生成的偏翻译中的错误单词。 Revisorrevisor 会基于源句上下文将 masked 单词重写改正；初始时，它会接收一个仅由 [MASK] 组成的输入序列，即重写输入序列上的每一个单词。具体流程如下： 给定前一迭代步 t-1 时 locator 所生成的偏翻译 $Y_{t-1}^r$，首先通过 transformer 块堆栈基于源句表示 $H^e$ 来生成出其对应的隐藏表示 $H_t^r$； 随后，包含着 [MASK] 标记的隐藏表示 $H_t^r$ 会被喂给 revisor classifier $\\pi^l$ 以生成翻译单词，而这些新生成的翻译单词会取代 [MASK] 标记，从而组成一个新的偏翻译 $Y_t^l$； 最后，生成的偏翻译 $Y_t^l$ 会被喂给 locator 模块。 Locatorlocator 会基于源句上下文来辨别整个偏翻译中的错误单词；由于它在 revisor 模块后面，所以它总是接收 revisor 输出的偏翻译即可。对于给定的偏翻译，locator 需要为其中的每一个单词都标注上标签；若该标签为 0 (keep)，那么这个单词在输出的偏翻译中将被保留；若该标签为 1 (revise)，那么这个单词在输出的偏翻译中将会被 [MASK] 标记所取代。其具体流程如下： 给定当前迭代步 t 时 revisor 所生成的偏翻译 $Y_t^l$，首先通过 transformer 块堆栈基于源句表示 $H^e$ 来生成出其对应的隐藏表示 $H^l$； 随后，将该隐藏表示喂给 locator classifier $\\pi^r$ 以生成对应的标注序列，如上将被标注 revise 的单词以 [MASK] 代替，从而组成一个新的偏翻译 $Y_t^r$； 最后，生成的偏翻译 $Y_t^r$ 会喂给 revisor 模块。 下面是一个 RewriteNAT 解码的例子，假设源句输入为 “Thank you .”；目标翻译为 “Vielen Dank .”；迭代次数为 2，则其流程如下图所示： Training训练期间，作者为这两个模块定义了两种不同的监督信号以使其能够更准确地定位并重写错误单词： revisor 的监督信号 $q(\\hat{Y}_t^r)$ $q(\\hat{Y}_t^r)$ 是一个权重向量，它将那些 masked 位置权重置为 1，其余的则置为 0；也就是说该信号旨在优化 revisor 在 masked 单词上的重写损失，其定义如下： $$ \\large q_i(\\hat{Y}t^r) = 1\\ \\ if\\ \\ \\hat{Y}{t_i}^r == [MASK]\\ \\ else\\ \\ 0 $$ locator 的监督信号 $z(\\hat{Y}_t^l)$ $z(\\hat{Y}_t^l)$ 向量则是统计了偏翻译对比 ground true 而言的错误单词位置，也就是说该信号旨在优化 locator 在整个偏翻译序列上的定位损失，其定义如下： $$ \\large z_i(\\hat{Y}t^l) = 1\\ \\ if\\ \\ \\hat{Y}{t_i}^l \\ne Y_i\\ \\ else\\ \\ 0 $$ 作者通过上述的两个监督信号来指导 revisor 和 locator 更加精确地去学习如何重写和定位错误单词，从而进一步提升模型性能，具体的训练目标函数如下：$$\\Large \\mathcal{L}(\\theta) = \\sum_{m=1}^M \\Big{ q(\\hat{Y}t^r) \\log \\pi{\\theta}^l (Y \\mid \\hat{Y}t^r, X) + \\log \\pi{\\theta}^r (z(\\hat{Y}_t^l) \\mid \\hat{Y}_t^l, X) \\Big}$$ Inference在训练期间，RewriteNAT 必须翻译出和 ground true 一样长的偏翻译才能够比较得出上述的两个监督信号；但在推理期间，RewriteNAT 无法预先得知目标序列的长度。因此，作者选择使用一个长度分类器来预测目标端长度，而且还能够实现长度束搜索解码。 除此之外，作者采用动态止停技术来使得 RewriteNAT 模型具备动态迭代能力从而缓解迭代损害速度问题。具体来说，当 locator 为整个偏翻译都标注上 keep 或者 revisor 无法再生成新单词时，迭代解码结束。当然，为了避免模型无休止地迭代，作者还设定了一个迭代上限。 Results作者在 WMT14 En-De 和 WMT16 En-Ro 数据集上的实验结果如下所示： 简单来说，作者得到了以下的模型优点： 模型取得了 SOTA 的 BLEU 得分； 模型大大降低了解码时迭代的次数； 模型对于长句子的泛化能力更强。","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"MHPLSTM","slug":"MHPLSTM","date":"2021-11-29T14:00:57.000Z","updated":"2021-11-29T14:03:36.353Z","comments":true,"path":"2021/11/29/MHPLSTM/","link":"","permalink":"http://example.com/2021/11/29/MHPLSTM/","excerpt":"自注意力网络因其序列级并行化能力而风靡，但值得注意的是 SAN 的计算复杂度是 $O(n^2)$，而 LSTM 的计算复杂度则只有 $O(n)$。而之所以 LSTM 远慢于 SAN 是因为每一时刻 LSTM 的状态都依赖于前一时刻的输出，从而使得其不得不顺序地进行 $n$ 次的计算 (假设输入序列长度为 $n$)。而在这些顺序计算中，LSTM 的 gate 计算和 state 计算是通过矩阵乘法实现的线性变换，其他则是由一些元素级运算实现，因此 LSTM 的 gate 和 state 计算是造成其速度过慢的主要矛盾。","text":"自注意力网络因其序列级并行化能力而风靡，但值得注意的是 SAN 的计算复杂度是 $O(n^2)$，而 LSTM 的计算复杂度则只有 $O(n)$。而之所以 LSTM 远慢于 SAN 是因为每一时刻 LSTM 的状态都依赖于前一时刻的输出，从而使得其不得不顺序地进行 $n$ 次的计算 (假设输入序列长度为 $n$)。而在这些顺序计算中，LSTM 的 gate 计算和 state 计算是通过矩阵乘法实现的线性变换，其他则是由一些元素级运算实现，因此 LSTM 的 gate 和 state 计算是造成其速度过慢的主要矛盾。 不难看出顺序的线性变换计算问题是问题的关键，因此为了使得 LSTM 能够获得序列级并行化能力，作者通过使用当前输入的嵌入表示和之前输入的词袋表示来计算 gate 和 state 以近似完全的 LSTM 上下文建模。这使得 LSTM 能够并行地计算每一个输入步，从而避免了笨拙的顺序线性变换问题，该模型被作者称为 Highly Parallelized LSTM (简称 HPLSTM)。除此之外，作者类似于 Transformer 中的多头注意力网络那样将整个 HPLSTM 进一步分解成多个更小的 HPLSTM，从而实现进一步的并行化能力以及模型参数量的有效缩减。 LN-LSTM首先，回顾一下层归一化增强的 LSTM 模型的工作流程，如下图所示： 首先，需要强调的是 LN-LSTM 在解码时获得了比 Transformer 更好的性能表现，而本文的模型就是基于 LN-LSTM 而设计的。LN-LSTM 的处理流程大概如下： 将前一时刻的输出表示 $o^{t-1}$ 和当前时刻的输入表示 $i^t$ 拼接成当前输入 $v^t$：$$\\large v^t = i^t \\mid o^{t-1}$$ 通过当前输入数据 $v^t$ 来计算输入门 $i_g^t$、输出门 $o_g^t$、遗忘门 $f_g^t$ 以及当前隐藏表示 $h^t$：$$\\large i_g^t = \\sigma(LayerNorm(W_i \\cdot v^t + b_i))$$$$\\large o_g^t = \\sigma(LayerNorm(W_o \\cdot v^t + b_o))$$$$\\large f_g^t = \\sigma(LayerNorm(W_f \\cdot v^t + b_f))$$$$\\large h^t = \\alpha(LayerNorm(W_h \\cdot v^t + b_h))$$ 通过输入门 $i_g^t$ 和遗忘门 $f_g^t$ 以及当前隐藏表示 $h^t$ 来更新记忆单元 $cell$ 中的内容：$$\\large c^t = c^{t-1} * f_g^t + h^t * i_g^t$$ 最后通过输出门 $o_g^t$ 和当前记忆单元内容 $c^t$ 来得到当前时刻的输出表示 $o^t$：$$\\large o^t = c^t * o_g^t$$ HPLSTM 使用累加运算(cumulative sum operation)求出**输入序列的词袋表示(bag-of-words)并令 $s^1 = [0, \\dots, 0] $**：$$\\large s^t = \\sum_{k=1}^{t-1} i^k$$ **对词袋表示进行层归一化以防止出现梯度爆炸问题，然后将其代替上述的 $o^{t-1}$ 与输入表示 $i^t$ 拼接起来获得 $v^t$**：$$\\large v^t = i \\mid LayerNorm(s^t)$$ 使用当前输入数据 $v^t$ 来计算出输入门 $i_g^t$、遗忘门 $f_g^t$：$$\\large i_g^t = \\sigma(LayerNorm(W_i \\cdot v^t + b_i))$$$$\\large f_g^t = \\sigma(LayerNorm(W_f \\cdot v^t + b_f))$$ 使用两层前馈网络来计算隐藏状态 $h^t$ 以缓解词袋表示的权重均等问题：$$\\large h^t = W_{h2} \\alpha(LayerNorm(W_{h1} \\cdot v^t + b_{h1})) + b_{h2}$$ 使用输入门 $i_g^t$ 和遗忘门 $f_g^t$ 以及隐藏表示 $h^t$ 来更新记忆单元内容：$$\\large c^t = c^{t-1} * f_g^t + h^t * i_g^t$$ **由于词袋表示远远不如当前记忆单元中所存储的隐藏表示，因此将记忆表示 $c^t$ 和输入表示 $i$ 拼接以获得输出门 $o_g^t$**：$$\\large o_g^t = \\sigma(LayerNorm(W_o \\cdot (i^t \\mid c^t) + b_o))$$ 最后输出门 $o_g^t$ 控制当前位置 t 上的输出表示 $o^t$：$$\\large o^t = c^t * o_g^t$$ 此过程中，虽然词袋表示 $s$ 不如原始 LSTM 中所依赖的 $o$，但是它使得模型获得了序列级并行化能力，而且上述过程中采用了两个手段来缓解词袋表示所带来的性能影响。此外，除了 $cell$ 的计算是顺序的，其他都是序列级并行化处理，而 $cell$ 的计算是由元素级运算实现的，因此 HPLSTM 模型的训练和解码速度可以得到大大提升。 Multi-Head HPLSTM上述的方法虽然带来的并行化能力，但是也使得 HPLSTM 模型涉及更多的参数量和计算量，为了限制模型的参数量，作者效仿 Multi-Head 将整个 HPLSTM 模型分成 $n$ 个更小的 HPLSTM 模型来并行地进行计算。 通过线性变换将输入数据 $i$ 映射到 $n$ 个不同的嵌入空间中去，并将变换后的输入分割成 $n$ 个段：$$\\large i_1 \\mid i_2 \\mid \\dots \\mid i_n = W_s \\cdot i + b_s$$ 将第 k 段的表示 $i_k$ 喂给对应的 HPLSTM 子网络，并经过网络处理后得到对应输出 $o_k$：$$\\large o_k = HPLSTM_k (i_k)$$ 将第 1 段到第 k 段的输出结果拼接起来后，对其再施加一次线性变换：$$\\large o = W_m \\cdot (o_1 \\mid o_2 \\mid \\dots \\mid o_n) + b_m$$ ExperimentsMHPLSTM 解码器模型不仅显著地提高了 BLEU 得分，而且大大加快了训练和解码速度。 除此之外，作者还发现： MHPLSTM 并不适合承担 encoder 这一角色； MHPLSTM 的长程建模能力远不如 Self-Attention (当长度大于 15 后)； MHPLSTM 解码器中的 FFN 层也起着一定的作用，不能去除。","categories":[{"name":"Neural Machine Translation","slug":"Neural-Machine-Translation","permalink":"http://example.com/categories/Neural-Machine-Translation/"}],"tags":[{"name":"NMT","slug":"NMT","permalink":"http://example.com/tags/NMT/"}]},{"title":"DSLP","slug":"DSLP","date":"2021-11-27T12:14:39.000Z","updated":"2021-11-27T13:03:24.841Z","comments":true,"path":"2021/11/27/DSLP/","link":"","permalink":"http://example.com/2021/11/27/DSLP/","excerpt":"之前拜读了 “Non-Autoregressive Translation with Layer-Wise Prediction and Deep Supervision” 这篇论文，最近发现它开源了代码，因此进行了如下实践。","text":"之前拜读了 “Non-Autoregressive Translation with Layer-Wise Prediction and Deep Supervision” 这篇论文，最近发现它开源了代码，因此进行了如下实践。 过程源码 可以直接在 DSLP 仓库打包下载 也可以使用 wget 下载 环境首先，进入 DSLP 文件目录下，然后依次执行如下指令： 123456pip install -e .pip install tensorflow tensorboard sacremoses nltk Ninja omegaconfpip install &#x27;fuzzywuzzy[speedup]&#x27;pip install hydra-core==1.0.6pip install sacrebleu==1.5.1pip install git+https://github.com/dugu9sword/lunanlp.git 然后，就需要配置 ctcdecode 了，如果条件允许的话，直接运行如下代码： 12git clone --recursive https://github.com/parlance/ctcdecode.gitcd ctcdecode &amp;&amp; pip install . 如果你出现如下问题，那么就算网再好，也不可能安装好的： 1234567891011121314151617181920212223242526Traceback (most recent call last): File &quot;setup.py&quot;, line 26, in &lt;module&gt; download_extract( File &quot;setup.py&quot;, line 16, in download_extract urllib.request.urlretrieve(url, dl_path) File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/urllib/request.py&quot;, line 247, in urlretrieve with contextlib.closing(urlopen(url, data)) as fp: File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/urllib/request.py&quot;, line 222, in urlopen return opener.open(url, data, timeout) File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/urllib/request.py&quot;, line 525, in open response = self._open(req, data) File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/urllib/request.py&quot;, line 542, in _open result = self._call_chain(self.handle_open, protocol, protocol + File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/urllib/request.py&quot;, line 502, in _call_chain result = func(*args) File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/urllib/request.py&quot;, line 1397, in https_open return self.do_open(http.client.HTTPSConnection, req, File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/urllib/request.py&quot;, line 1358, in do_open r = h.getresponse() File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/http/client.py&quot;, line 1348, in getresponse response.begin() File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/http/client.py&quot;, line 316, in begin version, status, reason = self._read_status() File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/http/client.py&quot;, line 285, in _read_status raise RemoteDisconnected(&quot;Remote end closed connection without&quot;http.client.RemoteDisconnected: Remote end closed connection without response 请按照如下步骤进行： 打开 google colab 并创建一个新的 notebook，运行如下代码： 12git clone --recursive https://github.com/parlance/ctcdecode.gitcd ctcdecode &amp;&amp; pip install . 然后按照它输出的提示信息，找到如下一个目录和两个文件： 123ctcdecode/~/third_party/openfst-1.6.7.tar.gz~/third_party/boost_1_67_0.tar.gz 将这两个文件添加到 ctcdecode/third_party 目录下，然后下载整个 ctcdecode 目录： 1ThreadPool kenlm utf8 openfst-1.6.7.tar.gz boost_1_67_0.tar.gz 将 ctcdecode 上传至 DSLP 目录下，运行如下指令： 12cd ctcdecodepip install . 数据fairseq 提供了蒸馏数据集，我们直接下载即可 运行 Preprocess 123456789TEXT=wmt14_ende_distillpython3 fairseq_cli/preprocess.py --source-lang en --target-lang de \\ --trainpref $TEXT/train.en-de \\ --validpref $TEXT/valid.en-de \\ --testpref $TEXT/test.en-de \\ --destdir data-bin/wmt14.en-de_kd \\ --workers 40 \\ --joined-dictionary Run 123456789101112131415161718192021222324252627282930313233343536373839404142434445python3 train.py data-bin/wmt14.en-de_kd --source-lang en --target-lang de \\ --save-dir checkpoints --eval-tokenized-bleu \\ --keep-interval-updates 5 --save-interval-updates 500 \\ --validate-interval-updates 500 \\ --maximize-best-checkpoint-metric \\ --eval-bleu-remove-bpe \\ --eval-bleu-print-samples \\ --best-checkpoint-metric bleu \\ --log-format simple \\ --log-interval 100 \\ --eval-bleu \\ --eval-bleu-detok space \\ --keep-last-epochs 5 \\ --keep-best-checkpoints 5 \\ --fixed-validation-seed 7 \\ --ddp-backend=no_c10d \\ --share-all-embeddings \\ --decoder-learned-pos \\ --encoder-learned-pos \\ --optimizer adam \\ --adam-betas &quot;(0.9,0.98)&quot; \\ --lr 0.0005 \\ --lr-scheduler inverse_sqrt \\ --stop-min-lr 1e-09 \\ --warmup-updates 10000 \\ --warmup-init-lr 1e-07 \\ --apply-bert-init \\ --weight-decay 0.01 \\ --fp16 \\ --clip-norm 2.0 \\ --max-update 300000 \\ --task translation_glat \\ --criterion glat_loss \\ --arch glat_sd \\ --noise full_mask \\ --concat-yhat \\ --concat-dropout 0.0 \\ --label-smoothing 0.1 \\ --activation-fn gelu \\ --dropout 0.1 \\ --max-tokens 8192 \\ --glat-mode glat \\ --length-loss-factor 0.1 \\ --pred-length-offset Evaluation 12345678910fairseq-generate data-bin/wmt14.en-de_kd --path checkpoints \\ --gen-subset test \\ --task translation_lev \\ --iter-decode-max-iter 0 \\ --iter-decode-eos-penalty 0 \\ --beam 1 \\ --remove-bpe \\ --print-step \\ --batch-size 100","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"Connectionist Temporal Classification","slug":"Connectionist-Temporal-Classification","date":"2021-11-26T07:21:26.000Z","updated":"2021-11-26T07:24:49.772Z","comments":true,"path":"2021/11/26/Connectionist-Temporal-Classification/","link":"","permalink":"http://example.com/2021/11/26/Connectionist-Temporal-Classification/","excerpt":"CTC 的提出就是旨在解决输入和输出之间没有先验对齐的情况。 本文来自Sequence Modeling With CTC，非常好的文章，中文是我自己翻译，如有错误还望赐教！","text":"CTC 的提出就是旨在解决输入和输出之间没有先验对齐的情况。 本文来自Sequence Modeling With CTC，非常好的文章，中文是我自己翻译，如有错误还望赐教！ 正式地来说，输入序列 $X = [x_1, x_2, \\dots , x_T]$ 和输出序列 $Y = [y_1, y_2, \\dots , y_U]$，我希望找到 $X$ 到 $Y$ 之间的映射。而我们所面对的困难有3点： $X$ 和 $Y$ 的长度都是可变的； $X$ 和 $Y$ 的长度间的比例是可变的； 我们并没有 $X$ 和 $Y$ 之间的词级先验对齐。 CTC 算法能够克服上述的几点缺陷。对于给定的输入序列 $X$，CTC 将会为我们提供所有可能 $Y$ 上的一个输出分布。借助这个输出分布，我们可以推理出一个可能的输出 $Y$ 或者评估某个给定输出的概率值。 对于给定的输入 $X$，我们希望训练我们的模型以最大化正确输出 $Y$ 的概率，而为此我们就需要能够有效的计算条件概率 $p(Y|X)$，不仅如此该条件概率必须是可微的从而能够使用梯度下降(gradient descent)算法来进行优化。 模型训练结束之后，我们将会使用该模型基于给定的 $X$ 来推理出一个可能的输出 $Y$，如贪婪解码 $\\hat{Y} = \\underset{Y}{\\operatorname {argmax}} p(Y \\mid X)$。如果我们使用 CTC 则可以获得一个近似解而不付出太大的代价。 AlignmentCTC 算法不需要输入和输出之间的先验对齐，它通过求和所有可能对齐的概率从而计算得到给定输入下一个输出的概率。首先我们举个例子，假设输入序列 $X$ 的长度为 6，而输出序列 $Y = [c, a, t]$，那么一种可能的对齐方式如下 (每个位置都会有一个预测输出，最终通过删除重复还获得 $Y$)，如下图所示： 这种方法有两个弊端： 一般，强迫每个输入单词都必须对齐某个输出是没有道理可言的，如翻译中的虚词； 我们无法产生连续而又正确的输出序列，如 $Y = [h, e, l, l, o]$。 为了克服上述问题，CTC 引入了一个额外的 token 来表示对应位置上没有任何输出 (或对齐空)，这个 token 称为 blank token，此处用 $\\epsilon$ 来表示。于是，CTC 允许任何能够映射到 $Y$ 的对齐，如下： 让我们再把目光放回到 cat 的例子上，我们就会发现能够映射到 $Y$ 的对齐映射关系存在着多种，下图举出了一些例子： 通过上面的例子，我们可以发现 CTC Alignments 具备几个值得注意的属性： $X$ 和 $Y$ 之间可能的对齐关系是单调的，即我们预测下一个方格时，要么就是与之前的方格预测结果相同，要么就是将结果也推进一位； $X$ 和 $Y$ 之间的对齐关系是多对一映射(Many-to-One Map)，这也就使得输入序列的长度必须不小于输出序列，即 $\\mid X \\mid \\ge \\mid Y \\mid$。 Loss FunctionCTC alignments 为我们提供了一种自然的概率计算方法，如下所示： 准确地讲，CTC 对于单个输入输出对 $(X, Y)$ 的目标函数如下：$$\\Large p(Y \\mid X)\\ \\ =\\ \\ \\sum_{A \\in \\mathcal{A}{X, Y}}\\ \\ \\ \\prod{t=1}^{T} p_t(a_t \\mid X)$$ 其中，$p(Y \\mid X)$ 是 CTC 条件概率；$\\sum_{A \\in \\mathcal{A}{X, Y}}$ 是边际化所有合法对齐映射所组成的集合；$\\prod{t=1}^{T} p_t(a_t \\mid X)$ 是逐个时间步地计算单个对齐映射的概率。我们当然可以直接求和所有合法的对齐映射的概率，但问题在于对齐数量会随着输入序列长度的增加而呈指数式增长，因此这种计算所付出的时间代价太大。万幸的是动态规划能够有效地解决这个问题，其关键在于如果存在两个对齐在同一个时间步预测出了同样的输出，那么我们就可以将它们合并起来： ​ $\\Longrightarrow$ 因为 CTC 在对齐映射中引入了 $\\epsilon$，所以我们自然而然地定义序列 $Z = [\\epsilon, y_1, \\epsilon, y_2, \\dots, \\epsilon, y_U, \\epsilon]$ (即在每个输出 label 之间以及整个序列的首尾都插入一个 $\\epsilon$)。接着，我们将 $\\alpha_{s, t}$ 表示时间步 $t$ 时的偏序列 $Z_{1:s}$ 的 CTC 得分，那么要使用动态规划算法，就得构造出状态转移方程，具体可分为如下情况： 预测重复标签 举上图为例 (显然有 $Z_s = Z_{s-2}$)，我们不能从 $Z_{1:s-2}^{t-1} = \\dots a$ 直接转移到 $Z_{1:s}^{t} = \\dots aa$，因为需要被预测出的重复标签 $aa$ 显然会变成单个标签 $a$。在这种情况下，我们只能从 $Z_{s}^{t-1}$ 或 $Z_{s-1}^{t-1}$ 转移而来 (如上图的两个实线箭头)；因此有下式：$$\\Large \\alpha_{s, t} = (\\alpha_{s-1, t-1} + \\alpha_{s, t-1}) \\cdot p_t(z_s \\mid X)$$ 其中，$\\alpha_{s-1, t-1} + \\alpha_{s, t-1}$ 是时刻 $t-1$ 能够转移到正确结果 $Z_s^t$ 的两个合法偏序列的得分和；$p_t(z_s \\mid X)$ 则是当前标签 $Z_s$ 在时刻 $t$ 上的概率。 预测不同标签 如上图 (显然有 $Z_s \\ne Z_{s-2}$ 且 $Z_{s-1} = \\epsilon$)，此时我们可以从 $Z_{s-2}^{t-1}$ 或 $Z_{s-1}^{t-1}$ 或 $Z_{s}^{t-1}$ 转移而来，因此有下式：$$\\Large \\alpha_{s, t} = (\\alpha_{s-2, t-1} + \\alpha_{s-1, t-1} + \\alpha_{s, t-1}) \\cdot p_t(z_s \\mid X)$$ 其中，$\\alpha_{s-2, t-1} + \\alpha_{s-1, t-1} + \\alpha_{s, t-1}$ 是时刻 $t-1$ 能够转移到正确结果 $Z_s^t$ 的三个合法偏序列的得分和；$p_t(z_s \\mid X)$ 则是当前标签 $Z_s$ 在时刻 $t$ 上的概率。 结合上述的两种情况，假定 $Z = [\\epsilon, a, \\epsilon, b, \\epsilon]$，那么动态规划得到的所有合法对齐映射路径 (一个对齐映射在图中表现为一条从起始节点到终止节点的路径) 如下图所示： 从上图中不难发现，起始节点有两个 ($\\epsilon$ 或 $a$)，而终止节点亦有两个 ($b$ 或 $\\epsilon$)，那么整个所有可能对齐映射的概率和就只需要将两个终止节点 ($b$ 或 $\\epsilon$) 的得分 $\\alpha$ 相加即可，即：$$\\Large p(Y \\mid X) = \\alpha_{2U, T} + \\alpha_{2U+1, T}$$ 因为每一步的动态规划都只涉及到乘法和加法，所以我们的损失函数是可微的，进一步来说模型可以通过梯度反向传播算法来训练。当然了，最大化条件概率也就可以说成最小化负的条件概率，因此我们的目标函数 (最小化负对数似然) 如下：$$\\large \\sum_{(X,Y) \\in \\mathcal{D}} - log p(Y \\mid X)$$ Inference当我们训练完模型之后，我们就得拿它来推理结果了，这是就需要解决如下问题：$$\\Large Y^{*} = \\underset{Y} {\\operatorname {argmax}} p(Y \\mid X)$$ 最佳路径解码 采用贪心策略，每个时间步上都选择概率最高的单词作为输出以期使得整个对齐映射概率最高，如下：$$\\Large A^{*} = \\underset{A} {\\operatorname {argmax}} \\prod_{t=1}^{T} p_t(a_t \\mid X)$$ 然后通过 collapse function 就可以根据得到的 $A^{*}$ 来获得输出序列 $Y$。这种贪心策略实际上能够应用且有效的场景少之甚少，它并不能保证我们所获得的 $Y$ 一定是最优的 $Y$。更进一步来说，它完全没有考虑到一个 $Y$ 可能对应着多个 $A$ 这个事实。 举个例子，$p(A_1 = [a, a, a] \\mid X) = 0.32$、$p(A_2 = [a, a, \\epsilon] \\mid X) = 0.18$ 以及 $p(A_3 = [\\epsilon, b, \\epsilon] \\mid X) = 0.42$，那么就要有 $Y = [b]$，因为对齐 $A_3$ 是概率最高的那个。但是我们要知道 $[a, a, a] \\Rightarrow [a]$ 且 $[a, a, \\epsilon] \\Rightarrow [a]$，所以 $p(Y = [a] \\mid X) = 0.32 + 0.18 = 0.5$，故 $Y = [a]$ 才更有可能是正确输出结果。 前缀搜索解码 该算法可以是作为束搜索(beam search)的变体，那么我们首先来看一下标准的束搜索是如何运作的，如下图(束大小为3)： 我们为什么不能使用标准的束搜索呢，正是因为我们的偏序列中存在着重复和 $\\epsilon$ tokens，因此到头来可能计算的只是同一个输出 $Y$ 的不同对齐映射罢了。正是因为如此，我们在每个时间步中都执行 collapse function 来将偏对齐转换成偏输出，这里称之为对应输出的前缀(prefix)，于是我们的“束”不再是“对齐束”而是“前缀束”。具体如下图所示： 从上图详细的流程中我们不难发现如下规律： 当前缀中的最后一个标签与当前标签不同时，我们只需要将正在处理的节点得分并入到前缀得分中去； 当前缀中的最后一个标签与当前标签相同时，我们需要将当前处理节点以及其上面的 $\\epsilon$ 节点一起并入前缀得分中去； 某一个得到前缀并不一定就只能从一个之前的前缀得到，有可能有多条路径都能构成该前缀。 注：CTC 计算的一种实现方法 像是在语音识别领，整个模型还会涉及一个语言模型来提升准确率，为此，我们可以将语言模型视作推理中的一个项：$$\\Large Y^{*} = \\underset{Y} {\\operatorname {argmax}} p(Y \\mid X) \\cdot p(Y)^\\alpha \\cdot L(Y)^\\beta$$ 其中，$L(Y)$ 会计算目标序列 $Y$ 的长度并充当一个词插入红利(word insertion bonus)。语言模型则只会在前缀被扩展时被包括在内，也容易理解，毕竟一个不变的东西再去算一次属实浪费时间了。如若这般，就很容易导致搜索更偏向于较短的前缀，此时上述的词插入红利将会帮助我们减缓这个问题。一般来说，参数 $\\alpha$ 和 $\\beta$ 会通过交叉验证(cross-validation)来设定。 CTC PropertiesConditional Independence它广为人诟病的缺陷就是条件独立性假设，但这一点与非自回归翻译模型却不谋而合。即，模型假设：对于给定的输入，每一个输出都条件独立于其他输出，自不必说，这是一个bad假设，尤其对于 Seq2Seq 问题来说。 例如，给定一个语音输入，其预测的输出序列可以是 $Y = tripple\\ \\ A$ 或 $Y = AAA$。那么，当第一个字符输出为 $A$ 时，下一个字符为 $A$ 的可能性应该因此而提升；当第一个字符输出为 $t$ 时，下一个字符为 $i$ 的概率就应该因此而升高。但是条件独立性假设移除了这种前后向的依赖关系，从而导致上述行为无法发生。 因此，使用 CTC 的模型性能都不高，因为它很难学习到一个像样的语言模型。但条件独立性假设并非全是坏处，它有助于学习到的语言知识能够快速迁移至其他领域。 Alignment PropertiesCTC 算法是无需先验对齐的，它的目标函数会边际化所有可能的对齐。尽管 CTC 对 $X$ 和 $Y$ 之间的对齐映射的排列形式做了强假设，但是它们之间的分布概率对于模型是不可知的。有些任务上，CTC 会给单个对齐排列分配绝大部分的概率，这是不合理的。 正如之前所说的，CTC 的对齐是单调的，这对于语音识别而言是行之有效的，但是对于机器翻译而言是不合理的 (因为一个当前翻译单词很可能就对齐到了源句中相对位置在其之前好几个单词位置的单词)。 CTC 对齐的另一特点就是它们是多对一的对齐关系，且必定是 Many-to-One Map 的对齐。这种映射关系对于一些情况是合理的，但显然对于所有情况一概视之是不合理的。简言之，CTC 不允许一对多映射关系的存在是不合理的。 从上述多对一映射，我们又引出令一问题：输入序列长度一定要大于等于输出序列长度。这会导致其难以处理输入序列长度小于输出序列长度的样本，而这在机器翻译中并非鲜有。即使是通过某种方法拉伸输入序列长度，那么长度预测又成为一个新问题，当长度预测过长时，冗余的计算量又成为一个问题。 CTC ContextHMMsCTC 其实和 HMM 十分相似，接下来我们就来分析一下。假设输入序列 $X$ 其长度为 $T$、输出序列 $Y$ 其长度为 $U$，一种学习 $p(Y \\mid X)$ 的策略就是应用贝叶斯法则：$$\\Large p(Y \\mid X) \\propto p(X \\mid Y) \\cdot p(Y)$$ 其中，$p(Y)$ 是任意的某个语言模型，我们需要关注的是 $p(X \\mid Y)$。如上所述，我们依旧让 $\\mathcal{A}$ 表示 $X$ 和 $Y$ 之间所有合法对齐排列的集合，然后我们边缘化这些对齐则有：$$\\Large p(X \\mid Y) = \\sum_{A \\in \\mathcal{A}} p(X, A \\mid Y)$$ 我们在这儿定义两个假设：(1). 给定前一个状态 $a_{t-1}$，状态 $a_t$ 条件独立于所有历史状态；(2). 给定当前状态 $a_t$，观察结果 $x_t$ 完全条件独立，(如下图所示) 于是我们有如下的标准隐马尔科夫模型：$$\\Large p(X) = \\sum_{A \\in \\mathcal{A}} \\prod_{t=1}^{T}\\ \\ \\ p(x_t \\mid a_t) \\cdot p(a_t \\mid a_{t-1})$$ 其中，$\\sum_{A \\in \\mathcal{A}} \\prod_{t=1}^{T}$ 表示边际化所有可能的对齐映射；$p(x_t \\mid a_t)$ 是输出概率(emission probability)；$p(a_t \\mid a_{t-1})$ 则是翻译概率(translation probability)。 接下来，我们首先假设翻译概率 $p(a_t \\mid a_{t-1})$ 是均匀分布，那么就有：$$\\Large p(X) \\propto \\sum_{A \\in \\mathcal{A}} \\prod_{t=1}^{T}\\ \\ \\ p(x_t \\mid a_t)$$ 比较两式，我们不难发现如下两点不同： 一个是给定 $X$ 去学习 $Y$，另一个则是给定 $Y$ 去学习 $X$； 集合 $\\mathcal{A}$ 的生成方法。 我们应用贝叶斯公式并重写模型：$$\\Large \\begin{aligned}p(X) &amp; \\propto \\sum_{A \\in \\mathcal{A}} \\prod_{t=1}^{T}\\ \\ \\ p(x_t \\mid a_t) \\&amp; \\propto \\sum_{A \\in \\mathcal{A}} \\prod_{t=1}^{T}\\ \\ \\ \\frac{p(a_t \\mid x_t) \\cdot p(x_t)}{p(a_t)} \\&amp; \\propto \\sum_{A \\in \\mathcal{A}} \\prod_{t=1}^{T}\\ \\ \\ \\frac{p(a_t \\mid x_t)}{p(a_t)}\\end{aligned}$$ 此时，我们为状态 $a$ 假设一个先验的均匀分布并使其基于输入序列 $X$ 而不是单个时刻的 $x_t$，则有：$$\\Large p(X) \\propto \\sum_{A \\in \\mathcal{A}} \\prod_{t=1}^{T}\\ \\ \\ p(a_t \\mid X)$$ 如果假设 $\\mathcal{A}$ 是一样的，那么上述式子实质上就是 CTC loss 函数；而实际上 HMM 框架并不会指定 $\\mathcal{A}$ 应该包含什么，它可由具体任务具体分析得到。在许多情况下，模型并不依赖于 $Y$ 并且 $\\mathcal{A}$ 包含了所有长度为 $T$ 的可能输出序列，此时 HMM 可以被表示为如下的遍历图： 而在我们的这个例子中，模型的转移与 $Y$ 是密切相关的，我们可以通过一个简单的线性状态转移图来表示之： CTC 通过 $\\epsilon$ 来增强输出词典，HMM 则允许一个从左至右的转移子集，而 CTC HMM 就会有两个开始状态和两个接受状态。虽然对于每个独立的 $Y$，状态图都是不同的，但是估计观察和转移概率的函数是共享的。CTC 模型能够给出可能对齐的集合从而为一些问题提供好的先验知识。并且 CTC 是辨别的，它直接建模 $p(Y \\mid X)$，而辨别训练能够让我们使用更强大的学习算法。 Enc-Dec Models编码器-解码器是用于序列建模的神经网络通用框架，其中编码器编码输入序列 $X$，而解码器则输出目标序列上的一个分布，我们可以表示成如下式子：$$\\Large p(Y \\mid X) = Decoder(Encoder(X))$$ Encoder CTC 模型的编码器和我们所见过的 Enc-Dec 架构中的编码器并无二样，但其有一个特殊的约束：输入序列长度大于目标序列长度。 Decoder CTC 模型的解码器可以视作一个简单的线性变换再加上一个 Softmax 激活函数。","categories":[{"name":"Speech Recognition","slug":"Speech-Recognition","permalink":"http://example.com/categories/Speech-Recognition/"}],"tags":[{"name":"CTC","slug":"CTC","permalink":"http://example.com/tags/CTC/"}]},{"title":"Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine Translation","slug":"Multi-Task-Learning-with-Shared-Encoder-for-Non-Autoregressive-Machine-Translation","date":"2021-11-25T07:20:39.000Z","updated":"2021-11-25T08:34:17.206Z","comments":true,"path":"2021/11/25/Multi-Task-Learning-with-Shared-Encoder-for-Non-Autoregressive-Machine-Translation/","link":"","permalink":"http://example.com/2021/11/25/Multi-Task-Learning-with-Shared-Encoder-for-Non-Autoregressive-Machine-Translation/","excerpt":"机器翻译具有两大解码范式：自回归解码和非自回归解码。自回归解码方式取得了 SOTA 的性能表现却其推理时延却很高；而非自回归解码方式大大提升了模型推理速度却饱受翻译质量偏低之苦。这正是因为 NAT 解码器中目标句内上下文依赖关系的缺失所致，而之前的学者大都采用知识蒸馏(KD)来实现迁移学习以缓解该问题。","text":"机器翻译具有两大解码范式：自回归解码和非自回归解码。自回归解码方式取得了 SOTA 的性能表现却其推理时延却很高；而非自回归解码方式大大提升了模型推理速度却饱受翻译质量偏低之苦。这正是因为 NAT 解码器中目标句内上下文依赖关系的缺失所致，而之前的学者大都采用知识蒸馏(KD)来实现迁移学习以缓解该问题。 本文中，作者大胆假设“AT 模型的 encoder 和 NAT 模型的 encoder 会捕捉输入句子中不同的语言特征”并通过探究实验(probing tasks)实证之。而基于上述假设，作者提出使用多任务学习并共享编码器来将 AT 模型的知识迁移到 NAT 模型中去从而提升 NAT 模型翻译的质量。确切地说，作者通过采用一个额外的 AT 模型作为辅助并让 AT 与 NAT 共享编码器参数以结合 AT encoder 和 NAT encoder 所能够捕捉到的不同语言特征从而提高 NAT 模型的翻译质量。 在 WMT14 En-De 和 WMT16 En-Ro 数据集上的实验结果表明 Multi-task NAT 相较于基线模型取得了显著的性能提升；在大规模数据集 WMT19 &amp; WMT20 En-De 上的结果也进一步验证了 Multi-task Learning 框架的有效性。除此之外，作者从实验结果中揭示多任务学习是知识蒸馏的互补方法。 1. Probing Tasks探究任务可以定量地衡量模型表示中嵌入的语言知识，作者借助该任务来实证上述的假设。具体的任务如下 SeLen : 预测句子的长度 WC : 在给定句子及其嵌入表示下预测出现的单词 TrDep : 检查编码器表示是否推理出了句子的等级结构 ToCo : 衡量句子节点下紧接着的顶层成分序列 BShif : 预测句子中的连续单词是否被反转 Tense : 预测主句中动词的时态 SubN : 预测主句中主语的个数 ObjN : 预测主句中直接宾语的个数 SoMo : 辨别每个句子的动词或名词是否被更改过 Coln : 辨别每个并列从句是否被修改过 最终的 probing tasks 结果如下图所示： 从上表中不难发现，AT encoder 和 NAT encoder 的确趋向于捕捉源句中不同的语言特征。总体上 AT encoder 更擅长于捕捉源句中的语义特征，而 NAT encoder 则更擅长于捕捉源句中的表面特征。值得一提的是，length prediction 对于 NAT 而言是至关重要的，所以 NAT encoder 在 SeLen 任务上的表现远超 AT encoder。于是，作者就此得出结论：AT encoder 和 NAT encoder 会捕捉源句中的不同语言属性特征，从而为作者的编码器共享结构提供了现实依据。 2. Multi-Task NAT 模型结构 作者采取了硬参数共享方法(hard parameter sharing method)来共享编码器参数，于是整体模型包含了以下三个部分： 共享编码器 (shared encoder) 自回归解码器 (AT decoder) 非自回归解码器 (NAT decoder) 这三个部分的参数会被联合优化(jointly optimize)以最小化多任务损失函数值。 损失函数 Multi-Task NAT 模型的损失函数定义为 AT 模型损失和 NAT 模型损失的加权和，具体如下式：$$\\large \\mathcal{L} = \\lambda_t \\mathcal{L}{nat}(X, Y; \\theta{enc}, \\theta_{dec}^{nat}) + (1-\\lambda_t) \\mathcal{L}{at} (X, Y; \\theta{enc}, \\theta_{dec}^{at})$$ 之前的学者已经表明了任务权重对于多任务学习而言是尤为重要的一环，因此作者提出了一种退火策略以动态地降低 AT loss 的权重。由上式不难发现，$\\lambda_t$ 不是一个常数，而是与当前时间步 t 相关的，如果 $\\lambda_t$ 随着 $t$ 而不断增长，此消彼长，那么 AT loss 的权重就会不断减小。 权重退火 作者将其权重退火成为重要性退火(importance annealing)策略，将 $\\lambda_t$ 定义为一个关于当前时间步 $t$ 和总时间步 $T$ 的线性函数，如下：$$\\large \\lambda_t = \\frac{t}{T}$$ 从上式不难发现有这样的关系成立 $t : 0 \\rightarrow T \\Longrightarrow \\lambda_t : 0 \\rightarrow 1 \\Longrightarrow (1 - \\lambda_t) : 1 \\rightarrow 0$ 。这是因为 AT loss 只是一个辅助项，并不应该直接影响 NAT 模型的实际推理，所以作者希望 AT loss 的权重在训练中不断减小直至归零。 训练推理 训练期间 作者将输入序列 $X$ 输入到共享编码器中，并将输出序列 $Y$ 分别输入到 AT 解码器和 NAT 解码器中去。此时的 $Y$ 既可以是原始数据集的 groundtrue 也可以是蒸馏数据集的 groundtrue。 推理期间 作者在本文中只使用共享编码器和 NAT 解码器来进行模型翻译任务，如此便能保持 NAT 的快速推理优点。 3. 实验结果作者在很多数据集上进行了实验，其结果说明多任务学习与编码器共享框架能够有效地提高 NAT 魔性的翻译质量，如下图所示：","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"Align Tools (Baby dataset)","slug":"Align-Tools-Baby-dataset","date":"2021-11-24T08:06:58.000Z","updated":"2021-11-29T14:07:19.251Z","comments":true,"path":"2021/11/24/Align-Tools-Baby-dataset/","link":"","permalink":"http://example.com/2021/11/24/Align-Tools-Baby-dataset/","excerpt":"1. 引言鉴于第一次运行的 fast align 和 Giza++ 是在一个大数据集上进行的，不仅耗时巨大，而且结果多的反而让我眼花缭乱，再加之有一些细节地方并没有得以实现，所以我在一个小数据集上再次进行了实验。而为了能够直观地解释对齐关系，我选用了如下数据集： 数据集：newstest2017 ch-en dataset which contains 2001 examples. 预处理：因为是直接从实验室找的，所以所有文本已经被 tokenize 过了。","text":"1. 引言鉴于第一次运行的 fast align 和 Giza++ 是在一个大数据集上进行的，不仅耗时巨大，而且结果多的反而让我眼花缭乱，再加之有一些细节地方并没有得以实现，所以我在一个小数据集上再次进行了实验。而为了能够直观地解释对齐关系，我选用了如下数据集： 数据集：newstest2017 ch-en dataset which contains 2001 examples. 预处理：因为是直接从实验室找的，所以所有文本已经被 tokenize 过了。 2. Fast Align1. 实验过程参考 Align Tools 即可。 2. 错误解决 – Could NOT find SparseHash (missing: SPARSEHASH_INCLUDE_DIR) 即使去安装 sparsehash 也是失败的，所以暂时没找到解决方法 但好像不解决这个也能够继续下去 如何检查自己的 fast align 是否已安装完成 输入下列指令 /data1/wbxu/fast_align/build/fast_align 返回如下结果即表明安装完成123456789101112131415Usage: /data1/wbxu/fast_align/build/fast_align -i file.fr-en Standard options ([USE] = strongly recommended): -i: [REQ] Input parallel corpus -v: [USE] Use Dirichlet prior on lexical translation distributions -d: [USE] Favor alignment points close to the monotonic diagonoal -o: [USE] Optimize how close to the diagonal alignment points should be -r: Run alignment in reverse (condition on target and predict source) -c: Output conditional probability table Advanced options: -I: number of iterations in EM training (default = 5) -q: p_null parameter (default = 0.08) -N: No null word -a: alpha parameter for optional Dirichlet prior (default = 0.01) -T: starting lambda for diagonal distance parameter (default = 4) -s: print alignment scores (alignment ||| score, disabled by default) 3. 结果分析1. 执行输出日志分析12345678910111213141516ITERATION 5 (FINAL) log_e likelihood: -203555 log_2 likelihood: -293667 cross entropy: 6.49534 perplexity: 90.2178 posterior p0: 0 posterior al-feat: 0ITERATION 5 (FINAL) log_e likelihood: -223478 log_2 likelihood: -322411 cross entropy: 6.07383 perplexity: 67.3607 posterior p0: 0 posterior al-feat: 0 size counts: 728 虽然，随着 EM 算法的执行，整个对齐的困惑度在不断下降，但是无论是前向对齐还是逆向对齐，它们**最终的困惑度都很高 (≈90, 67)**； 最终的的负对数似然也很高； 其他的参数尚不明了，因此不作分析。 2. 句内对齐分析 fast align 的输出结果是以位置间对齐映射关系的形式存储的，这虽然对于机器是友好的，但是对于人类是极其不友好的，如下：12(Source-Target) 0-0 0-1 2-2 4-3 5-4 6-5 7-6 9-7 11-8 11-9(Source-Target) 9-0 2-1 2-2 5-3 12-4 7-5 0-6 1-7 5-8 12-9 25-10 14-11 15-12 12-13 17-14 22-15 21-16 23-17 20-18 23-19 我们同时执行了源语言到目标语言的对齐和目标语言到源语言的对齐，而这两个方向的对齐并不是完全可逆的；En→Zh 方向上，’28’对齐到’岁’，而 Zh→En 方向上，’岁’对齐到‘@-@’，如下：12(Source-Target) 28-28 28-岁 Year-厨师 Old-被 Chef-发现 Found-死 Dead-于 San-旧金山 Mal-一家 Mal-商 (Target-Source) 28-28 岁-@-@ 厨师-Year 岁-@-@ 厨师-Old 发现-Chef 死-Found 死-Dead 旧金山-at 旧金山-San 旧金山-Francisco 商-Mal 我们发现tokenized数据会对 fast align 的对齐过程增加噪声；‘@-@’ 是一个子词表示，而正常来说，同一个单词的所有子词应该是对齐同一个单词或同样的几个单词的，如下：1(Source-Target) recently-近日 @-@-刚 @-@-搬 old-至 San-旧金山 who-的 a-一位 28-28 old-岁 San-厨师 week-本周 was-被 found-发现 在同一个句子内，诚然 fast align 建模了一些正确的对齐关系，但是错误的对齐映射关系也不少，甚至在有些句子中错误的数量超过了正确的数量，如下：12(Source-Target) to-针对 government-政府 &amp;apos;s-的 silence-沉默 to-态度 ,-， JDC-初级 JDC-医生 JDC-委员会 JDC-执行 JDC-委员会 has-已 today-今日 formal-正式 request-要求 BMA-英国 BMA-医学 BMA-协会 a-理事会 meeting-召开 special-特别 meeting-会议 authorise-批准 to-旨在 programme-九月初 beginning-开始 of-升级 industrial-劳工 action-行动 of-的 a-一项 (Source-Target) rejected-参与 rejected-投票 by-的 58-成员 of-中 of-, members-58% rejected-反对 the-该 ballot-合同 ballot-交易 不难发现，fast align 会将同一个单词对应多个目标单词，而这里面可能只有一个是对的，甚至一个对的都没有，给人一种蒙的感觉，如下：12(Target-Source) 该-the 数字-number 已-has 大幅-slumped 大幅-by 近-almost 90%-90 90%-% (Source-Target) her-拜 her-尔斯 in-因此 puts-跻身 like-像 puts-迈克尔 @-@-- league-菲尔 league-普斯 @-@-( @-@-Michael @-@-Phelps @-@-) like-一样 @-@-的 a-“ @-@-几十年 @-@-一遇 athletes-运动员 @-@-” who-的 Michael-行列 their-他们 to-将 Phelps-各自 their-的 sports-体育项目 sports-提升 to-到 new-新 heights-高度 3. 所有对齐分析首先，我们需要编写python程序来统计某个单词一共对齐哪些单词，我写的代码如下 (注：由于欠考虑原始的’-‘符号等原因，结果中会存在一定的错误，但是总体来说，已经能够反映出 fast align 的对齐情况了)： 123456789101112131415161718192021222324252627def mapping(file, output): text = [] with open(file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f: content = f.readlines() length = len(content) for i in range(length): text.append(content[i].split()) mappings = &#123;&#125; for i in range(length): for j in range(len(text[i])): for k in range(len(text[i][j])): if text[i][j][k] == &quot;-&quot;: key = text[i][j][:k] value = text[i][j][k+1:] break if key not in mappings.keys(): mappings[key] = [value, ] elif value not in mappings[key]: mappings[key].append(value) with open(output, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f: for key, value in mappings.items(): f.write(str(key) + str(value) + &quot;\\n&quot;) 上述程序执行之后就可以得到所有单词的所有对齐情况了，于是我们又发现了如下的问题： fast align 能够建模正确的对齐关系，如下：12recently [&#x27;近日&#x27;, &#x27;最近&#x27;, &#x27;连日来&#x27;, &#x27;日前&#x27;, &#x27;近期&#x27;]八个 [&#x27;eight&#x27;] 但是，在某个单词的所有对齐关系中存在着很多的不正确对齐关系，如下：12friends [&#x27;朋友&#x27;, &#x27;表达&#x27;, &#x27;深切&#x27;, &#x27;政变&#x27;, &#x27;亲朋好友&#x27;, &#x27;日子&#x27;, &#x27;误会&#x27;, &#x27;段时间&#x27;, &#x27;交流&#x27;]发现 [&#x27;Chef&#x27;, &#x27;found&#x27;, &#x27;in&#x27;, &#x27;that&#x27;, &#x27;find&#x27;, &#x27;body&#x27;, &#x27;continues&#x27;, &#x27;was&#x27;] 有一些单词甚至没有正确的对齐关系，如下：12weighing [&#x27;战术&#x27;, &#x27;理性&#x27;]验尸官[&#x27;body&#x27;, &#x27;Westfield&#x27;] 句子中的标点符号能够作为一种噪声存在，它对齐了很多非标点符号的(子)词，如下：12.[&#x27;。&#x27;, &#x27;时&#x27;, &#x27;，&#x27;, &#x27;”&#x27;, &#x27;都&#x27;, &#x27;食物&#x27;, &#x27;没想到&#x27;, &#x27;了&#x27;, &#x27;方式&#x27;, &#x27;；&#x27;, &#x27;的&#x27;, &#x27;这个&#x27;, &#x27;直播&#x27;, &#x27;保存&#x27;, &#x27;女士&#x27;, &#x27;用品&#x27;, &#x27;故&#x27;, &#x27;货品&#x27;, &#x27;非常&#x27;, &#x27;就&#x27;, &#x27;才&#x27;, &#x27;岁&#x27;, &#x27;地&#x27;, &#x27;跟&#x27;, &#x27;无小事&#x27;, &#x27;我&#x27;, &#x27;,&#x27;, &#x27;不&#x27;, &#x27;外地&#x27;, &#x27;防晒&#x27;, &#x27;常见&#x27;, &#x27;交工&#x27;, &#x27;特点&#x27;, &#x27;几句&#x27;, &#x27;事情&#x27;, &#x27;能力&#x27;, &#x27;职位&#x27;, &#x27;扮&#x27;, &#x27;很&#x27;],[&#x27;，&#x27;, &#x27;为&#x27;, &#x27;美国广播公司&#x27;, &#x27;KGO&#x27;, &#x27;波士顿&#x27;, &#x27;同时&#x27;, &#x27;在&#x27;, &#x27;烹饪&#x27;, &#x27;主席&#x27;, &#x27;一贯&#x27;, &#x27;感觉&#x27;, &#x27;的&#x27;, &#x27;政府&#x27;, &#x27;举行&#x27;, &#x27;。&#x27;, &#x27;顿&#x27;, &#x27;网球&#x27;, &#x27;例如&#x27;, &#x27;、&#x27;, &#x27;Co&#x27;, &#x27;您&#x27;, &#x27;已&#x27;, &#x27;这些&#x27;, &#x27;3&#x27;, &#x27;道&#x27;, &#x27;其中&#x27;, &#x27;比赛&#x27;, &#x27;真正&#x27;, &#x27;还是&#x27;, &#x27;年&#x27;, &#x27;表现&#x27;, &#x27;前&#x27;, &#x27;·&#x27;, &#x27;岁&#x27;, &#x27;后&#x27;, &#x27;了&#x27;, &#x27;（&#x27;, &#x27;）&#x27;, &#x27;请求&#x27;, &#x27;是&#x27;, &#x27;(&#x27;, &#x27;重新&#x27;, &#x27;他&#x27;, &#x27;之后&#x27;, &#x27;进行&#x27;, &#x27;表演&#x27;, &#x27;还有&#x27;, &#x27;分别&#x27;, &#x27;事实&#x27;, &#x27;但&#x27;, &#x27;腹部&#x27;, &#x27;部位&#x27;, &#x27;被&#x27;, &#x27;从&#x27;, &#x27;其次&#x27;, &#x27;里面&#x27;, &#x27;包含&#x27;, &#x27;犹太教&#x27;, &#x27;天主教&#x27;, &#x27;科&#x27;, &#x27;精彩绝伦&#x27;, &#x27;主要&#x27;, &#x27;差旅&#x27;, &#x27;服装&#x27;, &#x27;包括&#x27;, &#x27;教练&#x27;, &#x27;这&#x27;, &#x27;影响&#x27;, &#x27;中&#x27;, &#x27;15&#x27;, &#x27;丽埃勒及&#x27;, &#x27;参与&#x27;, &#x27;成本&#x27;, &#x27;课程&#x27;, &#x27;提纲&#x27;, &#x27;认知&#x27;, &#x27;进修&#x27;, &#x27;性别&#x27;, &#x27;重视&#x27;, &#x27;时间&#x27;, &#x27;罹患&#x27;, &#x27;评估&#x27;, &#x27;教授&#x27;, &#x27;但是&#x27;, &#x27;任&#x27;, &#x27;少数&#x27;, &#x27;族裔&#x27;, &#x27;移民&#x27;, &#x27;然而&#x27;, &#x27;从小&#x27;, &#x27;很&#x27;, &#x27;体操选手&#x27;, &#x27;安娜&#x27;, &#x27;Jennifer&#x27;, &#x27;Carpenter&#x27;, &#x27;盖&#x27;, &#x27;开始&#x27;, &#x27;现年&#x27;, &#x27;分&#x27;, &#x27;57&#x27;, &#x27;这种&#x27;, &#x27;镇&#x27;, &#x27;说&#x27;, &#x27;枪手&#x27;, &#x27;当&#x27;, &#x27;内&#x27;, &#x27;棍&#x27;, &#x27;要&#x27;, &#x27;人们&#x27;, &#x27;其&#x27;, &#x27;在内&#x27;, &#x27;理发&#x27;, &quot;&#x27;&quot;, &#x27;斯&#x27;, &#x27;目前&#x27;, &#x27;日前&#x27;, &#x27;自然&#x27;, &#x27;,&#x27;, &#x27;带来&#x27;, &#x27;其实&#x27;, &#x27;发微博&#x27;, &#x27;玩游戏&#x27;, &#x27;逛&#x27;, &#x27;需要&#x27;, &#x27;：&#x27;, &#x27;种植&#x27;, &#x27;个&#x27;, &#x27;股份&#x27;, &#x27;锦州市&#x27;, &#x27;大有&#x27;, &#x27;滨海公路&#x27;, &#x27;段&#x27;, &#x27;1&#x27;, &#x27;号&#x27;, &#x27;11&#x27;, &#x27;有&#x27;, &#x27;都&#x27;, &#x27;不久&#x27;, &#x27;打出&#x27;, &#x27;加上&#x27;, &#x27;无论是&#x27;, &#x27;见面&#x27;, &#x27;自己&#x27;, &#x27;家中&#x27;, &#x27;成立&#x27;, &#x27;保障&#x27;, &#x27;握&#x27;, &#x27;手心&#x27;, &#x27;轻飘飘&#x27;, &#x27;一小撮&#x27;, &#x27;百万元&#x27;, &#x27;海关&#x27;, &#x27;较&#x27;, &#x27;决定&#x27;, &#x27;部&#x27;, &#x27;总导演&#x27;, &#x27;一声令下&#x27;, &#x27;武警&#x27;, &#x27;和田&#x27;, &#x27;阿克苏&#x27;, &#x27;三地&#x27;, &#x27;部队&#x27;, &#x27;导调&#x27;, &#x27;指令&#x27;, &#x27;环绕&#x27;, &#x27;所以&#x27;, &#x27;质量&#x27;, &#x27;稀薄&#x27;, &#x27;车&#x27;, &#x27;总体&#x27;, &#x27;不断&#x27;, &#x27;行政院长&#x27;, &#x27;昨天上午&#x27;, &#x27;刚好&#x27;, &#x27;真的&#x27;, &#x27;一切&#x27;, &#x27;金牌&#x27;, &#x27;其间&#x27;, &#x27;比如说&#x27;, &#x27;多个&#x27;, &#x27;转而&#x27;, &#x27;运动员&#x27;, &#x27;)&#x27;, &#x27;监狱&#x27;, &#x27;时&#x27;, &#x27;地区&#x27;, &#x27;支队长&#x27;, &#x27;马琪&#x27;, &#x27;新闻&#x27;, &#x27;指标&#x27;, &#x27;情况&#x27;, &#x27;所在&#x27;, &#x27;39&#x27;, &#x27;格雷&#x27;, &#x27;如今&#x27;, &#x27;中部&#x27;, &#x27;厘米&#x27;, &#x27;等&#x27;, &#x27;随着&#x27;, &#x27;日&#x27;, &#x27;收到&#x27;, &#x27;仍&#x27;, &#x27;次&#x27;, &#x27;品类&#x27;, &#x27;涵盖&#x27;, &#x27;建材&#x27;, &#x27;轻工产品&#x27;, &#x27;出台&#x27;, &#x27;加大&#x27;, &#x27;引进&#x27;, &#x27;220&#x27;, &#x27;近期&#x27;, &#x27;方面&#x27;, &#x27;之声&#x27;, &#x27;基础&#x27;, &#x27;喀山&#x27;, &#x27;世锦赛&#x27;, &#x27;和&#x27;, &#x27;各种&#x27;, &#x27;宁泽涛&#x27;, &#x27;奠定&#x27;, &#x27;田亮&#x27;, &#x27;去&#x27;, &#x27;而&#x27;, &#x27;伦敦&#x27;, &#x27;功成身退&#x27;, &#x27;甚至&#x27;, &#x27;比如&#x27;, &#x27;快&#x27;, &#x27;回复&#x27;, &#x27;却&#x27;, &#x27;查&#x27;, &#x27;车辆&#x27;, &#x27;整治&#x27;, &#x27;看病&#x27;, &#x27;监管&#x27;, &#x27;存在&#x27;, &#x27;于&#x27;, &#x27;采访&#x27;, &#x27;四川&#x27;, &#x27;绵阳&#x27;, &#x27;往下沉&#x27;, &#x27;小时候&#x27;, &#x27;学过&#x27;, &#x27;坚持&#x27;, &#x27;足球&#x27;, &#x27;篮球&#x27;, &#x27;排球&#x27;, &#x27;自行车&#x27;, &#x27;摔跤&#x27;, &#x27;武术&#x27;, &#x27;市场运作&#x27;, &#x27;节俭办&#x27;, &#x27;同年&#x27;, &#x27;时许&#x27;, &#x27;房间内&#x27;, &#x27;研究&#x27;, &#x27;保持&#x27;, &#x27;8&#x27;, &#x27;再&#x27;, &#x27;主导&#x27;, &#x27;分析&#x27;, &#x27;把&#x27;, &#x27;强&#x27;, &#x27;科技&#x27;, &#x27;还&#x27;, &#x27;增加&#x27;, &#x27;设有&#x27;, &#x27;节日&#x27;, &#x27;布置&#x27;, &#x27;７&#x27;, &#x27;三亚市&#x27;, &#x27;对&#x27;, &#x27;他会&#x27;, &#x27;欧文&#x27;, &#x27;操刀&#x27;, &#x27;助攻&#x27;, &#x27;拦截&#x27;, &#x27;意甲&#x27;, &#x27;法甲&#x27;, &#x27;王国&#x27;, &#x27;抢断&#x27;, &#x27;配合&#x27;, &#x27;跑动&#x27;, &#x27;无懈可击&#x27;, &#x27;现在&#x27;, &#x27;担任&#x27;, &#x27;训练&#x27;, &#x27;这周&#x27;, &#x27;不好&#x27;, &#x27;功能&#x27;, &#x27;针对&#x27;, &#x27;照片&#x27;, &#x27;推特&#x27;, &#x27;各色&#x27;, &#x27;人种&#x27;, &#x27;表示&#x27;, &#x27;负责&#x27;, &#x27;战争&#x27;, &#x27;国防&#x27;, &#x27;位于&#x27;, &#x27;错误&#x27;, &#x27;结果&#x27;, &#x27;伊拉克&#x27;, &#x27;维和部队&#x27;, &#x27;降低&#x27;, &#x27;格雷戈里&#x27;, &#x27;撰写&#x27;, &#x27;一篇&#x27;, &#x27;文章&#x27;, &#x27;文中&#x27;, &#x27;布什&#x27;, &#x27;不顾&#x27;, &#x27;未尝&#x27;, &#x27;败绩&#x27;, &#x27;巴姆&#x27;, &#x27;上&#x27;, &#x27;作为&#x27;, &#x27;菜&#x27;, &#x27;共同&#x27;, &#x27;本书&#x27;, &#x27;三明治&#x27;, &#x27;们&#x27;, &#x27;并&#x27;, &#x27;至&#x27;, &#x27;发布&#x27;, &#x27;奶油&#x27;, &#x27;补充&#x27;, &#x27;果脯&#x27;, &#x27;赞加&#x27;, &#x27;沉得&#x27;, &#x27;下心&#x27;, &#x27;公司&#x27;, &#x27;形式&#x27;, &#x27;在线&#x27;, &#x27;学习&#x27;, &#x27;不仅&#x27;, &#x27;而且&#x27;, &#x27;如遇&#x27;, &#x27;9&#x27;, &#x27;章&#x27;, &#x27;法制&#x27;, &#x27;层面&#x27;, &#x27;框架&#x27;, &#x27;军力&#x27;, &#x27;文化公园&#x27;, &#x27;金额&#x27;, &#x27;奥马哈&#x27;, &#x27;巴克&#x27;, &#x27;所有&#x27;, &#x27;尤其&#x27;, &#x27;特&#x27;, &#x27;女子组&#x27;, &#x27;洛特&#x27;, &#x27;乔&#x27;, &#x27;埃莉诺&#x27;, &#x27;孩子&#x27;, &#x27;培养&#x27;, &#x27;朋友&#x27;, &#x27;总需求&#x27;, &#x27;减弱&#x27;, &#x27;海南省&#x27;, &#x27;路&#x27;, &#x27;走&#x27;, &#x27;天津&#x27;, &#x27;甘肃&#x27;, &#x27;省市&#x27;, &#x27;实施细则&#x27;, &#x27;即&#x27;, &#x27;经查&#x27;, &#x27;迎战&#x27;, &#x27;得到&#x27;, &#x27;拍照&#x27;, &#x27;通过&#x27;, &#x27;人中&#x27;, &#x27;法律咨询&#x27;, &#x27;代理服务&#x27;, &#x27;拒&#x27;, &#x27;巨额&#x27;, &#x27;债务&#x27;, &#x27;分崩离析&#x27;, &#x27;这位&#x27;, &#x27;名叫&#x27;, &#x27;预算&#x27;, &#x27;T5&#x27;, &#x27;由&#x27;, &#x27;来&#x27;, &#x27;无水&#x27;, &#x27;激情&#x27;, &#x27;过&#x27;, &#x27;太阳系&#x27;, &#x27;诸多方面&#x27;] 3. GIZA++1. 实验过程参考 Align Tools 即可。 2. 错误解决 align_sym.py 文件无法正常运行 需要使用 python2 的环境来运行，使用 conda 即可创建 python2 的虚拟环境 align_plot.py 文件无法正常运行 使用之前创建好的 python2 环境 下载依赖包 numpy 和 matplotlib 下载 sans-serif 字体，否则中文将无法正常显示 下载 simhei 字体 从网站 fontpalace 下载 SimHei 字体文件 找到存放字体文件的目录, 大约为~/matplotlib/mpl-data/fonts/ttf，可通过下列代码找到：12import matplotlibprint(matplotlib.matplotlib_fname()) 将文件 simhei.ttf 存放入 ~/fonts/ttf 目录下 将如下代码添加到文件 matplotlibrc 中去123font.family : sans-seriffont.sans-serif : SimHeiaxes.unicode_minus : False 删除缓存中已生成的 matplotlib 目录，大约为 ~/.cache/matplotlib 使用下述命令重启虚拟环境12conda deactivateconda activate python2 3. 结果分析 许多文件的不明所以，完全没有任何提示信息，对新手极其不友好，如下:12345678en2ch.a3.final:1 1 2 100 11 2 2 100 12 3 2 100 12 4 2 100 11 1 3 100 0.7773383 1 3 100 0.2226621 2 3 100 0.443757 en2ch.perp 文件表示的是困惑度，不难发现虽然困惑度呈现逐步下降趋势，但是最终的困惑度仍然很高，而且 HMM 的困惑度很接近于 Model 4 的困惑度；无独有偶，在之前的大数据集上 HMM 的困惑度甚至低于 Model 4 的困惑度，这一点是令我不解的，如下：123456789101112131415161718192021#trnsz tstsz iter model trn-pp test-pp trn-vit-pp tst-vit-pp2001 0 0 Model1 11770.9 N/A inf N/A2001 0 1 Model1 112.831 N/A 745.31 N/A2001 0 2 Model1 81.2514 N/A 413.337 N/A2001 0 3 Model1 67.9365 N/A 271.741 N/A2001 0 4 Model1 61.7402 N/A 211.242 N/A2001 0 5 HMM 58.0343 N/A 180.88 N/A2001 0 6 HMM 52.6243 N/A 91.7463 N/A2001 0 7 HMM 38.5041 N/A 54.2244 N/A2001 0 8 HMM 28.8229 N/A 36.4591 N/A2001 0 9 HMM 23.8012 N/A 28.4325 N/A2001 0 10 THTo3 20.6636 N/A 22.4276 N/A2001 0 11 Model3 47.6184 N/A 50.2598 N/A2001 0 12 Model3 42.8841 N/A 44.75 N/A2001 0 13 Model3 41.4157 N/A 43.0298 N/A2001 0 14 Model3 40.4792 N/A 41.9223 N/A2001 0 15 T3To4 39.7868 N/A 41.1109 N/A2001 0 16 Model4 21.8971 N/A 22.3101 N/A2001 0 17 Model4 20.2356 N/A 20.5329 N/A2001 0 18 Model4 19.7004 N/A 19.9528 N/A2001 0 19 Model4 19.3744 N/A 19.6003 N/A en2ch.a3.final 文件中的内容形式如下：$i\\ j\\ l\\ m\\ p(i/j, l, m)$，其中 $i$ 表示源语言 token 的位置；$j$ 表示目标语言 token 的位置；$l$ 表示源语言句子长度；$m$ 表示目标语言句子长度；$p(i/j, l, m)$ 表示 $i$ 位置上的源语言 token 被翻译到目标语言 $j$ 位置上 token 的概率；总的来说，这个文件应该可以解决对齐中的重排序问题(reordering)，粗略看一了下还是比较邻近的位置上的reordering概率比较大，如下：123456789101112131415161764 49 64 100 164 50 64 100 164 51 64 100 164 52 64 100 163 53 64 100 163 54 64 100 161 55 64 100 161 56 64 100 10 57 64 100 161 58 64 100 15 1 66 100 18 2 66 100 19 3 66 100 10 4 66 100 133 5 66 100 133 6 66 100 133 7 66 100 1 en2ch.n3.final 文件表示的是各个 token 的繁殖力概率，很多单词的繁殖力还是主要集中在 0、1、2 这些数字上，如下：1234562 0.790922 0.163175 0.0374083 0.00353707 0.00180963 0.00064888 0.00153155 0.000358334 0.000246704 0.000363033 3 0.967959 0.0279171 0.00146716 0.000960575 0.000645714 0.000464062 0.000311298 0.000119431 7.17008e-05 8.35965e-05 4 0.628078 0.223453 0.0939839 0.0166623 0.0138697 0.00923218 0.00555119 0.00400141 0.00302904 0.00213859 5 0.828264 0.123534 0.0269111 0.00769915 0.00517549 0.00371953 0.0024951 0.000957258 0.000574693 0.000670038 6 0.647396 0.229846 0.066597 0.017175 0.0142964 0.00951625 0.005722 0.00412453 0.00312224 0.00220439 7 0.519123 0.307894 0.0652615 0.0394383 0.0267116 0.0176144 0.0104919 0.00504176 0.00457916 0.003845 en2ch.t3.final 文件是 IBM Model 3 的翻译表，en2ch.d4.final 文件是 IBM Model 4 的翻译表，由于是 ids 间的对齐，并不直观，如下：1234567890 16 0.3829580 33 0.4460210 40 0.06683410 41 6.36139e-070 72 0.09552340 96 0.002046120 491 0.0001623880 696 6.19271e-060 737 0.00643965 en2ch.A3.final 文件是单向对齐文件，其中的数字代表了 token 在目标句中的位置，下标从 1 开始；如下：123456#Sentence pair (1) source length 12 target length 10 alignment score : 3.26715e-1428 岁 厨师 被 发现 死 于 旧金山 一家 商 NULL (&#123; &#125;) 28 (&#123; 1 &#125;) @-@ (&#123; &#125;) Year (&#123; &#125;) @-@ (&#123; &#125;) Old (&#123; &#125;) Chef (&#123; &#125;) Found (&#123; &#125;) Dead (&#123; 2 3 4 5 6 &#125;) at (&#123; &#125;) San (&#123; 8 &#125;) Francisco (&#123; 7 &#125;) Mal (&#123; 9 10 &#125;) #Sentence pair (2) source length 26 target length 20 alignment score : 3.26866e-28近日 刚 搬 至 旧金山 的 一位 28 岁 厨师 本周 被 发现 死 于 当地 一家 商场 的 楼梯间 NULL (&#123; 6 19 &#125;) a (&#123; &#125;) 28 (&#123; 8 &#125;) @-@ (&#123; &#125;) year (&#123; &#125;) @-@ (&#123; &#125;) old (&#123; 9 &#125;) chef (&#123; 10 &#125;) who (&#123; &#125;) had (&#123; &#125;) recently (&#123; &#125;) moved (&#123; &#125;) to (&#123; &#125;) San (&#123; &#125;) Francisco (&#123; 14 15 &#125;) was (&#123; &#125;) found (&#123; 13 &#125;) dead (&#123; &#125;) in (&#123; &#125;) the (&#123; &#125;) stairwell (&#123; 1 2 3 4 5 7 &#125;) of (&#123; &#125;) a (&#123; &#125;) local (&#123; 16 &#125;) mall (&#123; 12 17 18 20 &#125;) this (&#123; &#125;) week (&#123; 11 &#125;) 如上，GIZA++ 能够找出正确的对齐关系，如 28-28 week-本周 等； 如上，GIZA++ 也构造了错误的对齐关系，如 Dead-岁 stairwell-近日 等； 4. 可视化及分析如果需要可视化对齐结果的话，我们需要两个 pyhton 文件，分别是 align_sym.py 和 align_plot.py，在下面的实验结果连接中可以找到这两个文件（感谢爱心师兄改正的python文件 align_sym.py）。 首先，根据两个不同方向的单向对齐文件生成对称化文件，运行如下代码即可： 1python align_sym.py en2ch/en2ch.A3.final ch2en/ch2en.A3.final &gt; aligned.grow-diag-final-and 其次，运行如下文件绘制可视图： 1python align_plot.py test_data/en_tok.txt test_data/ch_tok.txt aligned.grow-diag-final-and &quot;example number&quot; 可视化结果如下所示： Example 9： Example 86： Example 126： Example 1511： 我随机选择了几条数据样本进行可视化，如上图所示，通过观察我发现： GIZA++ 不仅正确地建模了 One-to-One Mapping 的对齐关系，而且还正确建模了 One-to-Many Mapping 和 Many-to-One Mapping 的对齐关系； 虽然对齐中存在着 Reordering 问题，但是正确对齐的一大部分都呈现出一条斜对角线； GIZA++ 还建模了许多错误的对齐关系； 和 fast algin 一样，GIZA++ 也存在着一个词对齐着很多词的现象，而其中只有几个是正确的对齐。 4. GIZA++隐藏用法 实现方法 我们可以先删除整个 giza-pp-master 目录，在重新下载/解压； 然后，找到 GIZA++v2/Makefile 文件，将其打开； 将上述文件中第 9 行的 -DBINARY_SEARCH_FOR_TTABLE 删除； 重复之前的所有操作。 隐藏文件 en2ch.ti.final 文件是 ids 的双语对齐表，其形式为 $source\\ id\\ \\ \\ target\\ id\\ \\ \\ p(align(source\\ id, target\\ id))$，如下：123454459 2372 1.06663e-061405 6900 2.70731e-088416 162 4.56842e-068416 380 5.20437e-068416 737 8.84737e-06 en2ch.actual.ti.final 文件是 tokens 的双语对齐表, 其形式为 $source\\ token\\ \\ \\ target\\ token\\ \\ \\ p(align(source\\ token, target\\ token))$，如下：1234518 August 1.06663e-06奥运会 Athens 2.70731e-08熬夜 day 4.56842e-06熬夜 next 5.20437e-06熬夜 sports 8.84737e-06 结果分析 有很多完全错误的对齐关系，如下：12奖牌 event 0.159333产生 event 0.0866653 有些单词对齐了很多单词，但只有一个或几个是正确的，如下：123感到 worried 0.0346762比较 worried 0.0737838担心 worried 0.113913 虽然有些单词有对有错，但是正确的对齐关系概率最高，如下：12345意义 ecosystem 0.0287441具有 ecosystem 0.0234632方面 ecosystem 0.00794153生态圈 ecosystem 0.402861行业 ecosystem 0.0152548 粗略看下来，GIZA++的错误率还是很高。 5. 总结fast algin 的对齐结果直观地看起来是不如 GIZA++ 的，但是 GIZA++ 在小数据集上从搭建到运行出结果所花的时间远远大于 fast align，甚至在大数据集上花费了3天左右的时间。而在这两个对齐工具所展现的对齐结果中，我们都发现了一个单词对齐到很多单词的现象(仅有一个或几个单词是正确的对齐关系，其余皆是错误对齐)。如果我们需要提高对齐的质量以提升模型的性能，那么可以使用 GIZA++；而如果我们的对齐并不是模型提升性能的关键因素，那么或许也可以使用 fast align。关于后续的对齐学习，我将着手去学习 Connectionist Temporal Classification，该技术在 Non-Autoregressive Translation 中十分关键。","categories":[{"name":"Statistical Machine Translation","slug":"Statistical-Machine-Translation","permalink":"http://example.com/categories/Statistical-Machine-Translation/"}],"tags":[{"name":"Alignment","slug":"Alignment","permalink":"http://example.com/tags/Alignment/"}]},{"title":"End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification","slug":"End-to-End-Non-Autoregressive-Neural-Machine-Translation-with-Connectionist-Temporal-Classification","date":"2021-11-24T06:14:17.000Z","updated":"2021-11-25T08:35:20.049Z","comments":true,"path":"2021/11/24/End-to-End-Non-Autoregressive-Neural-Machine-Translation-with-Connectionist-Temporal-Classification/","link":"","permalink":"http://example.com/2021/11/24/End-to-End-Non-Autoregressive-Neural-Machine-Translation-with-Connectionist-Temporal-Classification/","excerpt":"Transformer 虽然帮助 NMT 模型摆脱了训练时的顺序执行问题，但是却没有使得其在推理时仍然保持这种并行化优势。非自回归翻译模型 (NAT) 让推理时的每个 token 都能够并行地被生成，但是付出了较大的性能衰减代价。作者提出了将序列标注问题中的 Connectionist Temporal Classification (CTC) 引入到 NAT 模型中去，从而在保持推理速度的同时提升其翻译质量。作者在 WMT En-Ro 和 WMT En-De 数据集上进行了实验，结果表明 CTC-based NAT 模型能够获得与其他非自回归模型相当的性能表现。","text":"Transformer 虽然帮助 NMT 模型摆脱了训练时的顺序执行问题，但是却没有使得其在推理时仍然保持这种并行化优势。非自回归翻译模型 (NAT) 让推理时的每个 token 都能够并行地被生成，但是付出了较大的性能衰减代价。作者提出了将序列标注问题中的 Connectionist Temporal Classification (CTC) 引入到 NAT 模型中去，从而在保持推理速度的同时提升其翻译质量。作者在 WMT En-Ro 和 WMT En-De 数据集上进行了实验，结果表明 CTC-based NAT 模型能够获得与其他非自回归模型相当的性能表现。 首先，作者的模型是基于强大的 Transformer 模型而搭建起来的，并且其编码器部分保持不变。因为无论是 AT 模型的编码器还是 NAT 模型的编码器，它们所承担的职责是完全相同的，所以作者选择将编码器部分保持不变。 鉴于 NAT 模型的强条件独立性假设，所以作者将非自回归翻译任务看作为一个序列标注问题。正如 CTC 的原始论文中所述，机器翻译中的源句和目标句长度往往是不相同的，这一点就和序列标注有所冲突，因此无法直接将序列标注器应用在机器翻译上。为此，作者采用了一种拉伸输入序列长度的方法，从而可以保证标注器的输入序列长度一定是不小于输出序列长度的。简言之，作者将编码器输出的每个隐藏状态 $h$ 映射到一个更长的隐藏表示 $W_{spl}\\cdot h$，然后再将该表示切分成 $k$ 个隐藏表示，如此便得到了长度为 $k \\cdot T_x$ 的隐藏表示序列，如下所示：$$s_{ci+b} = (W_{spl} h_c + b_{spl})_{bd:(b+1)d}$$ 既然作者已经让输入 CTC 解码器的输入序列长度大于等于目标序列的长度了，接下来就是将 CTC 正式引入了。上述增长的隐藏表示序列会被喂给解码器解码，值得注意的是作者取消了原始 Transformer 解码器自注意力网络中的 mask 机制以鼓励每个 token 的预测都依赖于其双向上下文信息。最终解码器输出的隐藏状态都会被标注成目标语言的 tokens 或者一个 null 标记，如果被标注上目标语言的 token 则表示该位置上的翻译内容，而如果被标注上 null 则表示该位置上无内容生成。由于输入输出之间没有先验的对齐关系，所以作者将所有能产生正确结果的输出序列都考虑进损失函数中。又因为直接对所有组合的指数数量进行求和是不切实际的，于是作者顺水推舟，将 CTC loss 引入模型之中。 CTC 通过使用动态规划(dynamic programming)算法来计算输出序列的负对数似然，而这个损失可以借助一个类似于训练隐马尔可夫模型的线性算法来计算得到。CTC 使用动态规划的方法计算出输出序列的所有前后缀的偏对数概率之和，而这些概率会被存储在一个预计算对数概率表格中，于是我们就可以通过结合对应前缀和后缀的对数概率来计算得到正确输出序列的部分概率了。除此之外，CTC 还为作者带来了束搜索算法，虽然这会让模型的推理时延稍微上升，但是却能够有效地提升模型性能。因为线性层上的顺序计算还是要比自回归解码快得多，但是束搜索已经被证明能够有效地提升模型翻译体质量了。 下图为作者所提出的 NAT 模型结构： 最终实验结果(如下图)表明，作者所提出的模型在 WMT16 En-Ro 和 WMT14 En-De 数据集上的表现不尽如人意，但是在 WMT15 En-De 数据集上则创造出了 NAT 模型中的 SOTA 成绩。 本论文感想：由于本文中没有详细介绍 CTC loss 的原理，所以后续还需要看回 CTC 的原始论文！","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"Align Tools","slug":"Align-Tools","date":"2021-11-23T07:08:19.000Z","updated":"2021-11-29T14:07:42.725Z","comments":true,"path":"2021/11/23/Align-Tools/","link":"","permalink":"http://example.com/2021/11/23/Align-Tools/","excerpt":"$\\mathfrak{Fast\\ \\ \\ Align}$","text":"$\\mathfrak{Fast\\ \\ \\ Align}$ 1. 从 Github 上下载 fast_align fast_align 2. 通过以下命令编译 fast_align1234mkdir buildcd buildcmake ..make 3. 准备平行语料 ( 可以 tokenize , 也可以不 tokenize ) 我使用的是 tokenized WMT14 En-De 训练数据集: 英语数据样本 12I declare resumed the session of the European Parliament ad@@ jour@@ ned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant fes@@ tive period .Although , as you will have seen , the d@@ read@@ ed &amp;apos; millenn@@ ium bug &amp;apos; failed to materi@@ alise , still the people in a number of countries suffered a series of natural disasters that truly were d@@ read@@ ful . 德语数据样本 12Ich erkläre die am Freitag , dem 17. Dezember unterbro@@ ch@@ ene Sitzungsperiode des Europäischen Parlaments für wieder@@ aufgenommen , wünsche Ihnen nochmals alles Gute zum Jahres@@ wechsel und hoffe , daß Sie schöne Ferien hatten .Wie Sie feststellen konnten , ist der ge@@ für@@ chtete &amp;quot; Mill@@ en@@ ium @-@ Bu@@ g &amp;quot; nicht eingetreten . Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden . 4. 编写 python 来执行 fast_align1234567891011121314151617181920212223242526272829303132333435363738394041import osdef fast_align_preprocess(path): src_path = path + &quot;/train.src&quot; tgt_path = path + &quot;/train.tgt&quot; res_path = path + &quot;/result.txt&quot; with open(src_path, &quot;r&quot;) as src_file, open(tgt_path, &quot;r&quot;) as tgt_file: src_sentences = src_file.readlines() tgt_sentences = tgt_file.readlines() with open(res_path, &quot;w&quot;) as file: for src, tgt in zip(src_sentences, tgt_sentences): res = src[:-1] + &quot; ||| &quot; + tgt[:-1] file.write(res + &quot;\\n&quot;)def use_fast_align(path): align = r&quot;/data/wbxu/fast_align-master/build/fast_align&quot; atools = r&quot;/data/wbxu/fast_align-master/build/atools&quot; res_path = path + &quot;/result.txt&quot; save_path1 = path + &quot;/wmt14_en-de_source-target.align&quot; save_path2 = path + &quot;/wmt14_en-de_target-source.align&quot; save_path3 = path + &quot;/wmt14_en-de_symmetrized.align&quot; total_path_forward = align + &quot; -i &quot; + res_path + &quot; -d -o -v &gt; &quot; + save_path1 os.system(total_path_forward) total_path_reverse = align + &quot; -i &quot; + res_path + &quot; -d -o -v -r &gt; &quot; + save_path2 os.system(total_path_reverse) symmetrize_path = atools + &quot; -i &quot; + save_path1 + &quot; -j &quot; + save_path2 + &quot; -c grow-diag-final-and &gt; &quot; + save_path3 os.system(symmetrize_path)def main(path=&quot;/data/wbxu/data&quot;): fast_align_preprocess(path) use_fast_align(path)main() 第一个函数用于预处理输入数据, 预处理完后的数据样本如下: 12I declare resumed the session of the European Parliament ad@@ jour@@ ned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant fes@@ tive period . ||| Ich erkläre die am Freitag , dem 17. Dezember unterbro@@ ch@@ ene Sitzungsperiode des Europäischen Parlaments für wieder@@ aufgenommen , wünsche Ihnen nochmals alles Gute zum Jahres@@ wechsel und hoffe , daß Sie schöne Ferien hatten .Although , as you will have seen , the d@@ read@@ ed &amp;apos; millenn@@ ium bug &amp;apos; failed to materi@@ alise , still the people in a number of countries suffered a series of natural disasters that truly were d@@ read@@ ful . ||| Wie Sie feststellen konnten , ist der ge@@ für@@ chtete &amp;quot; Mill@@ en@@ ium @-@ Bu@@ g &amp;quot; nicht eingetreten . Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden . 原始的输出文件是下标之间的对齐, 晦涩难懂, 输出样例如下: 正向对齐 forward align 120-0 1-1 3-2 2-3 13-4 6-6 14-7 15-8 10-9 10-10 11-11 11-12 11-13 7-14 8-15 10-16 11-17 2-18 17-19 25-20 26-21 23-22 25-23 25-24 24-25 30-26 30-27 18-28 33-29 34-30 34-31 35-32 38-33 38-34 36-35 42-362-0 3-1 6-2 6-3 7-4 8-6 11-7 10-8 10-9 12-10 13-11 14-12 14-13 14-14 15-15 15-16 16-17 17-18 19-19 42-20 30-21 24-23 27-24 33-25 29-26 30-27 33-28 40-29 35-30 42-31 42-32 逆向对齐 reverse align 120-0 1-1 2-1 3-2 4-1 5-2 6-6 7-14 8-15 9-9 10-9 11-9 12-3 13-4 14-7 15-8 16-18 17-19 18-28 19-20 20-20 21-20 22-22 23-22 24-25 25-20 26-21 28-24 29-27 30-26 32-25 33-29 34-31 35-32 36-35 37-33 38-33 39-33 42-360-5 1-4 2-0 3-1 4-1 5-3 6-2 7-4 8-6 9-8 10-8 11-9 12-10 13-11 14-13 15-15 16-10 17-18 19-19 20-19 24-23 27-24 28-24 29-26 30-27 31-24 32-28 33-28 34-30 35-30 38-29 39-29 40-29 41-29 42-32 Tips: 虽然是逆向对齐, 但是输出内容还是以 ‘src_id-tgt_id’ 的形式输出的~ 5. 将输出文件中数字 ids 转换成 tokens 对齐1234567891011121314151617181920212223242526272829303132def recover(path): align_path = path + &quot;/wmt14_en-de_source-target.align&quot; src_path = path + &quot;/train.src&quot; tgt_path = path + &quot;/train.tgt&quot; save_path = path + &quot;/token_source-target.align&quot; src_tokens, tgt_tokens, ali_mapping = [], [], [] with open(align_path, &quot;r&quot;) as fa, open(src_path, &#x27;r&#x27;) as fs, open(tgt_path, &#x27;r&#x27;) as ft: src = fs.readlines() tgt = ft.readlines() ali = fa.readlines() length = len(src) for i in range(length): src_tokens.append(src[i].split()) tgt_tokens.append(tgt[i].split()) ali_mapping.append(ali[i].split()) with open(save_path, &quot;w&quot;) as f: for i in range(length): res = &quot;&quot; for k in range(len(ali_mapping[i])): src_index, tgt_index = ali_mapping[i][k].split(&#x27;-&#x27;) res += src_tokens[i][int(src_index)] + &quot;-&quot; + tgt_tokens[i][int(tgt_index)] + &quot; &quot; res += &quot;\\n&quot; f.write(res)recover(path=&quot;/data/wbxu/data&quot;) 1234567891011121314151617181920212223242526272829303132def recover(path): align_path = path + &quot;/wmt14_en-de_target-source.align&quot; src_path = path + &quot;/train.src&quot; tgt_path = path + &quot;/train.tgt&quot; save_path = path + &quot;/token_target-source.align&quot; src_tokens, tgt_tokens, ali_mapping = [], [], [] with open(align_path, &quot;r&quot;) as fa, open(src_path, &#x27;r&#x27;) as fs, open(tgt_path, &#x27;r&#x27;) as ft: src = fs.readlines() tgt = ft.readlines() ali = fa.readlines() length = len(src) for i in range(length): src_tokens.append(src[i].split()) tgt_tokens.append(tgt[i].split()) ali_mapping.append(ali[i].split()) with open(save_path, &quot;w&quot;) as f: for i in range(length): res = &quot;&quot; for k in range(len(ali_mapping[i])): src_index, tgt_index = ali_mapping[i][k].split(&#x27;-&#x27;) res += tgt_tokens[i][int(tgt_index)] + &quot;-&quot; + src_tokens[i][int(src_index)] + &quot; &quot; res += &quot;\\n&quot; f.write(res)recover(path=&quot;/data/wbxu/data&quot;) 转换成 tokens 对齐后, 子词间的对齐关系更加直观可见, 输出样例如下: 正向对齐 forward align 12I-Ich declare-erkläre the-die resumed-am Friday-Freitag the-dem 17-17. December-Dezember jour@@-unterbro@@ jour@@-ch@@ ned-ene ned-Sitzungsperiode ned-des European-Europäischen Parliament-Parlaments jour@@-für ned-wieder@@ resumed-aufgenommen ,-, wish-wünsche you-Ihnen again-nochmals wish-alles wish-Gute to-zum year-Jahres@@ year-wechsel and-und hope-hoffe that-, that-daß you-Sie pleasant-schöne pleasant-Ferien enjoyed-hatten .-. as-Wie you-Sie seen-feststellen seen-konnten ,-, the-der ed-ge@@ read@@-für@@ read@@-chtete &amp;apos;-&amp;quot; millenn@@-Mill@@ ium-en@@ ium-ium ium-@-@ bug-Bu@@ bug-g &amp;apos;-&amp;quot; failed-nicht materi@@-eingetreten .-. suffered-Doch people-Bürger number-einiger of-unserer countries-Mitgliedstaaten suffered-Opfer of-von read@@-schrecklichen disasters-Naturkatastrophen .-geworden .-. 逆向对齐 reverse align 12Ich-I erkläre-declare erkläre-resumed die-the erkläre-session die-of dem-the Europäischen-European Parlaments-Parliament unterbro@@-ad@@ unterbro@@-jour@@ unterbro@@-ned am-on Freitag-Friday 17.-17 Dezember-December aufgenommen-1999 ,-, und-and wünsche-I wünsche-would wünsche-like nochmals-once nochmals-again zum-to wünsche-wish Ihnen-you Gute-happy wechsel-new Jahres@@-year zum-the hoffe-hope daß-that Sie-you hatten-enjoyed schöne-a schöne-pleasant schöne-fes@@ .-. ist-Although ,-, Wie-as Sie-you Sie-will konnten-have feststellen-seen ,-, der-the für@@-d@@ für@@-read@@ chtete-ed &amp;quot;-&amp;apos; Mill@@-millenn@@ ium-ium Bu@@-bug &amp;quot;-&amp;apos; nicht-failed eingetreten-materi@@ eingetreten-alise Bürger-people einiger-number einiger-of Mitgliedstaaten-countries Opfer-suffered einiger-a von-series von-of Naturkatastrophen-natural Naturkatastrophen-disasters schrecklichen-were schrecklichen-d@@ schrecklichen-read@@ schrecklichen-ful .-. $\\mathfrak{GIZA++}$1. 从 Github 上下载 GIZA++ giza-pp 2. 通过以下命令编译 GIZA++12cd giza-pp-mastermake 编译完成之后, 我们所需要的就是以下4个文件: /data1/wbxu/giza-pp-master/GIZA++-v2/plain2snt.out /data1/wbxu/giza-pp-master/GIZA++-v2/snt2cooc.out /data1/wbxu/giza-pp-master/GIZA++-v2/GIZA++ /data1/wbxu/giza-pp-master/mkcls-v2/mkcls 3. 准备平行语料 我选择了没有 tokenize 的 WMT14 En-De 训练数据集： 英文样本如下： 12I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .Although , as you will have seen , the dreaded &amp;apos; millennium bug &amp;apos; failed to materialise , still the people in a number of countries suffered a series of natural disasters that truly were dreadful . 德语样本如下： 12Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .Wie Sie feststellen konnten , ist der gefürchtete &amp;quot; Millenium @-@ Bug &amp;quot; nicht eingetreten . Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden . 4. 为句子和单词进行编号1GIZA++-v2/plain2snt.out /data1/wbxu/data/en.txt /data1/wbxu/data/de.txt 该命令会生成4个文件：2个以vcb为后缀；2个以snt为后缀 Vcb 文件, 其形式为 id : token : count 英语词典1234567892 I 595964 3 declare 2140 4 resumed 1401 5 the 5940442 6 session 6792 7 of 3104652 8 European 301463 9 Parliament 117592 10 adjourned 582 德语词典1234567892 Ich 2251133 erkläre 8504 die 28863525 am 1166296 Freitag 27257 , 59130698 dem 4568219 17. 285610 Dezember 10338 Tip: 该文件为对应语言的词典文件, 其中 count 统计了对应单词的出现次数 Snt 文件, 其形式为 count $Enter$ ids $Enter$ ids $Enter$ 英德句对12345612 3 4 5 6 7 5 8 9 10 11 12 13 14 15 16 17 2 18 19 20 21 22 23 24 25 26 27 28 29 5 30 31 24 32 25 33 34 35 36 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 7 18 19 20 21 22 23 24 25 26 7 27 28 29 30 31 32 137 16 38 24 39 40 41 16 5 42 43 44 45 43 46 22 47 16 48 5 49 29 25 50 7 51 52 25 53 7 54 55 31 56 57 58 36 33 28 34 35 7 36 37 38 39 40 41 42 39 43 44 32 45 46 47 48 49 50 51 52 53 54 55 32 德英句对1234561 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 7 18 19 20 21 22 23 24 25 26 7 27 28 29 30 31 32 2 3 4 5 6 7 5 8 9 10 11 12 13 14 15 16 17 2 18 19 20 21 22 23 24 25 26 27 28 29 5 30 31 24 32 25 33 34 35 36 1 33 28 34 35 7 36 37 38 39 40 41 42 39 43 44 32 45 46 47 48 49 50 51 52 53 54 55 32 37 16 38 24 39 40 41 16 5 42 43 44 45 43 46 22 47 16 48 5 49 29 25 50 7 51 52 25 53 7 54 55 31 56 57 58 36 Tip: 该文件为 ids 化的句对文件, 第一行为对应句对的出现次数, 第二行为源语言句子, 第三行为目标语言句子 5. 生成共现文件12GIZA++-v2/snt2cooc.out /data1/wbxu/data/en.vcb /data1/wbxu/data/de.vcb /data1/wbxu/data/en_de.snt &gt; en_de.coocGIZA++-v2/snt2cooc.out /data1/wbxu/data/de.vcb /data1/wbxu/data/en.vcb /data1/wbxu/data/de_en.snt &gt; de_en.cooc 该命令会生成两个后缀为cooc的文件 (不清楚什么意思, 好像是一样的)： en_de.cooc 文件如下： 123456780 20 30 40 50 60 70 80 9 de_en.cooc 123456780 20 30 40 50 60 70 80 9 6. 生成词类12mkcls-v2/mkcls -p/data1/wbxu/data/en.txt -V/data1/wbxu/data/en.vcb.classes optmkcls-v2/mkcls -p/data1/wbxu/data/de.txt -V/data1/wbxu/data/de.vcb.classes opt 该命令的参数如下： -c 词类数目 -n 优化次数，默认是1，越大越好 -p 输入文件 -V 输出文件 opt 优化输出 该命令会生成4个文件, 2个以classes为后缀, 2个以cats为后缀： 英语词类： classes 定义单词到类别编号的映射12345678910&amp;apos;ALGAR 64 &amp;apos;ALSACE 64 &amp;apos;ALT 66 &amp;apos;ALZINA 64 &amp;apos;AMBRE 66 &amp;apos;AMORE 64 &amp;apos;AMOUR 66 &amp;apos;AN 66 &amp;apos;ANGELY 66 &amp;apos;ANNONCE 64 cats 定义类别编号到单词的映射1230:$, 1: 2:-2º,host,.ccf,0.01sein,0.6mm,04835,07541,09599,09pol,1,0l,1.033.856.359,1.037.162,1.062,1.114.112,1.116.513,1.150.000,1.199,1.215.000,1.465,1.536,1.685,1.711m,10,47,10,970,10.250,10.450,100,50,100ps,10ns,10x106,11,978,11.850,116,554,135,698,12,48,12.900,12353,1236ha,125gr,12621,128.000,12K,12º,13,05,13,60,13.250,13347,133º,134,15,13445,14,60,14,74,14,900,14.280,14.500.000,14fachem,15.267,150MHz,15m2,15tes,16,221,166.442,1694.,17,500,17.238,17.317.774,17m-,18,11,18.----in,185,000,187,888,187.000.000,19.220,191.400.000,1965.The,1986.,1KM,2.300m2,2.5mm,2.754,2.855,2.Hand.Das,20,04,20,67,20.026,20.685,200,000,000,2005im,20459,205,366,880,20KHz,20hektar,217m2,22.000km,22jähriger,22º,23.909,24ps,24x36mm,25,378km,25.500,25.700,256fachen,256x256px,25dB,25jährigem,2620m,27.445,27jähriger,28.400,2809,28219,285.782,295km,29sten,2Mbit,2ter,3.000.000.000,3.6Mbps,30,10,30.70,306,6,309.684,30M,31,72,3253,3299m,32MByte,32x16,32º,333.488,33428,335er,341.624,35390,36,500,000,38,4,390,000,39031,39035,39039,399,99,3DVIA,3Loch,3RA6,3x3x3m,4,664,4.694,40.972,400ml,40549,40jährigem,41.917,41qm,43.07079,43.700 德语词类： classes 定义单词到类别编号的映射123456789101112131415---BEMERKUNG 73---WICHTIG--- 20---en 94---too 30--AMDRY 32--Asma 48--Audio 26--Brasilianische 99--COMING 49--CZK 31--Der 76--Die 76--Diese 76--EINFÜGEN-- 32--Feld 91 cats 定义类别编号到单词的映射1230:$,1:2:-bresaola,-effectively,-may,-members,-nor,-oac,-overwhelming,-personal,-solutions,-to,-would,.will,10a.m.We,1152x864,125K,16.10.1996,18000th,1986s,1un,2.2rev4.1,250ms,251,66,300or,3Dembossments,3Sis,3beds,3pt,4.95m,61,870,7.1.3.3,7020.021,7040.021,7040.030,7041.060,81g,8HP,93.55.xxx.xxx,ALLREADY,AMSTeX,AVPDOS32,Abatec,Abraser,Acherkogl,Adiv,Aenor,Affinati,Aier,Alcar,Algarvios,America--can,Anting,Aow,ApplicationsTo,Aquadynamic,Aqualog,ArccOS,Arilines,Atheatos,Aufeis,Autobild.de,BCTCS,Baantai,Babae,Backgammonboard,Balabranip,Bananera,BaoLong,Bartimaeus,Basothen 7. 运行GIZA++12/data1/wbxu/giza-pp-master/GIZA++-v2/GIZA++ -S /data1/wbxu/data/en.vcb -T /data1/wbxu/data/de.vcb -C /data1/wbxu/data/en_de.snt -CoocurrenceFile /data1/wbxu/giza-pp-master/en_de.cooc -o en2de -OutputPath en2de/data1/wbxu/giza-pp-master/GIZA++-v2/GIZA++ -S /data1/wbxu/data/de.vcb -T /data1/wbxu/data/en.vcb -C /data1/wbxu/data/de_en.snt -CoocurrenceFile /data1/wbxu/giza-pp-master/de_en.cooc -o de2en -OutputPath de2en 该命令的参数如下： -S 源语言文件 -T 目标语言文件 -C src_tgt.snt文件 -CoocurrenceFile src_tgt.cooc文件 -o 输出文件的前缀 -OutputPath 输出文件的目录 该命令执行时间较长, 须待良久, 最终会生成如下文件： 输出日志文件 1234567891011==========================================================writing Final tables to Disk Writing PERPLEXITY report to: de2en/de2en.perpWriting source vocabulary list to : de2en/de2en.trn.src.vcbWriting source vocabulary list to : de2en/de2en.trn.trg.vcbWriting source vocabulary list to : de2en/de2en.tst.src.vcbWriting source vocabulary list to : de2en/de2en.tst.trg.vcbwriting decoder configuration file to de2en/de2en.Decoder.configEntire Training took: 99500 secondsProgram Finished at: Mon Nov 22 23:45:49 2021========================================================== 困惑度文件 ~.perp 123456789101112131415161718192021#trnsz tstsz iter model trn-pp test-pp trn-vit-pp tst-vit-pp3995262 0 0 Model1 1.63537e+06 N/A inf N/A3995262 0 1 Model1 567.258 N/A 3215.22 N/A3995262 0 2 Model1 242.121 N/A 860.049 N/A3995262 0 3 Model1 195.809 N/A 544.603 N/A3995262 0 4 Model1 183.124 N/A 443.58 N/A3995262 0 5 HMM 156.719 N/A inf N/A3995262 0 6 HMM 110.575 N/A 169.095 N/A3995262 0 7 HMM 78.7195 N/A 101.809 N/A3995262 0 8 HMM 68.6674 N/A 83.8846 N/A3995262 0 9 HMM 65.3209 N/A 77.9282 N/A3995262 0 10 THTo3 74.0646 N/A 80.3015 N/A3995262 0 11 Model3 184.549 N/A 198.633 N/A3995262 0 12 Model3 164.136 N/A 175.439 N/A3995262 0 13 Model3 157.36 N/A 167.888 N/A3995262 0 14 Model3 154.949 N/A 165.288 N/A3995262 0 15 T3To4 153.757 N/A 163.999 N/A3995262 0 16 Model4 101.187 N/A 106.708 N/A3995262 0 17 Model4 92.3756 N/A 97.1254 N/A3995262 0 18 Model4 89.1882 N/A 93.4744 N/A3995262 0 19 Model4 87.5094 N/A 91.4966 N/A ~d3.final 文件 (不知所云) 123456789101 1 100 1 11 2 100 1 11 3 100 1 11 4 100 1 11 5 100 1 11 6 100 1 11 7 100 1 11 0 100 2 0.7535832 0 100 2 0.2464171 1 100 2 0.948539 n3.final 文件表示的是某个单词所对应的繁殖力大小的概率 (从左到右，繁殖力从0到9)，其形式为 $source\\ id$ $p(fertility=0)$ $p(fertility=1)$ $\\dots$ $p(fertility=9)$ 123456789102 0.0835292 0.881598 0.0270334 0.00658778 0.00100327 0.000200634 4.27065e-05 2.70696e-07 4.87095e-06 1.14867e-07 3 0.178227 0.790122 0.0275572 0.00175264 0.0018591 0.000106401 8.37043e-05 6.1348e-05 3.65072e-05 0.000194441 4 0.0758428 0.201973 0.253675 0.0130023 0.452613 0.000283031 0.000939822 0.00132266 5.494e-05 0.000292524 5 0.401716 0.594518 0.000245277 0.00334565 0.000124138 2.99709e-05 3.1172e-06 1.49017e-05 0 3.0277e-06 6 0.328716 0.648779 0.0206393 0.00136076 0.000203803 0.000180117 2.69242e-05 1.97331e-05 1.17428e-05 6.25434e-05 7 0.729189 0.269479 0.000124212 0.00117522 2.06356e-05 1.28205e-06 9.66674e-06 7.59257e-07 0 0 8 0.105242 0.894326 0.000120709 0.000260014 3.93617e-05 7.70443e-06 7.42444e-07 5.79232e-07 3.33635e-07 1.80787e-06 9 0.0415899 0.883176 0.0729691 0.00195852 0.000284182 3.15497e-06 2.54355e-06 9.94552e-06 9.9275e-07 5.42298e-06 10 0.0534802 0.455451 0.436919 0.0427366 0.00281034 0.000736073 0.000359615 0.00348065 0.00315132 0.000875467 11 0.518013 0.481123 0.000105029 0.000745583 9.45342e-06 1.92139e-06 1.87431e-06 0 0 0 ~t3.final 文件(翻译表)表示的是对齐概率，其形式为 $source\\ id$ $target\\ id$ $p(target\\ id|source\\ id)$ 1234567891011121314&gt;0 2 8.71379e-070 4 0.07727310 5 0.001774540 6 1.23743e-070 7 0.2158340 8 0.009932770 10 1.4587e-070 13 0.01017450 14 3.58152e-060 15 2.81992e-060 16 0.01415340 18 6.23962e-070 19 0.0005449380 20 1.49871e-06 ~A3.final 文件表示单向对齐文件，数字代表了单词在句中的位置 123456789#Sentence pair (1) source length 40 target length 33 alignment score : 2.17771e-70Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten . NULL (&#123; 3 6 7 15 17 23 25 27 &#125;) I (&#123; 1 &#125;) declare (&#123; 2 &#125;) resumed (&#123; 16 &#125;) the (&#123; &#125;) session (&#123; 11 &#125;) of (&#123; 12 &#125;) the (&#123; &#125;) European (&#123; 13 &#125;) Parliament (&#123; 14 &#125;) adjourned (&#123; 10 &#125;) on (&#123; 4 &#125;) Friday (&#123; 5 &#125;) 17 (&#123; 8 &#125;) December (&#123; 9 &#125;) 1999 (&#123; &#125;) , (&#123; &#125;) and (&#123; &#125;) I (&#123; &#125;) would (&#123; &#125;) like (&#123; &#125;) once (&#123; &#125;) again (&#123; 20 &#125;) to (&#123; &#125;) wish (&#123; 18 21 22 &#125;) you (&#123; 19 &#125;) a (&#123; &#125;) happy (&#123; &#125;) new (&#123; &#125;) year (&#123; 24 &#125;) in (&#123; &#125;) the (&#123; &#125;) hope (&#123; 26 &#125;) that (&#123; 28 &#125;) you (&#123; 29 &#125;) enjoyed (&#123; 32 &#125;) a (&#123; &#125;) pleasant (&#123; 30 &#125;) festive (&#123; 31 &#125;) period (&#123; &#125;) . (&#123; 33 &#125;) #Sentence pair (2) source length 37 target length 28 alignment score : 2.3304e-64Wie Sie feststellen konnten , ist der gefürchtete &amp;quot; Millenium @-@ Bug &amp;quot; nicht eingetreten . Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden . NULL (&#123; 6 11 16 18 24 &#125;) Although (&#123; 1 &#125;) , (&#123; &#125;) as (&#123; &#125;) you (&#123; 2 &#125;) will (&#123; &#125;) have (&#123; 4 &#125;) seen (&#123; 3 &#125;) , (&#123; 5 &#125;) the (&#123; 7 &#125;) dreaded (&#123; 8 &#125;) &amp;apos; (&#123; 9 &#125;) millennium (&#123; 10 &#125;) bug (&#123; 12 &#125;) &amp;apos; (&#123; 13 &#125;) failed (&#123; 14 &#125;) to (&#123; &#125;) materialise (&#123; 15 17 &#125;) , (&#123; &#125;) still (&#123; &#125;) the (&#123; &#125;) people (&#123; 19 &#125;) in (&#123; &#125;) a (&#123; &#125;) number (&#123; 20 &#125;) of (&#123; 21 &#125;) countries (&#123; 22 &#125;) suffered (&#123; 23 27 &#125;) a (&#123; &#125;) series (&#123; &#125;) of (&#123; &#125;) natural (&#123; &#125;) disasters (&#123; 26 &#125;) that (&#123; &#125;) truly (&#123; &#125;) were (&#123; &#125;) dreadful (&#123; 25 &#125;) . (&#123; 28 &#125;) #Sentence pair (3) source length 23 target length 17 alignment score : 7.25555e-37Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen . NULL (&#123; 3 4 6 &#125;) You (&#123; 1 &#125;) have (&#123; &#125;) requested (&#123; 2 5 &#125;) a (&#123; 7 &#125;) debate (&#123; 8 &#125;) on (&#123; 9 &#125;) this (&#123; &#125;) subject (&#123; &#125;) in (&#123; 13 &#125;) the (&#123; &#125;) course (&#123; &#125;) of (&#123; &#125;) the (&#123; 14 &#125;) next (&#123; 15 &#125;) few (&#123; &#125;) days (&#123; 16 &#125;) , (&#123; &#125;) during (&#123; 10 &#125;) this (&#123; 11 &#125;) part (&#123; &#125;) @-@ (&#123; &#125;) session (&#123; 12 &#125;) . (&#123; 17 &#125;) ~d4.final 文件是 IBM Model 4 翻译表 1234567891011#Translation tables for Model 4 .#Table for head of cept.F: 69 E: 0 SUM: 3109.6 1 2840.242 106.6663 31.57454 11.46715 13.09116 8.40917 7.46996 ~gizacfg 文件是 GIZA++ 的配置文件，包括一些超参数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192adbackoff 0c /data1/wbxu/data/en_de.sntcompactadtable 1compactalignmentformat 0coocurrencefile /data1/wbxu/giza-pp-master/en_de.cooccorpusfile /data1/wbxu/data/en_de.sntcountcutoff 1e-06countcutoffal 1e-05countincreasecutoff 1e-06countincreasecutoffal 1e-05d deficientdistortionforemptyword 0depm4 76depm5 68dictionary dopeggingyn 0emalignmentdependencies 2emalsmooth 0.2emprobforempty 0.4emsmoothhmm 2hmmdumpfrequency 0hmmiterations 5l en2de/2021-11-20.130008.wbxu.loglog 0logfile en2de/2021-11-20.130008.wbxu.logm1 5m2 0m3 5m4 5m5 0m5p0 -1m6 0manlexfactor1 0manlexfactor2 0manlexmaxmultiplicity 20maxfertility 10maxsentencelength 101mh 5mincountincrease 1e-07ml 101model1dumpfrequency 0model1iterations 5model23smoothfactor 0model2dumpfrequency 0model2iterations 0model345dumpfrequency 0model3dumpfrequency 0model3iterations 5model4iterations 5model4smoothfactor 0.2model5iterations 0model5smoothfactor 0.1model6iterations 0nbestalignments 0nodumps 0nofiledumpsyn 0noiterationsmodel1 5noiterationsmodel2 0noiterationsmodel3 5noiterationsmodel4 5noiterationsmodel5 0noiterationsmodel6 0nsmooth 64nsmoothgeneral 0numberofiterationsforhmmalignmentmodel 5o en2de/en2deonlyaldumps 0outputfileprefix en2de/en2deoutputpath en2de/p 0p0 -1peggedcutoff 0.03pegging 0probcutoff 1e-07probsmooth 1e-07readtableprefix s /data1/wbxu/data/en.vcbsourcevocabularyfile /data1/wbxu/data/en.vcbt /data1/wbxu/data/de.vcbt1 0t2 0t2to3 0t3 0t345 0targetvocabularyfile /data1/wbxu/data/de.vcbtc testcorpusfile th 0transferdumpfrequency 0v 0verbose 0verbosesentence -10 ~Decoder.config 文件用于 ISI Rewrite Decoder 1234567891011TTable = en2de.t3.finalInverseTTable = en2de.ti.finalNTable = en2de.n3.finalD3Table = en2de.d3.finalD4Table = en2de.D4.finalPZero = en2de.p0_3.finalSource.vcb = /data1/wbxu/data/en.vcbTarget.vcb = /data1/wbxu/data/de.vcbSource.classes = /data1/wbxu/data/en.vcb.classesTarget.classes = /data1/wbxu/data/de.vcb.classesFZeroWords = en2de.fe0_3.final $\\mathfrak{Thought}$1. fast align1.1 检查句内对齐关系 Source-Target 对齐样例 123456I-Ich declare-erkläre the-die resumed-am Friday-Freitag the-dem 17-17. December-Dezember jour@@-unterbro@@ jour@@-ch@@ ned-ene ned-Sitzungsperiode ned-des European-Europäischen Parliament-Parlaments jour@@-für ned-wieder@@ resumed-aufgenommen ,-, wish-wünsche you-Ihnen again-nochmals wish-alles wish-Gute to-zum year-Jahres@@ year-wechsel and-und hope-hoffe that-, that-daß you-Sie pleasant-schöne pleasant-Ferien enjoyed-hatten .-. as-Wie you-Sie seen-feststellen seen-konnten ,-, the-der ed-ge@@ read@@-für@@ read@@-chtete &amp;apos;-&amp;quot; millenn@@-Mill@@ ium-en@@ ium-ium ium-@-@ bug-Bu@@ bug-g &amp;apos;-&amp;quot; failed-nicht materi@@-eingetreten .-. suffered-Doch people-Bürger number-einiger of-unserer countries-Mitgliedstaaten suffered-Opfer of-von read@@-schrecklichen disasters-Naturkatastrophen .-geworden .-. You-Im requested-Parlament a-besteht on-der requested-Wunsch subject-nach a-einer debate-Aussprache in-im course-Verlauf this-dieser session-Sitzungsperiode during-in during-den next-nächsten days-Tagen .-. In-Heute like-möchte I-ich like-Sie like-bitten ,-- the-das a-ist as-auch of-der requested-Wunsch number-einiger Members-Kolleginnen Members-Kollegen ,-, all-allen victims-Opfern of-der stor@@-St@@ stor@@-ür@@ stor@@-me ,-, particularly-insbesondere in-in the-den various-verschiedenen countries-Ländern of-der European-Europäischen Union-Union ,-, in-in of-einer silence-Schwei@@ silence-minute victims-denken .-. Please-Ich Please-bitte Please-Sie ,-, ,-sich for-zu minute-einer silence-Schwei@@ silence-ge@@ silence-minute s-zu silence-erheben .-. (-( The-Das House-Parlament rose-erhebt rose-sich observed-zu a-einer silence-Schwei@@ silence-ge@@ silence-minute )-. )-) Target-Source 对齐样例 123456Ich-I erkläre-declare erkläre-resumed die-the erkläre-session die-of dem-the Europäischen-European Parlaments-Parliament unterbro@@-ad@@ unterbro@@-jour@@ unterbro@@-ned am-on Freitag-Friday 17.-17 Dezember-December aufgenommen-1999 ,-, und-and wünsche-I wünsche-would wünsche-like nochmals-once nochmals-again zum-to wünsche-wish Ihnen-you Gute-happy wechsel-new Jahres@@-year zum-the hoffe-hope daß-that Sie-you hatten-enjoyed schöne-a schöne-pleasant schöne-fes@@ .-. ist-Although ,-, Wie-as Sie-you Sie-will konnten-have feststellen-seen ,-, der-the für@@-d@@ für@@-read@@ chtete-ed &amp;quot;-&amp;apos; Mill@@-millenn@@ ium-ium Bu@@-bug &amp;quot;-&amp;apos; nicht-failed eingetreten-materi@@ eingetreten-alise Bürger-people einiger-number einiger-of Mitgliedstaaten-countries Opfer-suffered einiger-a von-series von-of Naturkatastrophen-natural Naturkatastrophen-disasters schrecklichen-were schrecklichen-d@@ schrecklichen-read@@ schrecklichen-ful .-. Im-You Parlament-have Wunsch-requested einer-a Aussprache-debate der-on dieser-this nach-subject im-in im-the Verlauf-course im-of im-the nächsten-next Tagen-few Tagen-days in-, Verlauf-during dieser-this Sitzungsperiode-part Sitzungsperiode-@-@ Sitzungsperiode-session .-. Heute-In das-the ich-meantime Heute-, ich-I möchte-should möchte-like bitten-to minute-observe ist-a minute-minute der-&amp;apos; der-s Schwei@@-silence Kolleginnen-, auch-as einiger-number der-of Kollegen-Members ,-have Wunsch-requested ,-, der-on Opfern-behalf der-of allen-all den-the Opfern-victims insbesondere-concerned ,-, insbesondere-particularly Ländern-those der-of der-the Opfern-terrible ür@@-stor@@ me-ms ,-, in-in der-the verschiedenen-various Ländern-countries einer-of zu-the Europäischen-European Union-Union .-. bitte-Please erheben-rise ,-, ,-then ,-, zu-for zu-this minute-minute minute-&amp;apos; minute-s minute-silence .-. (-( Das-The Parlament-House erhebt-rose sich-and erhebt-observed einer-a minute-minute minute-&amp;apos; minute-s minute-silence )-) 接下来，我有如下的几个发现: fast align 能够区分单词的大小写变化ich-I &amp; Ich-I ...; fast align 构造出的大部分都是一对一的对齐映射关系I-Ich declare-erkläre ...; fast align 是具备了一定的处理 tokenized 数据间对齐的能力silence-Schwei@@ &amp; silence-ge@@ &amp; silence-minute ...; fast align 所支持的正逆向对齐关系之间无法完全对应起来对应: I-Ich &amp; Ich-I ..., 不对应: rose-sich &amp; sich-and ...; fast align 处理 BPE 化的数据时, 仍然存在着问题quota-Quot@@ &amp; quota-en@@ ...; 1.2 检查语料库上的对齐关系编写如下程序： 12345678910111213141516171819202122232425262728293031def mapping(file, output): text = [] with open(file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f: content = f.readlines() length = len(content) for i in range(length): text.append(content[i].split()) mappings = &#123;&#125; for i in range(length): for j in range(len(text[i])): for k in range(len(text[i][j])): if text[i][j][k] == &quot;-&quot;: key = text[i][j][:k] value = text[i][j][k+1:] break if key not in mappings.keys(): mappings[key] = [value, ] elif value not in mappings[key]: mappings[key].append(value) with open(output, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f: for key, value in mappings.items(): f.write(str(key) + str(value) + &quot;\\n&quot;)mapping(file=&quot;token_source-target.align&quot;, output=&quot;token_source-target.txt&quot;)mapping(file=&quot;token_target-source.align&quot;, output=&quot;token_target-source.txt&quot;) 英语单词的对齐样例 12345I[&#x27;Ich&#x27;, &#x27;ich&#x27;, &#x27;mir&#x27;, &#x27;bin&#x27;, &#x27;Ihnen&#x27;, &#x27;Das&#x27;, &#x27;meiner&#x27;, &#x27;mich&#x27;, &#x27;Mir&#x27;, &#x27;halte&#x27;, &#x27;will&#x27;, &#x27;wünsche&#x27;, &#x27;habe&#x27;, &#x27;betrachte&#x27;, &#x27;daß&#x27;, &#x27;meine&#x27;, &#x27;meinen&#x27;, &#x27;Wie&#x27;, &#x27;möchte&#x27;, &#x27;tze&#x27;, &#x27;gehe&#x27;, &#x27;war&#x27;, &#x27;scheint&#x27;, &#x27;meinem&#x27;, &#x27;Dafürhalten&#x27;, &#x27;verstehe&#x27;, &#x27;hiermit&#x27;, &#x27;e&#x27;, &#x27;Hier&#x27;, &#x27;Den&#x27;, &#x27;br@@&#x27;, &#x27;inge&#x27;, &#x27;mache&#x27;, &#x27;stelle&#x27;, &#x27;darf&#x27;, &#x27;mein&#x27;, &#x27;Auch&#x27;, &#x27;sehe&#x27;, &#x27;Deshalb&#x27;, &#x27;Und&#x27;, &#x27;spreche&#x27;, &#x27;erlau@@&#x27;, &#x27;be&#x27;, &#x27;Außerdem&#x27;, &#x27;Was&#x27;, &#x27;In&#x27;, &#x27;kann&#x27;, &#x27;begrüße&#x27;, &#x27;Punkt&#x27;, &#x27;Darin&#x27;, &#x27;FR&#x27;, &#x27;sagen&#x27;, &#x27;Im&#x27;, &#x27;Da&#x27;, &#x27;Lassen&#x27;, &#x27;Meine&#x27;, &#x27;Damen&#x27;, &#x27;werde&#x27;, &#x27;komme&#x27;, &#x27;bitte&#x27;, &#x27;Gestatten&#x27;, &#x27;bedaure&#x27;, &#x27;stimme&#x27;, &#x27;lege&#x27;, &#x27;,&#x27;, &#x27;meines&#x27;, &#x27;Auf&#x27;, &#x27;nehme&#x27;, &#x27;finde&#x27;, &#x27;Dazu&#x27;, &#x27;Dabei&#x27;, &#x27;vertrete&#x27;, &#x27;Hier@@&#x27;, &#x27;danke&#x27;, &#x27;Ansicht&#x27;, &#x27;Darauf&#x27;, &#x27;aber&#x27;, &#x27;wollte&#x27;, &#x27;weise&#x27;, &#x27;Für&#x27;, &#x27;Mit&#x27;, &#x27;Meinung&#x27;, &#x27;Noch&#x27;, &#x27;Plenum&#x27;, &#x27;Nichts@@&#x27;, &#x27;diesem&#x27;, &#x27;jetzt&#x27;, &#x27;deswegen&#x27;, &#x27;Dies&#x27;, &#x27;befürworte&#x27;, &#x27;Leider&#x27;, &#x27;erkläre&#x27;, &#x27;Deswegen&#x27;, &#x27;hier&#x27;, &#x27;Ganz&#x27;, &#x27;muß&#x27;, &#x27;Als&#x27;, &#x27;I&#x27;, &#x27;Es&#x27;, &#x27;Meiner&#x27;, &#x27;gestatten&#x27;, &#x27;betone&#x27;, &#x27;erinnere&#x27;, &#x27;Herr&#x27;, &#x27;Herrn&#x27;, &#x27;teile&#x27;, &#x27;Meines&#x27;, &#x27;Erachtens&#x27;, &#x27;Damit&#x27;, &#x27;Darüber&#x27;, &#x27;Sitzung&#x27;, &#x27;Mehr&#x27;, &#x27;Präsident&#x27;, &#x27;sage&#x27;, &#x27;grei@@&#x27;, &#x27;schließe&#x27;, &#x27;M@@&#x27;, &#x27;freue&#x27;, &#x27;An&#x27;, &#x27;diesen&#x27;, &#x27;erwäh@@&#x27;, &#x27;glaube&#x27;, &#x27;Sehr&#x27;, &#x27;Vor&#x27;, &#x27;folgendes&#x27;, &#x27;gebe&#x27;, &#x27;Gerade&#x27;, &#x27;Doch&#x27;, &#x27;Mein&#x27;, &#x27;nochmals&#x27;, &#x27;einmal&#x27;, &#x27;liebe&#x27;, &#x27;unterstütze&#x27;, &#x27;Von&#x27;, &#x27;Zwar&#x27;, &#x27;zitiere&#x27;, &#x27;Wenn&#x27;, &#x27;Zu&#x27;, &#x27;Dies@@&#x27;, &#x27;hoffe&#x27;, &#x27;wage&#x27;, &#x27;stehe&#x27;, &#x27;hinweisen&#x27;, &#x27;Dennoch&#x27;, &#x27;Ausführungen&#x27;, &#x27;würde&#x27;, &#x27;denke&#x27;, &#x27;Obwohl&#x27;, &#x27;bekräf@@&#x27;, &#x27;verur@@&#x27;, &#x27;EN&#x27;, &#x27;Eigentlich&#x27;, &#x27;übrigen&#x27;, &#x27;Bei&#x27;, &#x27;Diese&#x27;, &#x27;sehr&#x27;, &#x27;Davon&#x27;, &#x27;Abgeordneter&#x27;, &#x27;wiederhole&#x27;, &#x27;Frau&#x27;, &#x27;daher&#x27;, &#x27;davon&#x27;, &#x27;mitteilen&#x27;, &#x27;persönlich&#x27;, &#x27;weiß&#x27;, &#x27;Aus&#x27;, &#x27;Zur&#x27;, &#x27;Gelegenheit&#x27;, &#x27;höre&#x27;, &#x27;trage&#x27;, &#x27;achte&#x27;, &#x27;fühle&#x27;, &#x27;aufmerksam&#x27;, &#x27;äuß@@&#x27;, &#x27;Insbesondere&#x27;, &#x27;Abgeordnete&#x27;, &#x27;wende&#x27;, &#x27;Aber&#x27;, &#x27;Zum&#x27;, &#x27;frage&#x27;, &#x27;Warum&#x27;, &#x27;ausdrücklich&#x27;, &#x27;verwende&#x27;, &#x27;unterstreichen&#x27;, &#x27;erkt&#x27;, &#x27;Wohl&#x27;, &#x27;fordere&#x27;, &#x27;erwarte&#x27;, &#x27;Ferner&#x27;, &#x27;schätze&#x27;, &#x27;suche&#x27;, &#x27;Äußerungen&#x27;, &#x27;hinzufügen&#x27;, &#x27;Weiterhin&#x27;, &#x27;gern&#x27;, &#x27;Angesichts&#x27;, &#x27;Soweit&#x27;, &#x27;beziehe&#x27;, &#x27;bewußt&#x27;, &#x27;Nun&#x27;, &#x27;fürchte&#x27;, &#x27;Bemerkung&#x27;, &#x27;Überzeugung&#x27;, &#x27;soeben&#x27;, &#x27;Daher&#x27;, &#x27;hätte&#x27;, &#x27;betrifft&#x27;, &#x27;darüber&#x27;, &#x27;soviel&#x27;, &#x27;Standpunkt&#x27;, &#x27;Abschließend&#x27;, &#x27;Des@@&#x27;, &#x27;vertrau@@&#x27;, &#x27;hatte&#x27;, &#x27;Zuerst&#x27;, &#x27;Daß&#x27;, &#x27;Zusammenhang&#x27;, &#x27;Persön@@&#x27;, &#x27;darauf&#x27;, &#x27;Damals&#x27;, &#x27;schlage&#x27;, &#x27;dankbar&#x27;, &#x27;erstaunt&#x27;, &#x27;Eine&#x27;, &#x27;Also&#x27;, &#x27;gehöre&#x27;, &#x27;kurz&#x27;, &#x27;Präsidentin&#x27;, &#x27;beg@@&#x27;, &#x27;inne&#x27;, &#x27;Vorredner&#x27;, &#x27;Worte&#x27;, &#x27;versichern&#x27;, &#x27;weshalb&#x27;, &#x27;Kollege&#x27;, &#x27;wohl&#x27;, &#x27;hne&#x27;, &#x27;Natürlich&#x27;, &#x27;fe&#x27;, &#x27;Dem&#x27;, &#x27;Sicht&#x27;, &#x27;Obgleich&#x27;, &#x27;Entschließung&#x27;, &#x27;Kollegen&#x27;, &#x27;Parlaments@@&#x27;, &#x27;Wissens&#x27;, &#x27;ßte&#x27;, &#x27;Nach&#x27;, &#x27;Zunächst&#x27;, &#x27;ha@@&#x27;, &#x27;Diesen&#x27;, &#x27;Auffassung&#x27;, &#x27;Dann&#x27;, &#x27;Wort@@&#x27;, &#x27;bean@@&#x27;, &#x27;Sicherlich&#x27;, &#x27;Ja&#x27;, &#x27;lasse&#x27;, &#x27;gesagt&#x27;, &#x27;Zudem&#x27;, &#x27;Herren&#x27;, &#x27;enne&#x27;, &#x27;Besonders&#x27;, &#x27;So@@&#x27;, &#x27;Hierzu&#x27;, &#x27;Dafür&#x27;, &#x27;Da@@&#x27;, &#x27;jedenfalls&#x27;, &#x27;Insofern&#x27;, &#x27;tu@@&#x27;, &#x27;anerk@@&#x27;, &#x27;anmerken&#x27;, &#x27;bemerken&#x27;, &#x27;schon&#x27;, &#x27;meldung&#x27;, &#x27;bedauere&#x27;, &#x27;einerseits&#x27;, &#x27;ere&#x27;, &#x27;brau@@&#x27;, &#x27;Allerdings&#x27;, &#x27;Frage&#x27;, &#x27;kenne&#x27;, &#x27;geehrte&#x27;, &#x27;Einer&#x27;, &#x27;abschließend&#x27;, &#x27;betonen&#x27;, &#x27;Ausgehend&#x27;, &#x27;Vielleicht&#x27;, &#x27;einverstanden&#x27;, &#x27;Rede@@&#x27;, &#x27;äußern&#x27;, &#x27;Vielen&#x27;, &#x27;Stelle&#x27;, &#x27;gestimmt&#x27;, &#x27;Des&#x27;, &#x27;rede&#x27;, &#x27;Eines&#x27;, &#x27;dachte&#x27;, &#x27;deshalb&#x27;, &#x27;Daran&#x27;, &#x27;pflich@@&#x27;, &#x27;Trotzdem&#x27;, &#x27;Änderungsanträgen&#x27;, &#x27;Kommissarin&#x27;, &#x27;erle@@&#x27;, &#x27;Vorbehalte&#x27;, &#x27;Bevor&#x27;, &#x27;hoffentlich&#x27;, &#x27;beglückwünsche&#x27;, &#x27;Schließlich&#x27;, &#x27;Zweitens&#x27;, &#x27;spare&#x27;, &#x27;Kommissionsmitglied&#x27;, &#x27;Abgeordneten&#x27;, &#x27;mag&#x27;, &#x27;aufrichtig&#x27;, &#x27;Kolleginnen&#x27;, &#x27;Sitzungs@@&#x27;, &#x27;Bedenken&#x27;, &#x27;hervorheben&#x27;, &#x27;erläu@@&#x27;, &#x27;Verständnis&#x27;, &#x27;he@@&#x27;, &#x27;feststellen&#x27;, &#x27;Hierbei&#x27;, &#x27;hierher&#x27;, &#x27;Um&#x27;, &#x27;Einen&#x27;, &#x27;mußte&#x27;, &#x27;verfol@@&#x27;, &#x27;Hinsichtlich&#x27;, &#x27;Genugtuung&#x27;, &#x27;wofür&#x27;, &#x27;danken&#x27;, &#x27;iere&#x27;, &#x27;bemerkt&#x27;, &#x27;behaup@@&#x27;, &#x27;diesbezüglich&#x27;, &#x27;Gleichwohl&#x27;, &#x27;gerne&#x27;, &#x27;muss&#x27;, &#x27;Weiteren&#x27;, &#x27;Bemerkungen&#x27;, &#x27;dass&#x27;, &#x27;betei@@&#x27;, &#x27;Einschätzung&#x27;, &#x27;Jetzt&#x27;, &#x27;Wort&#x27;, &#x27;Über&#x27;, &#x27;Wahrschein@@&#x27;, &#x27;Zugleich&#x27;, &#x27;respe@@&#x27;, &#x27;bekomm@@&#x27;, &#x27;Darum&#x27;, &#x27;ergrei@@&#x27;, &#x27;So&#x27;, &#x27;bestä@@&#x27;, &#x27;eingehen&#x27;, &#x27;Antwort&#x27;, &#x27;appelliere&#x27;, &#x27;lehne&#x27;, &#x27;akzep@@&#x27;, &#x27;tiere&#x27;, &#x27;Übrigen&#x27;, &#x27;beitrag&#x27;, &#x27;Dieser&#x27;, &#x27;Kön@@&#x27;, &#x27;Argumente&#x27;, &#x27;Dass&#x27;, &#x27;Je@@&#x27;, &#x27;denfalls&#x27;, &#x27;zweif@@&#x27;, &#x27;musste&#x27;, &#x27;letztes&#x27;, &#x27;Nein&#x27;, &#x27;empfehle&#x27;, &#x27;arbeit@@&#x27;, &#x27;erwähnen&#x27;, &#x27;erfahr@@&#x27;, &#x27;Ratspräsident&#x27;, &#x27;konnte&#x27;, &#x27;Ebenso&#x27;, &#x27;!&#x27;, &#x27;bezüglich&#x27;, &#x27;Heute&#x27;, &#x27;Bericht&#x27;, &#x27;anwesend&#x27;, &#x27;sagte&#x27;, &#x27;wiederholen&#x27;, &#x27;zunächst&#x27;, &#x27;beur@@&#x27;, &#x27;Folgendes&#x27;, &#x27;blei@@&#x27;, &#x27;außerordentlich&#x27;, &#x27;Kommissar&#x27;, &#x27;Dar@@&#x27;, &#x27;zäh@@&#x27;, &#x27;Berichterstatters&#x27;, &#x27;pfe&#x27;, &#x27;SV&#x27;, &#x27;Hoff@@&#x27;, &#x27;verehrte&#x27;, &#x27;einzugehen&#x27;, &#x27;Dieses&#x27;, &#x27;be@@&#x27;, &#x27;Welche&#x27;, &#x27;Dank&#x27;, &#x27;da&#x27;, &#x27;Andererseits&#x27;, &#x27;NL&#x27;, &#x27;Kann&#x27;, &#x27;beobach@@&#x27;, &#x27;gratuliere&#x27;, &#x27;rufe&#x27;, &#x27;Rat&#x27;, &#x27;vermutlich&#x27;, &#x27;Mitteilung&#x27;, &#x27;durfte&#x27;, &#x27;ausdrücken&#x27;, &#x27;Alles&#x27;, &#x27;vorliegenden&#x27;, &#x27;nachdrücklich&#x27;, &#x27;amtierenden&#x27;, &#x27;Ratspräsidenten&#x27;, &#x27;Hin@@&#x27;, &#x27;Zustimmung&#x27;, &#x27;übrigens&#x27;, &#x27;befür@@&#x27;, &#x27;chte&#x27;, &#x27;Ansonsten&#x27;, &#x27;erfreut&#x27;, &#x27;Ausdruck&#x27;, &#x27;Hinweis&#x27;, &#x27;Wertsch@@&#x27;, &#x27;klarstellen&#x27;, &#x27;rechn@@&#x27;, &#x27;-&#x27;, &#x27;kriti@@&#x27;, &#x27;Erwä@@&#x27;, &#x27;Kenntnis&#x27;, &#x27;Sicher&#x27;, &#x27;dies&#x27;, &#x27;Verheugen&#x27;, &#x27;chmals&#x27;, &#x27;Bitte&#x27;, &#x27;erinnern&#x27;, &#x27;anbelangt&#x27;, &#x27;daran&#x27;, &#x27;Selbstverständlich&#x27;, &#x27;anführen&#x27;, &#x27;Berichterstatter&#x27;, &#x27;Aussage&#x27;, &#x27;Ratspräsidentin&#x27;, &#x27;Ehr@@&#x27;, &#x27;Denn&#x27;, &#x27;wirklich&#x27;, &#x27;Präsidium&#x27;, &#x27;Unabhängi@@&#x27;, &#x27;Berichterstatterin&#x27;, &#x27;Nicht&#x27;, &#x27;Letz@@&#x27;, &#x27;Äuß@@&#x27;, &#x27;Anmerkung&#x27;, &#x27;Weil&#x27;, &#x27;erläutern&#x27;, &#x27;Liikanen&#x27;, &#x27;zutiefst&#x27;, &#x27;fas@@&#x27;, &#x27;itze&#x27;, &#x27;Punkte&#x27;, &#x27;versuche&#x27;, &#x27;füge&#x27;, &#x27;entschei@@&#x27;, &#x27;doch&#x27;, &#x27;noch&#x27;, &#x27;Aussprache&#x27;, &#x27;reiche&#x27;, &#x27;eigentlich&#x27;, &#x27;Anmerkungen&#x27;, &#x27;Moment&#x27;, &#x27;fand&#x27;, &#x27;drücklich&#x27;, &#x27;Vorschlag&#x27;, &#x27;hre&#x27;, &#x27;Worten&#x27;, &#x27;Lob&#x27;, &#x27;zuversichtlich&#x27;, &#x27;Zig@@&#x27;, &#x27;hole&#x27;, &#x27;antwor@@&#x27;, &#x27;liere&#x27;, &#x27;wissen&#x27;, &#x27;Appell&#x27;, &#x27;plä@@&#x27;, &#x27;bereits&#x27;, &#x27;überzeugt&#x27;, &#x27;schriftlich&#x27;, &#x27;Ein&#x27;, &#x27;entschuldigen&#x27;, &#x27;tre@@&#x27;, &#x27;Mitglied&#x27;, &#x27;Thema&#x27;, &#x27;ne&#x27;, &#x27;Verstän@@&#x27;, &#x27;verehrten&#x27;, &#x27;vermis@@&#x27;, &#x27;kunde&#x27;, &#x27;Barón&#x27;, &#x27;worten&#x27;, &#x27;froh&#x27;, &#x27;gefällt&#x27;, &#x27;auch&#x27;, &#x27;Bedauerlicherweise&#x27;, &#x27;Hohen&#x27;, &#x27;Vielmehr&#x27;, &#x27;bemü@@&#x27;, &#x27;wäre&#x27;, &#x27;benutz@@&#x27;, &#x27;schrei@@&#x27;, &#x27;vermu@@&#x27;, &#x27;Rede&#x27;, &#x27;ehrlich&#x27;, &#x27;Mein@@&#x27;, &#x27;messe&#x27;, &#x27;herz@@&#x27;, &#x27;Sinne&#x27;, &#x27;ganz&#x27;, &#x27;wünschte&#x27;, &#x27;Erstens&#x27;, &#x27;Beim&#x27;, &#x27;DA&#x27;, &#x27;Bezüglich&#x27;, &#x27;etwas&#x27;, &#x27;durchaus&#x27;, &#x27;verweisen&#x27;, &#x27;Fraktion&#x27;, &#x27;gangs&#x27;, &#x27;bitten&#x27;, &#x27;Aufgrund&#x27;, &#x27;Weiter&#x27;, &#x27;Gemeinsam&#x27;, &#x27;Gestern&#x27;, &#x27;Ausschusses&#x27;, &#x27;Gleichzeitig&#x27;, &#x27;Kommissars&#x27;, &#x27;zieh@@&#x27;, &#x27;so&#x27;, &#x27;letzt&#x27;, &#x27;te&#x27;, &#x27;Fraktionen&#x27;, &#x27;amtierender&#x27;, &#x27;fest&#x27;, &#x27;was&#x27;, &#x27;Hoffnung&#x27;, &#x27;weil&#x27;, &#x27;Eindruck&#x27;, &#x27;einige&#x27;, &#x27;antworten&#x27;, &#x27;dringlich&#x27;, &#x27;freut&#x27;, &#x27;berichten&#x27;, &#x27;Frattini&#x27;, &#x27;emp@@&#x27;, &#x27;Nur&#x27;, &#x27;Ist&#x27;, &#x27;Einwanderungspolitik&#x27;, &#x27;entlich&#x27;, &#x27;wus@@&#x27;, &#x27;beglückwünschen&#x27;, &#x27;Liebe&#x27;, &#x27;allerdings&#x27;, &#x27;beitra@@&#x27;, &#x27;Hab@@&#x27;, &#x27;soweit&#x27;, &#x27;damals&#x27;, &#x27;Moment@@&#x27;, &#x27;angeht&#x27;, &#x27;vorhin&#x27;, &#x27;verstanden&#x27;, &#x27;gefallen&#x27;, &#x27;Mandelson&#x27;, &#x27;Wor@@&#x27;, &#x27;Era@@&#x27;, &#x27;grü@@&#x27;, &#x27;Grunde&#x27;, &#x27;Einwände&#x27;, &#x27;Vertreterin&#x27;, &#x27;erklären&#x27;, &#x27;Werde&#x27;, &#x27;überein&#x27;, &#x27;erwähnte&#x27;, &#x27;Schluss&#x27;, &#x27;spiele&#x27;, &#x27;geehrter&#x27;, &#x27;sicher&#x27;, &#x27;zustimmen&#x27;, &#x27;Debatte&#x27;, &#x27;worüber&#x27;, &#x27;ermaßen&#x27;, &#x27;das&#x27;, &#x27;Übri@@&#x27;, &#x27;Vizepräsident&#x27;, &#x27;Kommission&#x27;, &#x27;benö@@&#x27;, &#x27;Ihre&#x27;, &#x27;gelesen&#x27;, &#x27;es&#x27;, &#x27;Insgesamt&#x27;, &#x27;Kollegin&#x27;, &#x27;Wo&#x27;, &#x27;PT&#x27;, &#x27;ermu@@&#x27;, &#x27;diesmal&#x27;, &#x27;Et@@&#x27;, &#x27;ansprechen&#x27;, &#x27;Glückwunsch&#x27;, &#x27;Leid&#x27;, &#x27;drücke&#x27;, &#x27;Ob&#x27;, &#x27;Zweifel@@&#x27;, &#x27;Lange&#x27;, &#x27;Man&#x27;, &#x27;zeige&#x27;, &#x27;gemeldet&#x27;, &#x27;plane&#x27;, &#x27;Redezeit&#x27;, &#x27;dieser&#x27;, &#x27;Plenar@@&#x27;, &#x27;üs@@&#x27;, &#x27;Genau&#x27;, &#x27;möge&#x27;, &#x27;spür@@&#x27;, &#x27;tim@@&#x27;, &#x27;zurückkommen&#x27;, &#x27;nu@@&#x27;, &#x27;fragte&#x27;, &#x27;Dür@@&#x27;, &#x27;Gedanken&#x27;, &#x27;dear&#x27;, &#x27;All&#x27;, &#x27;Stellungnahme&#x27;, &#x27;Umweltfragen&#x27;, &#x27;Verfasser&#x27;, &#x27;Überlegungen&#x27;, &#x27;nun&#x27;, &#x27;zol@@&#x27;, &#x27;Vorsitz&#x27;, &#x27;Schattenberichterstat@@&#x27;, &#x27;geäußer@@&#x27;, &#x27;anschließen&#x27;, &#x27;blicke&#x27;, &#x27;selber&#x27;, &#x27;Abschluß&#x27;, &#x27;stimmt&#x27;, &#x27;protesti@@&#x27;, &#x27;tut&#x27;, &#x27;Schluß&#x27;, &#x27;Bedauern&#x27;, &#x27;Parlaments&#x27;, &#x27;keines@@&#x27;, &#x27;Geschäftsordnung&#x27;, &#x27;wü@@&#x27;, &#x27;Entschuldigung&#x27;, &#x27;Parlament&#x27;, &#x27;nach&#x27;, &#x27;Änderungsanträge&#x27;, &#x27;aufgreifen&#x27;, &#x27;jedoch&#x27;, &#x27;führ@@&#x27;, &#x27;Viertens&#x27;, &#x27;Glück@@&#x27;, &#x27;hierzu&#x27;, &#x27;Plen@@&#x27;, &#x27;Angelegenheit&#x27;, &#x27;Empfehlung&#x27;, &#x27;Anbetracht&#x27;, &#x27;Befür@@&#x27;, &#x27;Präsidentschaft&#x27;, &#x27;festhalten&#x27;, &#x27;chmal&#x27;, &#x27;läßlich&#x27;, &#x27;Schon&#x27;, &#x27;ihn&#x27;, &#x27;Gegen&#x27;, &#x27;Nachdem&#x27;, &#x27;insofern&#x27;, &#x27;Feststellung&#x27;, &#x27;Freude&#x27;, &#x27;Befürwor@@&#x27;, &#x27;guilty&#x27;, &#x27;Möglicherweise&#x27;, &#x27;recht&#x27;, &#x27;schick@@&#x27;, &#x27;zufügen&#x27;, &#x27;allererst&#x27;, &#x27;Beitrag&#x27;, &#x27;vorgetragen&#x27;, &#x27;Mulder&#x27;, &#x27;Sache&#x27;, &#x27;We&#x27;, &#x27;talking&#x27;, &#x27;proo@@&#x27;, &#x27;fing&#x27;, &#x27;Anwesenheit&#x27;, &#x27;gave&#x27;, &#x27;think&#x27;, &#x27;receiving&#x27;, &#x27;downloaded&#x27;, &#x27;ordered&#x27;, &#x27;bought&#x27;, &#x27;Some&#x27;, &#x27;Manchmal&#x27;, &#x27;When&#x27;, &#x27;Beauti@@&#x27;, &#x27;ebenso&#x27;, &#x27;received&#x27;, &#x27;This&#x27;, &#x27;Thank&#x27;, &#x27;nice&#x27;, &#x27;really&#x27;, &#x27;gladly&#x27;, &#x27;ouldn&#x27;, &#x27;che&#x27;, &#x27;After&#x27;, &#x27;irgendwie&#x27;, &#x27;seems&#x27;, &#x27;It&#x27;, &#x27;allo&#x27;, &#x27;Which&#x27;, &#x27;Ebenfalls&#x27;, &#x27;kümm@@&#x27;, &#x27;remembered&#x27;, &#x27;The&#x27;, &#x27;Gute&#x27;, &#x27;someone&#x27;, &#x27;probably&#x27;, &#x27;Glei@@&#x27;, &#x27;My&#x27;, &#x27;promptly&#x27;, &#x27;How&#x27;, &#x27;always&#x27;, &#x27;love&#x27;, &#x27;Will&#x27;, &#x27;ately&#x27;, &#x27;dafür&#x27;, &#x27;Everything&#x27;, &#x27;Just&#x27;, &#x27;Maybe&#x27;, &#x27;actually&#x27;, &#x27;caught&#x27;, &#x27;father&#x27;, &#x27;Hi&#x27;, &#x27;ICH&#x27;, &#x27;toll&#x27;, &#x27;worse&#x27;, &#x27;le&#x27;, &#x27;Anyone&#x27;, &#x27;just&#x27;, &#x27;supposed&#x27;, &#x27;since&#x27;, &#x27;Obviously&#x27;, &#x27;wasn&#x27;, &#x27;Then&#x27;, &#x27;ese&#x27;, &#x27;Why&#x27;, &#x27;myself&#x27;, &#x27;Well&#x27;, &#x27;wrote&#x27;, &#x27;knew&#x27;, &#x27;Have&#x27;, &#x27;me&#x27;, &#x27;gerade&#x27;, &#x27;But&#x27;, &#x27;Any&#x27;, &#x27;went&#x27;, &#x27;nennen&#x27;, &#x27;could&#x27;, &#x27;Thanks&#x27;, &#x27;yesterday&#x27;, &#x27;There&#x27;, &#x27;Vergan@@&#x27;, &#x27;Good&#x27;, &#x27;tried&#x27;, &#x27;shame&#x27;, &#x27;stayed&#x27;, &#x27;You&#x27;, &#x27;Nice&#x27;, &#x27;happened&#x27;, &#x27;denn&#x27;, &#x27;Aussagen&#x27;, &#x27;While&#x27;, &#x27;have&#x27;, &#x27;Our&#x27;, &#x27;agree&#x27;, &#x27;besorgt&#x27;, &#x27;sah&#x27;, &#x27;my&#x27;, &#x27;importantly&#x27;, &#x27;bedanken&#x27;, &#x27;angesprochenen&#x27;, &#x27;beschäf@@&#x27;, &#x27;Muss&#x27;, &#x27;enttäuscht&#x27;, &#x27;Help&#x27;, &#x27;gestern&#x27;, &#x27;richtig&#x27;, &#x27;Behauptung&#x27;, &#x27;Hohe&#x27;, &#x27;bekam&#x27;, &#x27;Folgen@@&#x27;, &#x27;Glücklicherweise&#x27;, &#x27;I.&#x27;, &#x27;trotzdem&#x27;, &#x27;Nehmen&#x27;, &#x27;Schließ@@&#x27;, &#x27;hope&#x27;, &#x27;Folglich&#x27;, &#x27;Vorsitzender&#x27;, &#x27;Although&#x27;, &#x27;would&#x27;, &#x27;Not&#x27;, &#x27;very&#x27;, &#x27;euch&#x27;, &#x27;schau@@&#x27;, &#x27;gesprochen&#x27;, &#x27;vorstellen&#x27;, &#x27;beabsichti@@&#x27;, &#x27;mal&#x27;]resumed[&#x27;am&#x27;, &#x27;aufgenommen&#x27;, &#x27;15.00&#x27;, &#x27;wieder@@&#x27;, &#x27;21.@@&#x27;, &#x27;Donnerstag&#x27;, &#x27;unterbrochen&#x27;, &#x27;um&#x27;, &#x27;wieder&#x27;, &#x27;Wiederaufnahme&#x27;, &#x27;00&#x27;, &#x27;fortgesetzt&#x27;, &#x27;wurden&#x27;, &#x27;Uhr&#x27;, &#x27;15.@@&#x27;, &#x27;Aussprache&#x27;, &#x27;15.&#x27;, &#x27;nahm&#x27;, &#x27;resumed&#x27;, &#x27;è&#x27;, &#x27;pres@@&#x27;, &#x27;Freitag&#x27;, &#x27;13.&#x27;, &#x27;September&#x27;, &#x27;2005&#x27;]Friday[&#x27;Freitag&#x27;, &#x27;Am&#x27;, &#x27;am&#x27;, &#x27;Woche&#x27;, &#x27;:&#x27;, &#x27;Wochenende&#x27;, &#x27;bis&#x27;, &#x27;stag&#x27;, &#x27;fre@@&#x27;, &#x27;it@@&#x27;, &#x27;ags&#x27;, &#x27;Freit@@&#x27;, &#x27;ag@@&#x27;, &#x27;s@@&#x27;, &#x27;stattfinden&#x27;, &#x27;unseren&#x27;, &#x27;sitzung&#x27;, &#x27;vergangenen&#x27;, &#x27;Sitzung&#x27;, &#x27;kommenden&#x27;, &#x27;stattfindet&#x27;, &#x27;Sitz@@&#x27;, &#x27;stag@@&#x27;, &#x27;agen&#x27;, &#x27;andt&#x27;, &#x27;ag&#x27;, &#x27;jeweils&#x27;, &#x27;Samstag&#x27;, &#x27;Montag&#x27;, &#x27;vom&#x27;, &#x27;statt&#x27;, &#x27;gangenen&#x27;, &#x27;vor@@&#x27;, &#x27;abend&#x27;, &#x27;mittag&#x27;, &#x27;sam@@&#x27;, &#x27;Wochen@@&#x27;, &#x27;end@@&#x27;, &#x27;zwungen&#x27;, &#x27;Tag&#x27;, &#x27;tagen&#x27;, &#x27;sitz@@&#x27;, &#x27;Zum&#x27;, &#x27;tage&#x27;, &#x27;Sitzungen&#x27;, &#x27;Protokoll&#x27;, &#x27;aufgenommen&#x27;, &#x27;anwesend&#x27;, &#x27;itage&#x27;, &#x27;Der&#x27;, &#x27;Friday&#x27;, &#x27;täglich&#x27;, &#x27;findet&#x27;, &#x27;Sowohl&#x27;, &#x27;Juli&#x27;, &#x27;True&#x27;, &#x27;24.@@&#x27;, &#x27;7.@@&#x27;, &#x27;ven@@&#x27;, &#x27;dre@@&#x27;, &#x27;21.@@&#x27;, &#x27;Jeden&#x27;, &#x27;Uhr&#x27;, &#x27;Donnerstag&#x27;, &#x27;M@@&#x27;, &#x27;tags&#x27;, &#x27;2008&#x27;, &#x27;12.&#x27;, &#x27;2007&#x27;, &#x27;15.00&#x27;, &#x27;mitt@@&#x27;, &#x27;EST&#x27;, &#x27;10&#x27;, &#x27;di&#x27;, &#x27;jeden&#x27;, &#x27;besetzt&#x27;, &#x27;r.&#x27;, &#x27;Mon@@&#x27;, &#x27;März&#x27;, &#x27;Besu@@&#x27;, &#x27;Fei@@&#x27;, &#x27;geöffnet&#x27;, &#x27;tes@@&#x27;, &#x27;serviert&#x27;, &#x27;30&#x27;, &#x27;Januar&#x27;, &#x27;Sunday&#x27;, &#x27;00&#x27;, &#x27;Straßburg&#x27;, &#x27;03&#x27;, &#x27;Kar@@&#x27;, &#x27;09&#x27;, &#x27;pm&#x27;, &#x27;Tuesday&#x27;, &#x27;.00&#x27;, &#x27;mentary&#x27;, &#x27;Dienstag&#x27;, &#x27;26&#x27;, &#x27;ì&#x27;, &#x27;Mittag@@&#x27;, &#x27;Abend&#x27;, &#x27;fuhren&#x27;, &#x27;Shop&#x27;, &#x27;Mittwoch&#x27;, &#x27;zeiten&#x27;, &#x27;morgen&#x27;, &#x27;3&#x27;, &#x27;max&#x27;, &#x27;Sonntag&#x27;, &#x27;13.&#x27;, &#x27;September&#x27;, &#x27;Juni&#x27;, &#x27;Diesel@@&#x27;, &#x27;wertung&#x27;, &#x27;Von&#x27;, &#x27;Bern&#x27;, &#x27;morgens&#x27;]17[&#x27;17.&#x27;, &#x27;17&#x27;, &#x27;vom&#x27;, &#x27;:&#x27;, &#x27;sieb@@&#x27;, &#x27;zehn&#x27;, &#x27;17@@&#x27;, &#x27;;&#x27;, &#x27;@-@&#x27;, &#x27;17.@@&#x27;, &#x27;Sie@@&#x27;, &#x27;bis&#x27;, &#x27;insgesamt&#x27;, &#x27;.-@@&#x27;, &#x27;Jahren&#x27;, &#x27;Nr.&#x27;, &#x27;16.&#x27;, &#x27;19&#x27;, &#x27;am&#x27;, &#x27;jährigen&#x27;, &#x27;(&#x27;, &#x27;)&#x27;, &#x27;9&#x27;, &#x27;1@@&#x27;, &#x27;25&#x27;, &#x27;21&#x27;, &#x27;27&#x27;, &#x27;10&#x27;, &#x27;knapp&#x27;, &#x27;18&#x27;, &#x27;1&#x27;, &#x27;zehn@@&#x27;, &#x27;14&#x27;, &#x27;h&#x27;, &#x27;20&#x27;, &#x27;45&#x27;, &#x27;Uhr&#x27;, &#x27;06&#x27;, &#x27;Erdbeben&#x27;, &#x27;18.&#x27;, &#x27;28&#x27;, &#x27;93&#x27;, &#x27;7&#x27;, &#x27;32&#x27;, &#x27;16&#x27;, &#x27;Februar&#x27;, &#x27;Januar&#x27;, &#x27;15&#x27;, &#x27;33&#x27;, &#x27;37&#x27;, &#x27;09&#x27;, &#x27;00&#x27;, &#x27;30&#x27;, &#x27;11&#x27;, &#x27;Euro&#x27;, &#x27;77&#x27;, &#x27;92&#x27;, &#x27;November&#x27;, &#x27;2@@&#x27;, &#x27;September&#x27;, &#x27;12@@&#x27;, &#x27;112&#x27;, &#x27;12&#x27;, &#x27;41&#x27;, &#x27;2&#x27;, &#x27;31&#x27;, &#x27;23&#x27;, &#x27;6&#x27;, &#x27;08&#x27;, &#x27;22&#x27;, &#x27;35&#x27;, &#x27;24&#x27;, &#x27;um&#x27;, &#x27;44&#x27;, &#x27;5&#x27;, &#x27;07&#x27;, &#x27;Ju@@&#x27;, &#x27;8&#x27;, &#x27;/&#x27;, &#x27;Mai&#x27;, &#x27;2003&#x27;, &#x27;Jahr&#x27;, &#x27;04&#x27;, &#x27;Paris&#x27;, &#x27;v&#x27;, &#x27;last&#x27;, &#x27;statt&#x27;, &#x27;Juni&#x27;, &#x27;3@@&#x27;, &#x27;Alter&#x27;, &#x27;Air&#x27;, &#x27;34&#x27;, &#x27;stehung&#x27;, &#x27;August&#x27;, &#x27;15@@&#x27;, &#x27;term&#x27;, &#x27;Jahres&#x27;, &#x27;nan@@&#x27;, &#x27;Oktober&#x27;, &#x27;18@@&#x27;, &#x27;4@@&#x27;, &#x27;6@@&#x27;, &#x27;Juli&#x27;, &#x27;Dezember&#x27;, &#x27;13&#x27;, &#x27;19@@&#x27;, &#x27;20.&#x27;, &#x27;2006&#x27;, &#x27;47&#x27;, &#x27;blatt&#x27;, &#x27;December&#x27;, &#x27;27@@&#x27;, &#x27;Vereinbarung&#x27;, &#x27;95&#x27;, &#x27;54&#x27;, &#x27;00@@&#x27;, &#x27;03&#x27;, &#x27;9.&#x27;, &#x27;page&#x27;, &#x27;2009&#x27;, &#x27;3&#x27;, &#x27;mbol&#x27;, &#x27;June&#x27;, &#x27;43&#x27;, &#x27;*&#x27;, &#x27;000&#x27;]December[&#x27;Dezember&#x27;, &#x27;vom&#x27;, &#x27;am&#x27;, &#x27;letzten&#x27;, &#x27;Jahres&#x27;, &#x27;Dez@@&#x27;, &#x27;em@@&#x27;, &#x27;ber@@&#x27;, &#x27;12.@@&#x27;, &#x27;vergangenen&#x27;, &#x27;im&#x27;, &#x27;12.&#x27;, &#x27;hat&#x27;, &#x27;bis&#x27;, &#x27;soll&#x27;, &#x27;zum&#x27;, &#x27;1999&#x27;, &#x27;unterzeichnet&#x27;, &#x27;Anfang&#x27;, &#x27;vorgelegt&#x27;, &#x27;hatte&#x27;, &#x27;legte&#x27;, &#x27;wurde&#x27;, &#x27;stattgefunden&#x27;, &#x27;des@@&#x27;, &#x27;Monat&#x27;, &#x27;Einigung&#x27;, &#x27;ember&#x27;, &#x27;Im&#x27;, &#x27;aufgenommen&#x27;, &#x27;ab&#x27;, &#x27;abgeschlossen&#x27;, &#x27;verabschiedeten&#x27;, &#x27;Kraft&#x27;, &#x27;laufen&#x27;, &#x27;2001&#x27;, &#x27;verabschiedet&#x27;, &#x27;Am&#x27;, &#x27;September&#x27;, &#x27;statt&#x27;, &#x27;December&#x27;, &#x27;trat&#x27;, &#x27;nete&#x27;, &#x27;z&#x27;, &#x27;2002&#x27;, &#x27;begrü@@&#x27;, &#x27;kam&#x27;, &#x27;worden&#x27;, &#x27;2000&#x27;, &#x27;De@@&#x27;, &#x27;nahm&#x27;, &#x27;15.&#x27;, &#x27;übergeben&#x27;, &#x27;beschlossen&#x27;, &#x27;tag@@&#x27;, &#x27;angenommen&#x27;, &#x27;vorzulegen&#x27;, &#x27;getreten&#x27;, &#x27;seiner&#x27;, &#x27;Tagung&#x27;, &#x27;waren&#x27;, &#x27;gebilligt&#x27;, &#x27;unterbreit@@&#x27;, &#x27;ete&#x27;, &#x27;stattfand&#x27;, &#x27;stattfindet&#x27;, &#x27;3.&#x27;, &#x27;Staats-&#x27;, &#x27;Regierungschefs&#x27;, &#x27;2003&#x27;, &#x27;erneut&#x27;, &#x27;dieses&#x27;, &#x27;verkün@@&#x27;, &#x27;dete&#x27;, &#x27;2004&#x27;, &#x27;vorlegen&#x27;, &#x27;erreichte&#x27;, &#x27;2005&#x27;, &#x27;Gültigkeit&#x27;, &#x27;5.&#x27;, &#x27;Rates&#x27;, &#x27;2006&#x27;, &#x27;bestätigt&#x27;, &#x27;13.&#x27;, &#x27;2007&#x27;, &#x27;veröffentlicht&#x27;, &#x27;1.&#x27;, &#x27;19.&#x27;, &#x27;31.&#x27;, &#x27;angekün@@&#x27;, &#x27;embers&#x27;, &#x27;angenommene&#x27;, &#x27;unterzeichnen&#x27;, &#x27;kommenden&#x27;, &#x27;mussten&#x27;, &#x27;erfolgte&#x27;, &#x27;Ab&#x27;, &#x27;stattfinden&#x27;, &#x27;Sitzung&#x27;, &#x27;rat&#x27;, &#x27;endete&#x27;, &#x27;2010&#x27;, &#x27;festgelegt&#x27;, &#x27;1995&#x27;, &#x27;1996&#x27;, &#x27;datum&#x27;, &#x27;7.&#x27;, &#x27;gültig&#x27;, &#x27;1997&#x27;, &#x27;Ende&#x27;, &#x27;eingereicht&#x27;, &#x27;8.&#x27;, &#x27;selben&#x27;, &#x27;ECOFIN&#x27;, &#x27;Oktober&#x27;, &#x27;17.&#x27;, &#x27;Monats&#x27;, &#x27;November&#x27;, &#x27;Erklärung&#x27;, &#x27;1998&#x27;, &#x27;Forderungen&#x27;, &#x27;verabschie@@&#x27;, &#x27;spätestens&#x27;, &#x27;Inneres&#x27;, &#x27;konferenz&#x27;, &#x27;Betrieb&#x27;, &#x27;16.&#x27;, &#x27;29.&#x27;, &#x27;23.&#x27;, &#x27;27.&#x27;, &#x27;bre&#x27;, &#x27;ami&#x27;, &#x27;9.&#x27;, &#x27;4.&#x27;, &#x27;ey&#x27;, &#x27;26.&#x27;, &#x27;2.&#x27;, &#x27;hatten&#x27;, &#x27;14.&#x27;, &#x27;10.&#x27;, &#x27;US&#x27;, &#x27;Heinrich&#x27;, &#x27;2008&#x27;, &#x27;beschäf@@&#x27;, &#x27;August&#x27;, &#x27;22.&#x27;, &#x27;erklärte&#x27;, &#x27;April&#x27;, &#x27;stag&#x27;, &#x27;3@@&#x27;, &#x27;bunal&#x27;, &#x27;nachdem&#x27;, &#x27;Kriegs@@&#x27;, &#x27;seit&#x27;, &#x27;2012&#x27;, &#x27;igte&#x27;, &#x27;2009&#x27;, &#x27;race&#x27;, &#x27;Weihnachts@@&#x27;, &#x27;Verlei@@&#x27;, &#x27;hung&#x27;, &#x27;1.@@&#x27;, &#x27;Monaten&#x27;, &#x27;20.&#x27;, &#x27;Dé@@&#x27;, &#x27;geleistet&#x27;, &#x27;Weihnachten&#x27;, &#x27;Januar&#x27;, &#x27;25.&#x27;, &#x27;Februar&#x27;, &#x27;Bis&#x27;, &#x27;Vom&#x27;, &#x27;18.&#x27;, &#x27;Der&#x27;, &#x27;abgehalten&#x27;, &#x27;01&#x27;, &#x27;12&#x27;, &#x27;Vorbereitungen&#x27;, &#x27;farbe&#x27;, &#x27;fassten&#x27;, &#x27;stimm@@&#x27;] 德语单词的对齐样例 12Ich[&#x27;I&#x27;, &#x27;It&#x27;, &#x27;like&#x27;, &#x27;Madam&#x27;, &#x27;My&#x27;, &#x27;saying&#x27;, &#x27;should&#x27;, &#x27;In&#x27;, &#x27;As&#x27;, &#x27;But&#x27;, &#x27;would&#x27;, &#x27;want&#x27;, &#x27;Let&#x27;, &#x27;wish&#x27;, &#x27;At&#x27;, &#x27;shall&#x27;, &#x27;There&#x27;, &#x27;am&#x27;, &#x27;This&#x27;, &#x27;On&#x27;, &#x27;see&#x27;, &#x27;Having&#x27;, &#x27;Commissioner&#x27;, &#x27;will&#x27;, &#x27;that&#x27;, &#x27;What&#x27;, &#x27;me&#x27;, &#x27;do&#x27;, &#x27;have&#x27;, &#x27;Like&#x27;, &#x27;honourable&#x27;, &#x27;Well&#x27;, &#x27;Allow&#x27;, &#x27;agree&#x27;, &#x27;myself&#x27;, &#x27;When&#x27;, &#x27;So&#x27;, &#x27;And&#x27;, &#x27;hope&#x27;, &#x27;also&#x27;, &#x27;believe&#x27;, &#x27;think&#x27;, &#x27;For&#x27;, &#x27;my&#x27;, &#x27;SV&#x27;, &#x27;Mr&#x27;, &#x27;President&#x27;, &#x27;May&#x27;, &#x27;note&#x27;, &#x27;sympathy&#x27;, &#x27;urge&#x27;, &#x27;DE&#x27;, &#x27;However&#x27;, &#x27;With&#x27;, &#x27;wholeheartedly&#x27;, &#x27;Indeed&#x27;, &#x27;remind&#x27;, &#x27;welcome&#x27;, &#x27;To&#x27;, &#x27;commend&#x27;, &#x27;Although&#x27;, &#x27;Moreover&#x27;, &#x27;wonder&#x27;, &#x27;(&#x27;, &#x27;FR&#x27;, &#x27;mention&#x27;, &#x27;gentlemen&#x27;, &#x27;PT&#x27;, &#x27;Without&#x27;, &#x27;Perhaps&#x27;, &#x27;All&#x27;, &#x27;Ladies&#x27;, &#x27;Can&#x27;, &#x27;Please&#x27;, &#x27;give&#x27;, &#x27;Here&#x27;, &#x27;must&#x27;, &#x27;Could&#x27;, &#x27;say&#x27;, &#x27;Together&#x27;, &#x27;Now&#x27;, &#x27;Yet&#x27;, &#x27;While&#x27;, &#x27;feel&#x27;, &#x27;If&#x27;, &#x27;suggest&#x27;, &#x27;Just&#x27;, &#x27;ladies&#x27;, &#x27;Further&#x27;, &#x27;certainly&#x27;, &#x27;That&#x27;, &#x27;Certainly&#x27;, &#x27;Spe@@&#x27;, &#x27;aking&#x27;, &#x27;Next&#x27;, &#x27;personally&#x27;, &#x27;accept&#x27;, &#x27;really&#x27;, &#x27;thoughts&#x27;, &#x27;understand&#x27;, &#x27;refer&#x27;, &#x27;Yes&#x27;, &#x27;Instead&#x27;, &#x27;therefore&#x27;, &#x27;Another&#x27;, &#x27;just&#x27;, &#x27;repeat&#x27;, &#x27;speak&#x27;, &#x27;favour&#x27;, &#x27;very&#x27;, &#x27;One&#x27;, &#x27;strongly&#x27;, &#x27;By&#x27;, &#x27;NL&#x27;, &#x27;Along&#x27;, &#x27;Furthermore&#x27;, &#x27;appreciate&#x27;, &#x27;Will&#x27;, &#x27;applau@@&#x27;, &#x27;know&#x27;, &#x27;hear&#x27;, &#x27;regret&#x27;, &#x27;Whilst&#x27;, &#x27;argue&#x27;, &#x27;Finally&#x27;, &#x27;suspect&#x27;, &#x27;Firstly&#x27;, &#x27;Of&#x27;, &#x27;emphasise&#x27;, &#x27;Once&#x27;, &#x27;support&#x27;, &#x27;grateful&#x27;, &#x27;call&#x27;, &#x27;Following&#x27;, &#x27;sincerely&#x27;, &#x27;Person@@&#x27;, &#x27;Naturally&#x27;, &#x27;Tur@@&#x27;, &#x27;Mrs&#x27;, &#x27;point&#x27;, &#x27;From&#x27;, &#x27;Thank&#x27;, &#x27;Since&#x27;, &#x27;Be&#x27;, &#x27;Quite&#x27;, &#x27;opinion&#x27;, &#x27;ally&#x27;, &#x27;fact&#x27;, &#x27;gratul@@&#x27;, &#x27;Very&#x27;, &#x27;tell&#x27;, &#x27;cannot&#x27;, &#x27;ask&#x27;, &#x27;Neither&#x27;, &#x27;pleased&#x27;, &#x27;Above&#x27;, &#x27;conclude&#x27;, &#x27;endorse&#x27;, &#x27;Therefore&#x27;, &#x27;Nevertheless&#x27;, &#x27;Some&#x27;, &#x27;listened&#x27;, &#x27;thank&#x27;, &#x27;Rather&#x27;, &#x27;delighted&#x27;, &#x27;acknowledge&#x27;, &#x27;recognise&#x27;, &#x27;quote&#x27;, &#x27;share&#x27;, &#x27;firmly&#x27;, &#x27;trust&#x27;, &#x27;Lastly&#x27;, &#x27;owe&#x27;, &#x27;Regarding&#x27;, &#x27;Unlike&#x27;, &#x27;No&#x27;, &#x27;reiterate&#x27;, &#x27;look&#x27;, &#x27;imagine&#x27;, &#x27;writing&#x27;, &#x27;happy&#x27;, &#x27;glad&#x27;, &#x27;wanted&#x27;, &#x27;invite&#x27;, &#x27;Where&#x27;, &#x27;referring&#x27;, &#x27;Any@@&#x27;, &#x27;ij&#x27;, &#x27;Consequently&#x27;, &#x27;express&#x27;, &#x27;Not&#x27;, &#x27;pay&#x27;, &#x27;draw&#x27;, &#x27;Being&#x27;, &#x27;Again&#x27;, &#x27;declare&#x27;, &#x27;After&#x27;, &#x27;recall&#x27;, &#x27;Then&#x27;, &#x27;sal@@&#x27;, &#x27;Today&#x27;, &#x27;gladly&#x27;, &#x27;Would&#x27;, &#x27;reply&#x27;, &#x27;Nor&#x27;, &#x27;Do&#x27;, &#x27;thinking&#x27;, &#x27;confirm&#x27;, &#x27;stress&#x27;, &#x27;IT&#x27;, &#x27;said&#x27;, &#x27;apolog@@&#x27;, &#x27;explain&#x27;, &#x27;genuinely&#x27;, &#x27;Taking&#x27;, &#x27;First&#x27;, &#x27;Nonetheless&#x27;, &#x27;sorry&#x27;, &#x27;attention&#x27;, &#x27;voted&#x27;, &#x27;hereby&#x27;, &#x27;Even&#x27;, &#x27;Surely&#x27;, &#x27;hear@@&#x27;, &#x27;How&#x27;, &#x27;comments&#x27;, &#x27;.-&#x27;, &#x27;Ich&#x27;, &#x27;During&#x27;, &#x27;convinced&#x27;, &#x27;pointing&#x27;, &#x27;My@@&#x27;, &#x27;Commission&#x27;, &#x27;this&#x27;, &#x27;&amp;apos;m&#x27;, &#x27;simply&#x27;, &#x27;Obviously&#x27;, &#x27;realise&#x27;, &#x27;Thanks&#x27;, &#x27;behalf&#x27;, &#x27;objection&#x27;, &#x27;assure&#x27;, &#x27;emph@@&#x27;, &#x27;dare&#x27;, &#x27;Wallström&#x27;, &#x27;seems&#x27;, &#x27;Defin@@&#x27;, &#x27;SK&#x27;, &#x27;PL&#x27;, &#x27;ALDE&#x27;, &#x27;EL&#x27;, &#x27;Member&#x27;, &#x27;rapporteur&#x27;, &#x27;Verts&#x27;, &#x27;inform&#x27;, &#x27;HU&#x27;, &#x27;approve&#x27;, &#x27;ES&#x27;, &#x27;fully&#x27;, &#x27;intend&#x27;, &#x27;to&#x27;, &#x27;Have&#x27;, &#x27;honest@@&#x27;, &#x27;depl@@&#x27;, &#x27;convey&#x27;, &#x27;Me&#x27;, &#x27;congratulate&#x27;, &#x27;abstained&#x27;, &#x27;Hence&#x27;, &#x27;More&#x27;, &#x27;add&#x27;, &#x27;Frank@@&#x27;, &#x27;emphasize&#x27;, &#x27;Take&#x27;, &#x27;comment&#x27;, &#x27;I.&#x27;, &#x27;disagree&#x27;, &#x27;consider&#x27;, &#x27;conf@@&#x27;, &#x27;Amendment&#x27;, &#x27;Did&#x27;, &#x27;congratulations&#x27;, &#x27;speaking&#x27;, &#x27;echo&#x27;, &#x27;prefer&#x27;, &#x27;She&#x27;, &#x27;Hon@@&#x27;, &#x27;&amp;apos;d&#x27;, &#x27;)&#x27;, &#x27;don&#x27;, &#x27;ego&#x27;, &#x27;&amp;apos;ve&#x27;, &#x27;Und&#x27;, &#x27;Happ@@&#x27;, &#x27;Excellent&#x27;, &#x27;thought&#x27;, &#x27;Wir&#x27;, &#x27;Hi&#x27;, &#x27;Kann&#x27;, &#x27;Hop@@&#x27;, &#x27;Je&#x27;, &#x27;propose&#x27;, &#x27;Sor@@&#x27;, &#x27;&amp;apos;ll&#x27;, &#x27;keen&#x27;, &#x27;Danke&#x27;, &#x27;Still&#x27;, &#x27;sehe&#x27;]erkläre[&#x27;declare&#x27;, &#x27;resumed&#x27;, &#x27;session&#x27;, &#x27;explain&#x27;, &#x27;ad@@&#x27;, &#x27;state&#x27;, &#x27;say&#x27;, &#x27;announce&#x27;, &#x27;confirm&#x27;, &#x27;hereby&#x27;, &#x27;open&#x27;, &#x27;shall&#x27;, &#x27;declar@@&#x27;, &#x27;I&#x27;, &#x27;jour@@&#x27;, &#x27;ned&#x27;, &#x27;inge&#x27;, &#x27;closed&#x27;, &#x27;affir@@&#x27;, &#x27;concluded&#x27;, &#x27;agree&#x27;, &#x27;explains&#x27;, &#x27;the&#x27;, &#x27;196&#x27;, &#x27;03&#x27;] 接下来，我有如下的几个发现: fast align 是可以建模出正确的对齐的I→Ich erkläre→declare ...; fast align 在大规模数据集上制造了太多的不正确对齐关系I→nice really gladly ...。 1.3 个人想法总的来说，fast align 能够起到对齐的作用，但它的对齐准确率显然不高。所以，我觉得如果只是那 fast align 去指导一个完全没有中间结构来辅助的翻译模型的话，该模型的翻译质量能够得到有效的提升；但是，如果想在 NAT 方面促成一个 SOTA 成绩的非自回归翻译模型的话，恐怕是心有余而力不足了。 2 GIZA++2.1 检查词对齐文本数据从之前展示的一些文件内容来看，我发现 一些文件en2de.A3.final en2de.n3.final ...的含义明显要比 fast align 所生成的对齐文件内容要丰富很多，如en2de.n3.final文件可以更好地观察单词的繁殖力、en2de.A3.final文件可以更好地观察整个句子的对齐情况； 虽然随着迭代次数的增加, 我们可以发现en2de.perp对齐的困惑度有在减小，但是最终的困惑度仍然很高(≈90)，而且我们还可以发现隐马尔可夫模型的困惑度是最低的，而不是最终迭代的IBM Model 4； 有些文件en2de.d3.final en2de.a3.final ...的内容对于新手而言根本看不懂是什么意思，过于晦涩； en2de.A3.final文件中I (&#123; 1 &#125;) declare (&#123; 2 &#125;)说明模型能够建模正确的对齐关系(‘I’对齐’Ich’, ‘declare’对齐’erkläre’)，NULL (&#123; 3 6 7 15 17 23 25 27 &#125;) the (&#123; &#125;)说明模型还是会有漏掉对齐的情况存在(‘die’表示’这’，’the’应该是对齐’die’的)，I (&#123; 1 &#125;) would (&#123; &#125;)说明模型存在错误对齐的情况(‘Könnten’可以与’would’对齐，但是模型却将’I’对齐到’Könnten’)； 2.2 个人想法GIZA++的对齐能力明显优于fast align，但这也意味着更多的时间开销，并且GIZA++的对齐中也有不正确的情况存在。总的来说，如果用GIZA++就需要在获取对齐时花费更多的时间，但是能够在性能上取得一定的提升。","categories":[{"name":"Statistical Machine Translation","slug":"Statistical-Machine-Translation","permalink":"http://example.com/categories/Statistical-Machine-Translation/"}],"tags":[{"name":"Alignment","slug":"Alignment","permalink":"http://example.com/tags/Alignment/"}]}],"categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"},{"name":"Combinatorial Mathematices","slug":"Combinatorial-Mathematices","permalink":"http://example.com/categories/Combinatorial-Mathematices/"},{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"},{"name":"Neural Machine Translation","slug":"Neural-Machine-Translation","permalink":"http://example.com/categories/Neural-Machine-Translation/"},{"name":"Speech Recognition","slug":"Speech-Recognition","permalink":"http://example.com/categories/Speech-Recognition/"},{"name":"Statistical Machine Translation","slug":"Statistical-Machine-Translation","permalink":"http://example.com/categories/Statistical-Machine-Translation/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"},{"name":"Combinatorial Mathematices","slug":"Combinatorial-Mathematices","permalink":"http://example.com/tags/Combinatorial-Mathematices/"},{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"},{"name":"NMT","slug":"NMT","permalink":"http://example.com/tags/NMT/"},{"name":"CTC","slug":"CTC","permalink":"http://example.com/tags/CTC/"},{"name":"Alignment","slug":"Alignment","permalink":"http://example.com/tags/Alignment/"}]}