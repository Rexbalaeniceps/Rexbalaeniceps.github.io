{"meta":{"title":"Sylvanas Forever","subtitle":"自然语言处理在读研究生","description":"wbxu's blog","author":"wbxu","url":"http://example.com","root":"/"},"pages":[{"title":"","date":"2015-08-16T06:58:08.000Z","updated":"2021-11-24T08:26:17.551Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"Hello, everyone.大家好，我是一名苏州大学在读的硕士研究生，我选的方向是自然语言处理。 这个博客主要是用于发表一些我的实践经历和论文阅读，才疏学浅，若有错误还请指正。"},{"title":"Categories","date":"2016-08-16T07:00:44.000Z","updated":"2021-12-28T03:08:49.121Z","comments":false,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2016-08-11T04:12:45.000Z","updated":"2021-11-21T07:40:51.587Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Automatic Essay Scoring Incorporating Rating Schema via Reinforcement Learning","slug":"Automatic-Essay-Scoring-Incorporating-Rating-Schema-via-Reinforcement-Learning","date":"2022-08-25T06:20:20.000Z","updated":"2022-08-25T06:27:49.337Z","comments":true,"path":"2022/08/25/Automatic-Essay-Scoring-Incorporating-Rating-Schema-via-Reinforcement-Learning/","link":"","permalink":"http://example.com/2022/08/25/Automatic-Essay-Scoring-Incorporating-Rating-Schema-via-Reinforcement-Learning/","excerpt":"","text":"提出问题每次预测文章分数时并没有考虑到评分的模式 predict the score of each single essay at a time without considering the rating schema 文章模型 作文表示作者采用了两种不同的方案来建模作文表示向量, 但都是基于 LSTM Bi-LSTM 两层双向 LSTM 网络进行语义建模 计算两层网络产生的所有隐层表示向量的均值 将两层的平均状态拼接以作为最终的作文表示向量 In particular, the average value over all hidden states of each LSTM layer are computed, and we concatenate the mean states of the two layers together as the embedding vector of the essay Dilated LSTM 使用 Dilated LSTM 代替普通 LSTM, 因为该网络更擅长处理长文本 拼接每一层隐层状态的均值来形成作文表示向量 simply concatenate the average hidden states of every layer toform the essay embedding 作文评分 回归 Linear + Sigmoid Mean Square Error 分类 Linear + Softmax Cross Entropy Loss 惩罚系数 作者认为交叉熵损失并没能隐含类别间的差异, 因此在交叉熵损失上额外添加一个惩罚系数, 其定义如下: $$ \\large p_i = \\frac{(i - score)^2}{(N - 1)^2} $$ 混合评分 作者联合训练一个回归和一个分类来预测分数, 以期望能够有助于分类评分收敛, 总体损失函数如下: $$ \\Large \\mathcal{loss}{pre} = \\alpha_0 \\cdot \\mathcal{loss}{MSE} + \\alpha_1 \\cdot \\mathcal{loss}{RL} + \\beta_0 \\mathcal{loss}{CE} + \\gamma_0 \\mathcal{loss}_{p} $$ 强化学习因为回归任务并不支持强化学习的策略梯度, 因此上述的回归评分只辅助分类, 同时强化学习过程也只在分类上进行。强化学习旨在将 QWK 作为指导来优化评分系统。 性能结果","categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"}]},{"title":"Unsupervised Learning of Discourse-Aware Text Representation for Essay Scoring","slug":"Unsupervised-Learning-of-Discourse-Aware-Text-Representation-for-Essay-Scoring","date":"2022-02-14T23:37:44.000Z","updated":"2022-02-14T23:38:36.994Z","comments":true,"path":"2022/02/15/Unsupervised-Learning-of-Discourse-Aware-Text-Representation-for-Essay-Scoring/","link":"","permalink":"http://example.com/2022/02/15/Unsupervised-Learning-of-Discourse-Aware-Text-Representation-for-Essay-Scoring/","excerpt":"现有的文档嵌入方法主要集中在捕捉文档中的词的序列，而诸如作文评分这类任务则需要考虑文档的篇章结构；之前的方法利用文本的篇章结构进行文档分类，但其依赖于计算成本较高的解析器。因此作者提出了一种无监督的方法，以连贯性和内聚性来捕捉篇章结构，并用于文档嵌入表示，这就不再需要任何昂贵的解析器或注释了；实证表明该方法得到的文档表示能够改善文章组织评分 (essay Organization scoring) 和论证力度评分 (Argument Strength scoring) 的性能。","text":"现有的文档嵌入方法主要集中在捕捉文档中的词的序列，而诸如作文评分这类任务则需要考虑文档的篇章结构；之前的方法利用文本的篇章结构进行文档分类，但其依赖于计算成本较高的解析器。因此作者提出了一种无监督的方法，以连贯性和内聚性来捕捉篇章结构，并用于文档嵌入表示，这就不再需要任何昂贵的解析器或注释了；实证表明该方法得到的文档表示能够改善文章组织评分 (essay Organization scoring) 和论证力度评分 (Argument Strength scoring) 的性能。 Features 使用无监督方法 建模了作文篇章中的连贯性和凝聚力 针对文章组织评分和论证力度评分提升模型性能 Methods模型基础模型包含 $3$ 个部分： 基础文档编码器，由嵌入层、双向 LSTM、MOT 池化层构成，它通过**建模作文中的词序列来产生向量表示 $h^{base}$**； 辅助编码器，**负责捕捉额外作文相关的信息来产生向量表示 $h^{aux}$**； 段落功能编码器：由嵌入层、双向 LSTM 构成，负责建模作文中的段落功能序列 提示语编码器：由嵌入层、LSTM 构成，负责建模作文对应提示语中的信息 打分函数，将以上的两个表示向量作为输入，从而为作文输出一个预测分数$$\\Large score = Sigmoid\\big(\\mathbf{w} \\cdot tanh(\\mathbf{W} \\cdot [ h^{base}; h^{aux} ]) + b \\big)$$ 方法作者的整体流程分为预训练和微调两个步骤： 预训练 (Unsupervised manner) 通过人工破坏原作文的方法，作者构造负样本；即原始作文被认为是连贯且凝聚的，其标签为 $1$，而损坏作文则被认为是不连贯且不凝聚的，故其标签为 $0$。打乱篇章指示词能够破坏原作文的凝聚力，而打乱段落顺序则能够破坏原作文的连贯性，因此作者将损坏作文作为预训练的负例。在这个过程中，作者首先是在大规模数据集上进行预训练，然后又将模型转到作文评分数据集上进行继续预训练，旨在缓解模型在大规模数据集和目标数据集上的不一致。 训练 (Supervised manner) 在目标数据集上以有监督的方式微调预训练过的编码器，使其适用于作文评分任务。 Analyses设置 数据集 训练数据集：International Corpus of Learner English 数据集中的 Organization scores 和 Argument Strength scores 部分，共使用 $1003$ 篇作文 预训练数据集包括：ASAP 数据集、TOEFL11 数据集、ICNALE 数据集、ICLE 数据集的剩余部分 损失函数： 二元交叉熵 均方误差 模型选择：$5$ - 折交叉验证 结果 作者结论： 无监督预训练提高了组织和论证强度评分的性能表现，证明了其有效性 用随机损坏的文档进行训练有助于文档编码器学习带有逻辑顺序意识的文本表示方法 在大多数情况下，针对每个评分任务再次对编码器进行微调有助于提高性能 打乱段落在这两项计分任务中都是最有效的，这可能是由于段落顺序创造了一个更清晰的组织和论证结构 如引言-正文-结论，这样的抽象层次在段落层面上可以很好地捕捉到，但仅仅在句子层面或DI层面上却不能 鉴于 DIs 的识别不准确及其不一定会导致文章变为不凝聚的缘故，DIs 方面还有待改进 附1 - Unsupervised Learning of Discourse-Aware Text Representation for Essay Scoring “ACL 2019” 附2 - DiscoShuffle","categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"}]},{"title":"Enriching the ASAP Automated Essay Grading Dataset with Essay Attribute Scores","slug":"Enriching-the-ASAP-Automated-Essay-Grading-Dataset-with-Essay-Attribute-Scores","date":"2022-02-14T12:23:58.000Z","updated":"2022-02-14T12:25:24.280Z","comments":true,"path":"2022/02/14/Enriching-the-ASAP-Automated-Essay-Grading-Dataset-with-Essay-Attribute-Scores/","link":"","permalink":"http://example.com/2022/02/14/Enriching-the-ASAP-Automated-Essay-Grading-Dataset-with-Essay-Attribute-Scores/","excerpt":"虽然之前的学者在整体论文评分方面做了很多工作，但在论文的特定属性评分方面做得不多。因此，作者构造的 ASAP++ 数据集基本上是 ASAP 的注释，但这些注释是对论文的不同属性的评分，如内容、选词、组织、句子流畅性等。此外，作者还报告了使用随机森林分类器对每个属性的结果，该分类器使用了 Zesch等人所描述的属性独立特征基线集。","text":"虽然之前的学者在整体论文评分方面做了很多工作，但在论文的特定属性评分方面做得不多。因此，作者构造的 ASAP++ 数据集基本上是 ASAP 的注释，但这些注释是对论文的不同属性的评分，如内容、选词、组织、句子流畅性等。此外，作者还报告了使用随机森林分类器对每个属性的结果，该分类器使用了 Zesch等人所描述的属性独立特征基线集。 Features 为 ASAP 数据集中作文的各方面属性进行评分 有利于开发针对特点属性的 AES 系统 Methods首先，ASAP 数据集的统计数字如下： 作者为不同体裁的文章设置了不同的属性集合，如下： 议论文 Attribute Description $\\mathbf{Content}$ 文章中出现相关文本的数量 $\\mathbf{Organization}$ 文章的结构组织方式 $\\mathbf{Word\\ \\ Choice}$ 文章中的选词及其贴切性 $\\mathbf{Sentence\\ \\ Fluency}$ 作文中的句子质量 $\\mathbf{Conventions}$ 文章应遵循的总体写作惯例，如拼写、标点符号等 回复文 Attribute Description $\\mathbf{Content}$ 文章中出现相关文本的数量 $\\mathbf{Prompt\\ \\ Adherence}$ 对作者如何紧扣文题的衡量 $\\mathbf{Language}$ 答复中语法与拼写的质量 $\\mathbf{Narrativity}$ 对回复针对提示的连贯性和凝聚力的衡量 作者对每篇文章只使用一名注释者，并充分利用 ASAP 数据集的原始注释者对文章的总体评分。如果对某一特定提示的特定属性的评分与原始评分者的评分相差 $2$ 分或更多，那么这篇论文将由另一位评分者进行评分；而最终选择的分数是来自注释者的最接近总体分数的分数。原因之一就是在两个由原始评分者评分的提示中，总体分数和个别属性分数之间有非常高的皮尔逊相关性 (约 $0.9$)。 作者总共使用了 $3$ 位注释者来注释这些文章，而其中每位注释者的英语能力，要么在高中考试中获得相当高的分数 (英语成绩超过 $90%$ )，要么在托福考试中获得了 $110$ 分以上的成绩。他们每个人都有一些评估文本的经验，例如在 The Hindu7 实习，或是大学杂志的主编，等等。此外，所有的注释者都至少有文学硕士水平的英语。 AnalysesSettings 使用 Stanford Core NLP 工具抽取诸多特征 Results 作者结论： 由于特征集是专门为论文的总分而设计的，所以预计总分实现了最好的结果 注释者面临的一个主要问题是：所有论文都是匿名的 对回复文，对内容最重要的特征是长度，而对于议论文，则是连贯性和凝聚力特征，其次才是长度；这主要是因为回复文高度依赖于源文本，而议论文则可以利用来自任何文本范围之外的论据，故这些论据必须是连贯和凝聚的 对回复文，连贯性和凝聚力特征集是其他 $3$ 个属性中最重要的特征集；虽然叙述性是对文本的连贯性和凝聚力的衡量，但语言和对提示的遵守得分也恰好受此影响；这主要是因为回复文应该切合提示 对议论文，连贯性和凝聚力特征是 $5$ 个属性中最重要的特征；这主要是因为连贯性和凝聚力对论证的组织很重要 句法特征被认为是句子流畅性的最重要特征，因为它们衡量了文章中各个句子的写作水平 总的来说，最重要的特征是一致性和内聚性特征 附1 - Enriching the ASAP Automated Essay Grading Dataset with Essay Attribute Scores “LREC 2018” 附2 - Task-Independent Features for Automated Essay Grading “属性独立特征” 附3 - ASAP++ “好像打不开？”","categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"}]},{"title":"Hierarchical Multi-task Learning for Organization Evaluation of Argumentative Student Essays","slug":"Hierarchical-Multi-task-Learning-for-Organization-Evaluation-of-Argumentative-Student-Essays","date":"2022-02-13T15:01:28.000Z","updated":"2022-02-14T06:16:59.861Z","comments":true,"path":"2022/02/13/Hierarchical-Multi-task-Learning-for-Organization-Evaluation-of-Argumentative-Student-Essays/","link":"","permalink":"http://example.com/2022/02/13/Hierarchical-Multi-task-Learning-for-Organization-Evaluation-of-Argumentative-Student-Essays/","excerpt":"现有的方法将篇章元素和结构识别区分开来，而作者提出了一种神经分层多任务学习方法，用于联合优化句子和段落级别的语篇元素识别和组织评估，他将组织表示为一个网格以模拟文章的视觉布局，并在多个语言层面整合篇章元素。实验结果表明了这种方法能够取得显著的性能提升，并且多层语篇元素识别通过互相强化也从多任务学习中获益匪浅。","text":"现有的方法将篇章元素和结构识别区分开来，而作者提出了一种神经分层多任务学习方法，用于联合优化句子和段落级别的语篇元素识别和组织评估，他将组织表示为一个网格以模拟文章的视觉布局，并在多个语言层面整合篇章元素。实验结果表明了这种方法能够取得显著的性能提升，并且多层语篇元素识别通过互相强化也从多任务学习中获益匪浅。 Features 自建数据集 以句子功能和段落功能识别为辅助任务的层次化神经多任务学习方法 一种组织的网格表示，以整合来自多层次话语元素的监督和表示 针对于作文自动评分的组织评分方面 Methods语篇元素作者在本文中将句子功能和段落功能并称为语篇元素 (discourse elements)，具体如下表： Sentence Functions Sentence Function Description Introduction 介绍背景或在提出主张之前吸引读者的注意 Thesis 表达了作者对文章主题的核心主张 Main Idea 阐述了与核心主张相关的基础性观点或方面 Evidence 指出用于支持主要观点和主张的例子或其他类型的证据 Elaboration 进一步解释主要观点或证据，但不包含证据 Conclusion 总结文章全文，呼应或延伸核心主张 Paragraph Functions Paragraph Function Description IntroductionPara 包含介绍性句子，但不能包含主张或论点的句子 ThesisPara 至少包含一个主张句 IdeaPara 至少包含一个论点句，但不能包含主张句 SupportPara 包含证据或阐述句子，但不能包含论点、主张、总结句 ConclusionPara 含有结论句子，但不能包含主张句 模型 Sentence Model作者使用 BiLSTM 来建模单词序列，并将双方向的隐藏表示拼接起来以得到每个单词的最终隐藏表示。然后通过对同一句子中单词的隐藏表示取均值得到上下文表示 $c$ (content representation)，其维度为 $d$。 Sentence Function Identification因为句子的位置对于其功能而言之也十分重要，因此作者在该模块中额外考虑了位置编码，如下： Global position：句子在整个文章中的绝对位置 Paragraph position：句子所在段落在文章中的绝对位置 Local position：句子在段落中的相对位置作者效仿 Transformer 使用正余弦波的形式建模位置，并通过一个线性层来将这 $3$ 种位置编码整合到一起。 故该模块的流程为： 上下文表示 $C$ 将会加上位置编码 $pos$，并送入到下一个 BiLSTM 模型中去 此处，作者使用了一个 BiLSTM 和一个非线性层来建模上下文化的句子表示 通过一个线性层和 Softmax 来获得模型的概率分布 模型的损失函数为负对数似然的平均值 $\\mathcal{L}_{SFI}$ Paragraph Function Identification作者决定预测一个段落的功能，而不是从其句子功能中推导出来，是因为希望以此来加强不同语言层次之间的互动。 该模块的流程为： 上下文化句子表示 $D$ 将被送入一个 BiLSTM 层 因段落的功能取决于其内部句子功能，所以作者在这儿使用了注意力机制来捕捉关键句子功能 通过一个线性层和 Softmax 来获得模型的概率分布 模型的损失函数为负对数似然的平均值 $\\mathcal{L}_{PFI}$ Organization Evaluation作者提出使用网格组织表示法来将段落功能和句子功能恰当地结合起来以进行组织评价。 如上图所示，所谓的网格组织表示法就是构建一张表格，该表格的每一行都表示了一个段落；而每行的首个位置需要存放之前生成的段落功能表示向量，后续的位置则依次存放各个句子的功能表示向量；最后，将每一行都填充到最大长度形成一个段落最大数量 $m$ 乘以句子最大数量 $n_p + 1$ 的矩阵。因此，我们不难知道这个网格组织表示的最终形状为 $d × m × (n_p + 1)$。 该模块的流程为： 通过句子功能表示和段落功能表示构造出网格组织表示张量 将该张量喂给 $2$ 个 2d CNN 块以抽取组织表示中的特征 卷积操作是沿着网格平面进行的，并使用最大池化层和 relu 激活函数来建模特征图 特征图最终会被转换成一个向量，经由一个线性层和 Softmax 层来获取概率分布 模型的损失函数为负对数似然的平均值 $\\mathcal{L}_{OE}$ Loss Function模型的最终损失函数如下，其中超参数 $\\gamma$ 用于控制 Organization Evaluation 任务的相对重要性$$\\large \\mathcal{L} = \\mathcal{L}{SFI} + \\mathcal{L}{PFI} + \\gamma \\cdot \\mathcal{L}_{OE}$$ 作者动态地更新 $\\gamma$ 的值并设其初值为 $0.1$，以期模型先专注于低级任务，当低级任务性能提升后则渐渐转向高级任务$$\\large \\gamma = max(min(\\frac{\\mathcal{L}{OE}}{\\mathcal{L}{SFI}} \\cdot \\gamma, 1), 0.01)$$ Analyses数据集对于组织质量，作者划分了 $3$ 个标签： Bad：文章结构很差，即不完整或遗漏了关键的语篇元素 Medium：文章结构良好，内容完整，但还可以进一步改进 Graet：文章的结构相当好，组织非常清晰，逻辑性强 作者雇佣两个两位高中中文老师来标注功能标签，同时他们也会为文章组织进行打分；每当两位标注者产生分歧时，就会有第三位标注者参与标注。该自建数据集的统计数字如下，其中人类的 acc 为 $80%$、F1 值为 $77%$、评分者内部一致性 (基于 Kappa) 为 $0.73$： 设置 最大句子数 $n = 50$ 最大段落数 $m = 20$ 最大段内句子数 $n_p = 20$ 结果Sentence Function Identification作者结论： 单一任务模型 SFI 优于 HiBiLSTM，很大程度上表明了位置编码的有效性 多任务学习下的模型取得了最佳成绩，很大程度上证明了多任务学习的有效性 Paragraph Function Identification作者结论： 联合训练SFI和PFI可以达到最佳性能 用与SFI+PFI相同的结构直接预测段落功能，其性能很差，这表明 SFI 监督的重要性 Organization Evaluation作者结论： 作者的大多数模型以及基于多任务学习的变体的性能表现都很优秀，这表明考虑语篇元素是很重要的 SFI+PFI+OE 模型取得了最佳的宏观 F1 分数和 MAE，其 MSE 也还行，这表明联合模型在结合段落函数提供的高水平抽象和句子函数提供的细节方面保持了更好的平衡 基于多任务学习的模型获得了更好的宏观 F1 分数，因为其识别少数等级 (bad &amp; great) 方面更加有效 与 SeqCNN 和 SeqLSTM 相比，GridCNN 获得了优越的性能，这表明网格组织表示是代表组织评估的层次化语篇元素的有效方式 讨论 组织评估可以从多任务学习中受益，表明具有多级监督的端到端模型可以避免遭受次优的媒介表征和错误传播。 结合 SFI 和 PFI，在多任务学习环境中增强了 OE，而段落函数标签很容易得到；也就是说分层结构可以以很小的代价提高模型能力 语篇元素识别从多任务学习中获益，这主要来自于 SFI 和 PFI 之间的相互作用 组织网格显示是有效的，其原因可能是网格表示法恢复了文章的视觉布局，并抓住了更大的接受领域 除了相邻句子的序列模式外，还可以捕捉到多个段落的句子之间的关系，从而使清晰而有规律的结构获得奖励 附1 - Hierarchical Multi-task Learning for Organization Evaluation of Argumentative Student Essays “IJCAI 2020”","categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"}]},{"title":"RESEARCH ON MODEL AND METHOD OF AUTOMATED ESSAY SCORING","slug":"RESEARCH-ON-MODEL-AND-METHOD-OF-AUTOMATED-ESSAY-SCORING","date":"2022-02-13T09:15:09.000Z","updated":"2022-02-13T09:20:24.185Z","comments":true,"path":"2022/02/13/RESEARCH-ON-MODEL-AND-METHOD-OF-AUTOMATED-ESSAY-SCORING/","link":"","permalink":"http://example.com/2022/02/13/RESEARCH-ON-MODEL-AND-METHOD-OF-AUTOMATED-ESSAY-SCORING/","excerpt":"哈工大的硕士毕业论文，三个方法还是涉及到了比较多的特征工程，算是特征工程和深度网络结合的方法，后面还实现了一个网页版的自动评分系统。论文中的模型融合方法获得了最高的 QWK 值。","text":"哈工大的硕士毕业论文，三个方法还是涉及到了比较多的特征工程，算是特征工程和深度网络结合的方法，后面还实现了一个网页版的自动评分系统。论文中的模型融合方法获得了最高的 QWK 值。 Features 综合使用了深度学习方法和特征方法 语言、内容、逻辑、合题等多个方面进行建模 特征工程比较大 一个作文评分网页，而且附带纠错功能 MethodsDiscourse-level Features based作者采用基于机器学习方法的评分模型，将抽取的浅层语言学特征和语义特征合并作为作文篇章级别的特征表示，训练随机森林模型，再对新的作文的质量进行预测，得到最终的得分。因为 ASAP 数据集中各个作文集存在着较大的差异，所以作者对其中每一个数据集都构造了不同的模型以研究各自的表现效果。 篇章级特征 浅层语言学特征：在一定程度上，这些特征能够反映出学生功底、作文内容、写作质量 词法特征 句法特征 语法正确性特征：基于现有的 N-gram 字典对作文的语法进行检测 语义特征：这些特征能够反应作文的语义等深层信息 LDA 主题建模 主题模型是对文本隐含的主题建模的一种方法，目的是发现隐含在文档中的主题结构，它不仅可以降低文本表示的维度，还使得文本表示中蕴含了语义信息。主题模型包括了 LSA、pLSA、LDA 等。LDA 是一种基于全概率的生成模型，旨在通过 “文档-单词” 分布计· 算求解 “文档-主题” 分布和 “主题-单词” 分布，具体则是通过引入贝叶斯假设在先验分布的条件下对参数做最大后验估计。但是值得注意的是，LDA 主题建模没有涉及到单词上下文的语境。 词聚类特征 Word2Vec 词向量 Word2vec 利用了词的上下文，蕴含的语义信息更加地丰富，可以挖掘词之间的关系。 Word2Vec 词聚类Word2Vec 模型将词汇表示成词向量后，由于作文文本长度不一，因此需要基于词向量抽取一个统一长度的篇章向量。作者采用了布朗聚类算法来对词向量进行分类，进而根据词汇对应的类别获取作文篇章在各个类别下的词汇分布情况。词聚类特征可以一定程度上反映篇章的词汇语义分布情况，进而反映作文的内容质量。 随机森林模型 随机森林模型（Random Forest，RF）是一种基于决策树和 Bagging 方法的集成模型，通过训练 $n$ 颗相互独立的决策树对样本进行投票，根据投票结果获得分类结果。其中的决策树模型是树形结构的分类器。每个树的非叶子节点代表了对于一个特征的划分，每个分支代表符合当前节点划分范围的输出，每个叶子节点中存放具体的类别或结果值。决策树模型预测时首先从根节点开始，测试待决策样本中相应的特征属性，并选择满足条件的分支进行划分，直到到达叶子节点，将叶子节点中存储的类别或值作为决策结果。 随机森林模型由多颗决策树组合得到，其随机性主要体现在两个方面：1) 采用 bootstrap 方法从样本集合 $S$ 中有放回的随机抽取 $|S|$ 个样本训练每颗决策树；2) 每次从总体特征集合 $F$ 中随机抽取特征子集 $f$（$f&lt;&lt;F$）训练决策树，依据 $|F|$ 的大小，特征子集 $f$ 中的特征数量 $|f|$ 通常选取 $sqrt(|F|)$、$log(|F|)$等。 由于随机森林采用 bootstrap 方法，因此构建的决策树即使不采用剪枝策略也可以很好的避免过拟合。同时，由于决策树建树时使用的特征集合是从原始特征集中随机抽取出的特征子集，因此每个基分类器既可以很好的表征样本的某方面特点，也存在着一定的差异性。这也使得最终通过 Bagging 方式组合得到的随机森林模型对于不同问题，通常都会有不错的学习能力及泛化能力。 随机森林模型来自于 scikit-learn 工具包。 Sentences Representations based主要挖掘作文中包含的上下文信息，将作文篇章表示成语句序列，代入深度学习模型中进行训练以构建句向量，此外还尝试了几种自动抽取句子特征的方法，并结合句子级别的启发式规则抽取有代表性的时序特征。 句向量 启发式规则特征 基础词法、句法特征：结合抽取浅层语言学特征的方法抽取句子特征 连接词特征：连接词是句子之间关联性一个重要的方面，也可以反应语篇前后的语义关系；作者基于以下 $6$ 类的连接词对句子进行统计，计算出每一类下包含的连接词个数，为句子构造一个 6 维的向量，每一维对应的值为该类别下的连接词数量 情感特征：情感是评估作文质量的一个重要因素，情感变化可以在一定程度上反映篇章的语义的变化；作者通过提取句子级别的情感特征，挖掘篇章中隐藏的情感变化信息 情感词特征：基于情感词典抽取 情感极性特征：CoreNLP 引擎可以很详细分析出句子的情感极性，分成 $5$ 种类别：非常消极、消极、中立、积极、非常积极；根据句子所属类别为句子构建一个维度为 $5$ 的特征向量，所属类别对应的值为 $1$，其余置为 $0$ 词向量组合：作者提出了一种极值词向量组合方法，也就是选择句子中的词向量在每个维度上的最大值和最小值，拼接它们得到句向量；作者认为 Word2Vec 频繁词相对于不频繁词训练出的词向量更接近于零向量，因此，通过这种极大值、极小值的组合方式可以挖掘更多的不频繁词信息，而在作文中不频繁词一定程度可以反映作文语义方面的质量，它又相当于在平均语义信息上补充了不频繁词的语义信息，从而使得构建的句向量包含更多的语义信息 Doc2Vec：Word2Vec 主要基于词汇层面进行语义分析，一定程度上缺乏上下文语义分析的能力，使用 Word2Vec 组合构建的句向量表示的语义信息比较有限，为了表示可变长度文本的向量，引入了 Doc2Vec 模型；Word2Vec 模型主要增加了一个段落向量，也包括两种模型结构：DM 和 DBOW；作者使用 Doc2Vec 方法构建句向量的实验中，使用到的句向量一种通过 DM 模型进行训练，一种通过 DBOW 模型训练，最后将两种句向量进行拼接以得到最终的句向量表示 递归自编码器：RAE 模型通过结合递归神经网络和自编码模型，构成了一个无监督的句子表示模型，可以将不定长的句子转化成相同维度的句向量 深度学习模型 CNN-based LSTM-based 模型融合 为了构建更好的作文评分模型，作者将深度学习方法与传统机器学习方法的模型进行融合，从而充分利用作文中包含的信息，更全面的衡量作文的质量。 基于特征的融合基于特征的融合方法即对作文各个角度的特征进行融合，将合并的特征代入一个分类器中进行训练；但是作者当前抽取的基于句子表示的特征具有时序性信息，如果简单地与之前的篇章特征进行拼接融合的话，会导致失去较多的上下文信息因此，作者构建了一个多输入的深度学习模型，利用深度学习模型中间层的输出来表示篇章的上下文特征，再与篇章特征进行拼接，输入到多层神经网络里，最终得到 softmax 分类器的输出结果，作为评分模型最终的得分；通过这种特征组合方式，一方面可以充分利用作文在篇章语言、内容方面的质量特征，另一方面也不会损失作文逻辑方面的信息 基于决策的融合根据以下模型在验证集上的表现结果决定权重，再根据这些权重对最终的结果进行加权投票 Topic Relevance basedASAP 标注语料中，主要包括说明文和材料作文两种体裁；其中说明文的题目文本通常是一段短文本。材料作文的题目文本则是大篇幅的材料文章。作者在该章节只研究讨论说明文体裁的主题相关度，主要通过计算题目文本与作文文本之间的相似度，并引入语义离散度来构建主题相关度特征。衡量一篇作文是否符合题意，通常考察作文内容与题目内容是否相关，可以通过计算文本之间的语义相似度进行实现，此外还引入了语义离散度的概念，即通过语义离散度来挖掘句子与主题联系的紧密程度，更加细化的衡量作文符合题意的情况。 主题相关度特征 文本相似度特征 词重叠相似度特征：通过两个文本词汇重合比例来判断相似性；因为题目文本多数情况为短文本，简单的统计题目文本与作文文本的词汇重合情况会使得特征比较稀疏，作者通过 WordNet 对题目文本进行词汇上的语义扩充 词向量相似度特征：ASAP 中存在部分低分作文篇幅较小，且由于学生词汇量匮乏等原因，题目文本中的词汇在作文中出现频率很高，简单的使用词重叠特征很难区分这类作文；所以作者基于 Word2Vec 和余弦相似度的方法挖掘词汇的语义相似性，而不是简单的根据词汇的重叠来判断作文与主题的相关性 篇章表示相似度特征：衡量主题文本与作文内容文本的语义相似度，可以通过将文本表示成可计算的语义向量，从而计算两个向量间的相似度 LDA 方法：使用吉布斯采样方法获取作文和题目文本的主题分布向量，计算二者主题分布向量的相似度 Doc2Vec 方法：使用维基百科语料预训练的 Doc2Vec 模型，在该模型基础上使用本课题的标注语料进行增量式的训练，得到作文文本和题目文本的向量表示，再计算二者之间的余弦相似度作为特征 文本离散度特征：作者将文章看作句子的集合，主要从两个方面进行计算语义离散度 (1) 计算作文中每个句子相对于题目的语义差异；(2) 计算作文中每个句子相对于整体篇章的语义差异。前者挖掘作文上下文与题目文本的关联，后者挖掘句子相对于篇章主题的语义分布情况；作者综合以上两个方面进行语义离散度的计算，从而更加全面地衡量作文中句子的主题相关程度 基于距离的语义离散度特征： 基于中心的语义离散度特征：分别将篇章语义和题目语义作为文本语义的中心点，以该中心点为原点，重新定义作文内每个句子的坐标，通过该过程可以获得代表文本语义离散度的矩阵 基于主题相关度的模型考虑到作文的语义离散度特征包含了一定的上下文时序信息，即句子相对于主题的差异分布情况。为了更好的利用这部分信息，作者构建的基于语义离散度的深度学习模型主要使用了适合处理时序问题的 LSTM 模型。如下图，作者设置了一个双输入的 BLSTM 模型，在原本的句向量序列的输入基础上，增加语义离散度向量的输入序列，分别用两个 BLSTM 进行处理，将二者在 BLSTM 模型池化后的输出进行拼接，输入到三层连接层里。作者希望通过该模型能够较好的利用基于中心的语义离散度序列，挖掘作文中语义围绕中心变化的上下文信息。 AnaylsesSettings 数据集：ASAP 数据集，该数据集的统计数字如下图： 评估指标：Quadratic Weight Kappa 特征抽取方法： 使用 Stanford Parser 工具包对作文进行句法分析，构建句法树，并对句法树的结构进行统计、提取句法特征 使用 PyEnchant 库进行单词拼写错误的检查 使用 gibbslda 工具包训练 LDA 主题模型 使用 Word2Vec 工具包训练词向量 使用 CoreNLP 情感分析引擎获取句子的情感极性 使用 Doc2Vec 模型自动构建句向量 使用 RAE 模型自动构建句向量 使用 TextRank 算法抽取文本关键词 使用 WordNet 同义语库对关键词进行扩充 ResultsDiscourse-level Features based 作者结论： 构建的浅层语言学特征能够很大程度上反映学生的语言功底，而学生的语言水平与作文的质量通常密切相关 浅层语言特征在评分模型上取得了很好的效果，且词法特征、句法特征、语法错误特征对模型结果均有不同程度的提高 只依据用词、用句情况来评价作文的方法并不全面 作者通过 LDA 主题模型和词聚类方法挖掘出的语义信息有限，因此仅仅使用语义特征 C1、C2 的评分模型的结果很不理想 本章抽取的语义特征对影响了评分模型预测作文的准确程度，因为在语言特征 L1、L2、L3 的基础上引入语义特征 C1、C3，评分模型的平均指标没有得到提升，反而有将近 1%的下降 与基线方法中评测比赛排行榜的结果（ $0.81407$ ）相比，作者的评分模型结果存在着较大差距，这说明了仅仅依靠粗粒度的篇章特征进行评分的结果比较片面，需要挖掘更加深层的作文信息，比如作文的逻辑信息、结构信息等，对评分问题进行细化 语言学特征对基于随机森林的评分模型的影响因子很大，其中重要性排名为：词法特征 &gt; 句法特征 &gt; 语法正确性特征 语义特征对该评分模型影响较小，但是其能够一定程度上检查作文内容方面的质量，从而使评分模型的结果更加科学 Sentences Representations based 不同句向量比较实验结果 作者结论： 结合多种句表示方法构建的句向量能够表示更多的语义信息 句子之间时序信息在评分问题中的重要性，反映了作文在连贯性和逻辑性方面的质量 词向量多种组合方式优于单一的组合方式，能够更好的利用词向量包含的语义信息 无监督的句表示方法没有依靠任何人工规则提取特征，最后取得了比较不错的效果 Doc2Vec、递归自编码模型的句表示方法没有超过词向量组合的句表示方法；作者认为是因为这里无监督模型使用的训练语料较少 深度学习模型比较实验结果 作者结论： 在使用词向量拼接构建句向量的方法下，CNN 模型相比 NN 模型效果更好，可以一定程度上说明 CNN 模型在该问题中的适用性，其卷积和池化操作可以更好的提取输入数据的特征 在 CNN 模型上使用词向量拼接构建句向量的方式效果较好，分析是因为词向量拼接相对其他句表示方式，包含的语义更加完善，且构建的句向量维度较大，有利于发挥 CNN 模型在特征抽取方面的优势 使用综合型句向量表示作为输入时，CNN 模型结果不如双向 LSTM 模型的实验结果，分析是因为虽然 CNN 在特征抽取方面存在优势，但是模型会损失一部分句子之间的时序信息，这点对自动作文评分问题的影响更为重要 对于输入为句向量序列的情况，基于 LSTM 的评分模型的实验结果明显优于非时序的 NN 模型的实验结果，一定程度上说明了 LSTM 模型在挖掘句子上下文信息方面的有效性 虽然 CNN 在特征抽取方面存在一定优势，但是在 LSTM 前面加卷积层和池化层，会一定程度的损坏作文中句子之间的时序信息，这种上下文信息在自动作文评分问题中影响较大 双向 LSTM 模型的效果要优于单向 LSTM 效果，说明了双向 LSTM 模型能够挖掘出更多的上下文信息，相比单向 LSTM 存在优势 在单一的双向 LSTM 模型基础上，利用了句子的启发式规则特征，增加了一层双向 LSTM 用于平衡句向量和启发式特征的维度，同时不损失句子之间的时序信息，更加细化的抽取作文篇章的句子逻辑信息 基于 LSTM 的评分模型能够一定程度上挖掘作文中句子之间的隐藏的逻辑关系，且这点对于说明文体裁的评分显得较为重要 模型融合比较实验结果 作者结论： 两种融合方法都相比单模型的结果得到了很大的提升；而基于决策的加权投票融合方法的效果相对基于特征的融合方法略好，该融合方法基于多种评分模型，这些评分模型充分利用了本课题抽取的作文特征以及各种模型在不同问题中的优势，得到的结果更加全面，且这种基于加权投票的融合方法处理起来速度较快，模型复杂度低 基于决策的融合方法在说明文数据集 #1，#2，#7，#8 上的提升效果相对材料作文更加明显 Topic Relevance based首先，将主题相关度特征进行汇总，方便后续在比较性能时查看模型所用特征：作者结论： 引入语义离散度特征对评分模型提升效果较好，这部分特征挖掘了篇章中句子与主题结合的紧密程度，也能够一定程度上反映语义的分布情况 部分主题相关度特征无法提升评分结果；作者推测原因可能是同一题目下的作文集较少有明显的跑题现象，因此样本中负例较少，且语料中的评分结果是综合各个方面的衡量结果，与语言、逻辑等特征相比，主题相关度对最后评分结果影响较小 主题相关度特征的引入使得评分模型对跑题作文预测的分数明显降低，能够一定程度上说明这些主题相关度特征在检查作文是否符合题意方面的效果 附1 - 自动作文评分模型及方法研究 “哈工大” 附2 - Latent Dirichlet allocation “LDA” 附3 - 作文网 “English writings” 附4 - CoreNLP “Stanford Parser” 附5 - pyenchant 3.2.2 “check the spelling of words” 附6 - GibbsLDA++ “ C/C++ implementation of Latent Dirichlet Allocation” 附7 - Word2Vec Tutorials “TensorFlow” 附8 - TextRank: Bringing Order into Texts “Extract keywords” 附9 - WordNet “Large lexical database of English”","categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"}]},{"title":"Hierarchical Attention Networks for Document Classification","slug":"Hierarchical-Attention-Networks-for-Document-Classification","date":"2022-02-12T09:19:26.000Z","updated":"2022-02-12T09:20:56.195Z","comments":true,"path":"2022/02/12/Hierarchical-Attention-Networks-for-Document-Classification/","link":"","permalink":"http://example.com/2022/02/12/Hierarchical-Attention-Networks-for-Document-Classification/","excerpt":"基于“通过在模型结构中加入文件结构的知识，可以获得更好的表述”这种直觉，作者由词嵌入表示构造出句子表示，再由句子表示构造出篇章表示。这种层次化的结构十分符合文章自带的层次结构，此外注意力机制也能够在构造句子/篇章表示时，有选择地控制各个单词/句子在该表示中的贡献度。而与以往工作的关键区别在于，作者的系统使用上下文来发现一连串的 tokens 是相关的，而不是简单地过滤断章取义的 tokens。因此，作者在 $6$ 个文本分类任务上取得了很好的成绩。","text":"基于“通过在模型结构中加入文件结构的知识，可以获得更好的表述”这种直觉，作者由词嵌入表示构造出句子表示，再由句子表示构造出篇章表示。这种层次化的结构十分符合文章自带的层次结构，此外注意力机制也能够在构造句子/篇章表示时，有选择地控制各个单词/句子在该表示中的贡献度。而与以往工作的关键区别在于，作者的系统使用上下文来发现一连串的 tokens 是相关的，而不是简单地过滤断章取义的 tokens。因此，作者在 $6$ 个文本分类任务上取得了很好的成绩。 Features 使用了深度学习技术，适用于文档分类任务 模拟了文章层次结构的模型层次结构 注意力机制用于区别重要与不重要的内容在最终句\\篇章表示中的贡献度 Methods 如上图所示，HAN 模型分为四个部分：1) 词序列编码器；2) 词级注意力层；3) 句子编码器；4) 句级注意力层。 Word Encoder 使用 Embedding 矩阵将句中的每个单词转换成嵌入向量表示 (embeddings $x_{it}$) 使用双向 GRU 网络来建模句内的双向依赖关系，从而获得上下文化的单词表示 (annotations $h_{it}$) 将正向和逆向的 GRU 表示拼接起来，进一步获得双向上下文化的单词表示 (annotations $h_{it}$) Word Attention 使用单层 MLP 来得到上述表示 $h_{it}$ 的隐藏表示 (hidden representation $u_{it}$) 使用 $u_{it}$ 与词级上下文向量 $u_w$ 的相似度来衡量单词的重要性，并通过 softmax 函数得到归一化的重要性权重 $α_{it}$ (注意力权重 $α_{it}$) 将句子的向量表示 $s_i$ 计算为基于权重的词注释 $h_{it}$ 的加权和 (sentence vector $s_i$) 上述的词级上下文向量 $u_w$ 被随机初始化并被联合学习，可以被看作是固定查询 “what is the informative word” 的高级表示 Sentence Encoder 使用双向 GRU 来建模句子向量序列，并将其拼接以获得上下文化的句子表示 (sentence annotation $h_i$) Sentence Attention 使用单层 MLP 来得到上述表示 $h_{i}$ 的隐藏表示 (hidden representation $u_{i}$) 使用 $u_{i}$ 与句级上下文向量 $u_s$ 的相似度来衡量句子的重要性，并通过 softmax 函数得到归一化的重要性权重 $α_{i}$ (注意力权重 $α_{i}$) 将文档的向量表示 $v$ 计算为基于权重的句注释 $h_{i}$ 的加权和 (document vector $v$) 句级上下文向量类似于上述的词级上下文向量 AnalysesConfigs 任务： Sentiment analysis Topic Classification 数据集： Yelp reviews IMDB reviews Yahoo answers Amazon reviews 基线： Linear models：BOW and BOW+TFIDF、n-grams and n-grams+TFIDF、Bag-of-means SVMs models：SVM+Unigrams、Bigrams、Text Features、AverageSG、SSWE NN models：CNN-word、CNN-char、LSTM、Conv-GRNN and LSTM-GRNN Results 作者结论： 作者模型所带来的性能提升不仅无关乎数据大小而且无关乎任务类型，足以证明其有效性 引入层次化结构能够有效地提升性能，而引入注意力机制也能够进一步提升性能 全局词和句子上下文向量在 HAN 中也具有有效性 HAN 模型能够捕捉到不同的语境，并为词汇分配与语境相关的权重 HAN 模型能够捕捉到上下文化的单词重要性 Visual 作者结论： 在情感分析任务中，模型能够捕捉到那些强烈感情的单词和句子 在主题分类任务中，模型能够定位到那些标识主题的单词和句子 除此之外，模型能够捕捉到句间上下文的复杂语义 (如上图左所示，单词 delicious 和 a-m-a-z-i-n-g 被赋予了较大的注意力权重，它们表示的是正向感情；而句子 i don&#39;t even like scallops 则被赋予了较小的注意力权重，它所表达的是一种对比强调式的负向感情) 附1 - Hierarchical Attention Networks for Document Classification “NAACL 2016” 附2 - hierarchical-attention-networks “document classification model”","categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"}]},{"title":"Automated Essay Scoring with Discourse-Aware Neural Models","slug":"Automated-Essay-Scoring-with-Discourse-Aware-Neural-Models","date":"2022-02-10T07:15:38.000Z","updated":"2022-02-10T07:17:20.995Z","comments":true,"path":"2022/02/10/Automated-Essay-Scoring-with-Discourse-Aware-Neural-Models/","link":"","permalink":"http://example.com/2022/02/10/Automated-Essay-Scoring-with-Discourse-Aware-Neural-Models/","excerpt":"AES 系统尤其依赖于人工特征来预测作文质量，这就导致其受限于特征工程，虽然神经网络可以作为特征工程的替代，但它增大了对标注数据的需求。该论文探索了网络结构、上下文化嵌入、预训练策略来捕捉作文的语篇特征，并在 $3$ 个作文评分任务上实证了其方法的有效性。","text":"AES 系统尤其依赖于人工特征来预测作文质量，这就导致其受限于特征工程，虽然神经网络可以作为特征工程的替代，但它增大了对标注数据的需求。该论文探索了网络结构、上下文化嵌入、预训练策略来捕捉作文的语篇特征，并在 $3$ 个作文评分任务上实证了其方法的有效性。 Features 使用深度学习方法 使用 BERT 获取上下文化嵌入表示 使用 NLI 和 DMP 来预训练模型以弥补数据量问题 Methods模型为了让模型能够并入篇章结构，作者选择了如下两个模型： Hierarchical recurrent network with attention HAN 模型捕捉了篇章的等级结构，由第一层 LSTM + Attention 将词嵌入建模成上下文化的句向量，再由第二层 LSTM + Attention 将句向量建模成篇章向量 Bidirectional context with attention BCA 考虑到了句间依赖关系建模。基于词相似度，使用第一层输出的上下文化词向量来计算一个前向上下文向量和后向上下文向量，而最终的词表示为 LSTM 输出、look-back 上下文向量、look-ahead 上下文向量的拼接，再之后使用注意力机制将它们构造成一个句向量。后续则与 HAN 无异。而这种跨句依赖建模使得 BCA 具有了语篇意识。 预训练因为神经网络方法对于训练数据量的要求更大，因此作者希望通过预训练来帮助模型，使用了如下两个任务： Natural language inference NLI — 对于给定的句子，模型需要预测它们间的关系是中性的、矛盾的、连带的 Discourse marker prediction DMP — 对于给定句子对，模型需要预测连接它们的话语标记类别，如：however 对应于 opposition 类别 其中，NLI 具有普遍的提升效果，而 DMP 则因涉及到句子对而被希望能够影响到 HAN 和 BCA 的第一层 LSTM。 上下文化嵌入使用预训练模型 BERT 来生成上下文化的词嵌入表示，同时它的 NSP 预训练任务则被期望能够捕捉到语篇连贯性方面特征。 训练方法所有 HAN 模型和部分 BCA 模型使用预训练的 Glove 词嵌入进行初始化。 预训练时，句子表示由句对表示拼接而来，通过前馈神经网络和任务特定的权重偏差来预测对应任务的标签。其中对 BCA 的预训练而言，前向上下文向量为第一个句子计算，而后向上下文向量则为第二句子计算，这能模型学习到相似度映射矩阵。 作文评分训练时，就是正常的 HAN 模型或者 BCA 模型。 对于 BERT，作者采用了 bert-base-uncased 模型并将其参数冻结，这样就不会微调 BERT 了。作者所需要的上下文化嵌入表示则来自于 BERT 的倒数第二层，因为作者觉得最后一层的表示更加贴近于原始 BERT 的预训练任务而非作文评分任务。 Analyses实验设置 数据集 LDC 数据集，该数据集包含了 $12100$ 篇托福作文，其统计数字如下： ASAP 数据集，作者仅选取了前两个作文集作为训练数据，其统计数字如下： SNLI 数据集，用于 NLI 预训练任务 smashwords 数据集，用于 DMP 预训练任务 预训练 在 SNLI 数据集中增加一个标签 “X”，用来表示那些没有黄金标签的数据的标签，该方法实证有效 作者在网站上自主构造了用于 DMP 的训练数据，标签包括了：1) Idea opposition → nonetheless、on the other hand、however 等；2) Idea justification → in other words、for example、alternatively 等；3) Time relation → meanwhile、in the past、simultaneously 等；等等 结果 作者结论： BERT 的上下文化嵌入和 BCA 的跨句注意力这两种方法是互补的，且都对作文评分有益 ASAP 相较于 LDC 的数据量匮乏问题导致了神经网络方法并没能比得过人工特征方法 附1 - Automated Essay Scoring with Discourse-Aware Neural Models “ACL | BEA | WS 2019” 附2 - Github repository “code” 附3 - ETS Corpus of Non-Native Written English from the LinguisticData Consortium “LDC” 附4 - Automated Student Assessment Prize ”ASAP“ 附5 - Stanford Natural Language Inference “SNLI” 附6 - smashwords “an online book distribution platform” 附7 - hierarchical-attention-networks “HAN” 附8 - Liguistic-Complexity “BCA”","categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"}]},{"title":"Research on Key Techniques of Automated Essay Scoring","slug":"Research-on-Key-Techniques-of-Automated-Essay-Scoring","date":"2022-02-10T00:30:04.000Z","updated":"2022-02-10T01:01:53.025Z","comments":true,"path":"2022/02/10/Research-on-Key-Techniques-of-Automated-Essay-Scoring/","link":"","permalink":"http://example.com/2022/02/10/Research-on-Key-Techniques-of-Automated-Essay-Scoring/","excerpt":"本文为 $2020$ 年较新的 AES 方向硕士毕业论文，运用了当下流行的深度学习方法，主要是构造了语句通顺度特征和文本匹配度特征来辅助作文的自动评分。","text":"本文为 $2020$ 年较新的 AES 方向硕士毕业论文，运用了当下流行的深度学习方法，主要是构造了语句通顺度特征和文本匹配度特征来辅助作文的自动评分。 Features 融合多特征的深度语句通顺度计算 基于图神经网络的文本匹配度计算 使用了深度学习方法 将 AES 视作分类任务 Methods语句通顺度 该算法主要包括了：1) 使用多种特征对词语进行向量化表示从而形成多种不同的句子表示矩阵；2) 神经网络进行特征提取和句子关系建模。 句子表示矩阵将连续的 $L$ 个句子划分为一组，且**当组内句子时通顺的时，其通顺度标签为 $y_q = 1$，否则 $y_q = 0$**。假设文档 $D = [s_1, s_2, \\cdots, s_N]$ 包含 $N$ 个句子，那么就可以分为如下的 $N - L + 1$ 个组。 每个句子的矩阵表示中，行表示词语而列表示词向量。以 $L=3$ 为例，那么每一组句子就可以产生 $15$ 个句子表示矩阵，这是因为每个词语都有如上图的 $5$ 种不同的表示方法。$$\\large S = [w_1^{(i)}, w_2^{(i)}, \\cdots, w_{|S|}^{(i)}]$$其中，$S$ 表示句子，$|S|$ 表示句子长度，$w_{j}^{(i)}$ 表示句子中第 $j$ 个词语在第 $i$ 种词表示方法下的词向量。 基于 Word2vec：使用 Gensim 库在维基百科数据上训练 Word2vec 词向量，该表示维度为 $100$ 基于 GloVe：使用预训练好的 Glove 词向量，该表示维度为 $300$ 基于 wordnet：使用 OpenKE 库中的 TransE 算法对 WN18 进行训练，该表示维度为 $200$ 基于句法分析：使用 CoreNLP 进行依存句法分析，将 $50$ 维的句法类型嵌入和最相关依存词的 Word2vec 向量拼接，该表示维度为 $150$ 基于同义词：使用 Gensim 库找到最相近的 $3$ 个词，然后将 $3$ 个词的 Word2vec 向量拼接，该表示维度为 $300$ 网络模型卷积层 ( $n$ 个卷积核) 对输入的句子矩阵进行特征提取，并通过 Relu 激活函数学习非线性信息。紧接着使用最大池化将提取到的特征池化，于是就能够得到 $15$ 个 $n$ × $1$ 的句向量。 之后，作者使用自注意力网络来进一步建模句子表示，值得注意的是该注意力机制使用了 mask 机制来保证先后顺序关系 (但是作者在图中将 mask 标为 opt)。 然后，作者使用全连接层 (多层感知机) 来获得通顺度特征的表示向量。 最后，使用 Sigmoid 激活函数来获得最终的通顺度类别。 篇章级通顺度作者将所有的组通顺度概率的乘积来表示整个文档的通顺度，如此一个文档中任意几个不通顺的不通顺的组都可能对结果产生很大的影响。 $$\\large S_D = \\prod_{q \\in D} p(y_q = 1) $$ 文本匹配度 如上图，该算法主要分为：1) 对句子进行预处理并进行句向量的预训练，用递归神经网络提取到文章的结构特征；2) 对句子进行聚类处理，并将三个文档分别聚类出的句向量进行结合，形成“概念”；3) 使用 Triplet Network 形成图中顶点的特征向量，使用句子 Tf-idf 相似度形成边权重；4) 包括图卷积网络层、多层感知机层和输出层，用来对顶点特征向量进行训练，进而得到文档的相似度特征。类似地，若输入的两个文本为匹配文本，则 $y = 1$；若为非匹配文本，则 $y = 0$。 句向量句向量的预训练旨在捕捉到文章的结构特征，使用到了递归自编码器。 首先，使用 NLTK 工具对句子进行分词、移除停止词等，然后使用预训练好的 Word2vec 向量来表示句子中的每个单词。 其次，在句向量各维度位置上都取各个词向量中的最大值作为该位置的值，故该句向量的维度是 $200$。 然后，将上述的句向量输入到递归自编码器中进行预训练，若已预训练过则直接用 RAE 预训练得到的句向量代替之。 概念使用 KMeans 算法根据向量欧氏距离进行聚类，如果句子数量小于 $k = 10$ 的话，那么就会用全零向量补全句向量。于是，聚类后的三篇文章都包含 $10$ 组句子。 $A$ 和 $B$ 为匹配文本，$C$ 为 $A$ 和 $B$ 的非匹配文本；并将 $A$ 和 $B$ 相近的句子群称为一个“概念”。 对于 $A$ 的每组句子，匹配得到 $B$ 句子群中与 $A$ 该句子群余弦相似度最大的句子群，再匹配得到 $C$ 句子群中与 $A$ 该句子群余弦相似度最小的句子群，此次匹配结束；依照顺序，对于已经得到匹配的句子组，需要将其从对应的 $A$、$B$、$C$ 句子群中删除；连续匹配 $10$ 次，最终得到 $10$ 个“概念”，每一个“概念”中都包含一定数量的来自 $A$、$B$、$C$ 中的句子组。 构建图Triplet Network 的作用是度量 $A$、$B$、$C$ 的相似程度，其主要特点是三个输入的中间网络层参数共享，训练的目标是使得同类别的样本距离尽可能地大，不同类别的样本距离尽可能的小。 首先，我们需要将各个概念中 $A$、$B$、$C$ 的所有句子维度对齐，即各个维度上都取平均值。因此，最终会得到 $3$ 个 $200$ 维的句向量 $SA$、$SB$、$SC$。 然后，将它们输入到多层感知机中去，该多层感知机的第一个隐藏层为 Context 层。因此，最终可以获得 $Context(SA)$、$Context(SB)$、$Context(SC)$ 这三个上下文向量。 最后，根据上下文向量构建文本的文档图顶点向量和边权重。 网络层首先，图卷积网络进行 GCN 训练以获得 $10$ 个顶点的特征向量，并取它们的均值作为整个文档的相似度向量表征。 然后，使用多层感知机来建模句子间关系向量和输出之间的网络关系，并通过 Sigmoid 激活函数进行文本语义匹配判别。 AES Model 首先，将句子用词向量矩阵表示，使用卷积层和最大池化层提取特征得到句向量。 然后，在 LSTM 层融合句向量和语句通顺度向量后，进行平均池化操作。 最后，在多层感知机层融合文档匹配度特征后，进行得分的 Softmax 分类。 Analyses实验设置 数据集：ASAP 数据集 指标：Quadratic Weight Kappa 对比模型： EASE：EASE 基于手工设计的特征和回归模型，在 $154$ 只队伍中排名第三 CNN-LSTM：将文章表示成词向量矩阵，每两个词之间使用 CNN 提取特征，下面是一个 LSTM 层用于对文章的结构进行编码，最后添加一个带激活函数的全连接层用作最终的评分分类 语句通顺度 作者结论： 神经网络可以更好地处理语句通顺度问题 自注意力机制对建模句子间关系的效果更好 句子越多，相比传统的基于矩阵方式，提升效果更明显 使用多种方式对句子进行建模对效果提升有很重要的作用 使用包括基于 GloVe、基于句法分析、基于知识表示、基于同义词的方式表示词向量，对任务加入了很多信息，形成的句子表示矩阵可以更好地表示一个句子的意思 卷积核的宽度相当于 n-gram 中的 n 的大小；卷积核宽度太小，对词语之间关系的表征可能不完整；宽度过大，可能获得了许多无关噪声，没有提取到关键的特征 文本匹配度 作者结论： 神经网络可以很好地解决文本匹配问题 图神经网络比较适合建模长文本匹配的任务 使用递归自编码器进行句向量编码可以很好地得到文本的结构信息 Triplet Network 确实可以在训练过程中得到更多的信息，有着很好的适用性 AES Model 作者结论： 自动作文评分任务可以通过手工挑选出很多篇章特征和一些深层特征，来对这个问题进行很好的建模 语句通顺度和文本匹配度这两个深层特征向量对模型效果的提升有效果 上述这两个特征可能在自动作文评分的建模中重要性相似 附1 - 自动作文评分的关键技术研究 附2 - EASE “传统机器学习方法” 附3 - 语句通顺度：对句子每三个连续句子的通顺度进行了建模，可以将最后的多层感知机中的一个隐藏层向量提取出来，该向量代表了这三个句子的通顺度特征，因此对一个包含 $N$ 个句子的文档，可以得到 $N-2$ 个通顺度特征，这些特征融合了通顺度计算网络的诸多特征，对句子有多种形式的表示，可以被看作一种对三个句子表示的句子特征，因此，可以在 LSTM 层对这个特征和句向量特征等同看待。 附4 - 文本匹配度：对作文和范文的语义匹配关系进行了建模，可以将最后的多层感知机中的一个隐藏层向量提取出来，该向量代表了两个文本的语义匹配度特征，这个特征融合了语义匹配度计算网络的诸多特征，可以看作是对文章的一种整体的表示，因此，可以在多层感知机层和 LSTM 提取到的特征等同看待。","categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"}]},{"title":"Multi-Stage Pre-training for Automated Chinese Essay Scoring","slug":"Multi-Stage-Pre-training-for-Automated-Chinese-Essay-Scoring","date":"2022-02-09T08:31:56.000Z","updated":"2022-02-09T08:34:00.599Z","comments":true,"path":"2022/02/09/Multi-Stage-Pre-training-for-Automated-Chinese-Essay-Scoring/","link":"","permalink":"http://example.com/2022/02/09/Multi-Stage-Pre-training-for-Automated-Chinese-Essay-Scoring/","excerpt":"作者通过多阶段的预训练和微调搭建了一个提示语特定的中文作文自动评分系统。","text":"作者通过多阶段的预训练和微调搭建了一个提示语特定的中文作文自动评分系统。 Features 基于预训练的自动中文作文评分 自建语料 基线模型：ARC Model Stage characteristic weakly supervised pre-training 提示语无关; 粗糙评分 ( good / poor ) supervised cross-prompt pre-training / fine-tuning 与目标提示语同分数范围 supervised target-prompt fine-tuning 目标提示语的训练集 Methods WSP 数据 在乐乐课堂上搜集了大规模的中文作文数据，这些作文由七年级到高二的学生所创作，涉及记叙文、议论文和散文，并由教师进行评分。作文的平均句数和平均汉字数分别为 $30$ 和 $779$。此外，作者将 good 和 excellent 的作文评分合并为 good，让 poor 的作文保持不变，将 normal 的作文移除以更好地区分作文质量。该数据集的统计数字如下： 预训练 使用如上的数据集训练 ARC 模型来辨别 good 作文和 poor 作文。 Trans使用和目标提示语作文集相同分数范围的跨主题作文数据微调上一步得到的模型，由于作者训练的是回归模型，所以分数会被缩放到 $[0,\\ 1]$ 区间内 (评估时，模型预测的分数则会还原回原分数区间)。此时，token representations 会被固定，模型的其余参数进行微调。 $$\\Large y_{scaled} = \\frac{\\hat{y} - min}{max - min}$$ Finetune最终使用目标提示语数据微调模型以获得提示语特定的 AES 模型。Trans 阶段和 Finetune 阶段所用的数据集的统计数字如下图。作者选择了 $2012$ - $2014$ 中国两省的高考作文考试的提示语作为数据集的提示语，故一共有 $4$ 个提示语。然后再让几所高中的学生根据他们对每个提示的理解写一篇文章，并以标准流程来为这些作文评分，其分值范围是 $[0,\\ 60]$ 。 Analyses 作者得出的结论包括： 预训练阶段的作文具有一定程度的区分度 不同主题、体裁的作文之间也共享着一些能够指示作文质量的特征 多阶段预训练策略是可行且有效的方法 在目标提示语上进行微调是必要的 模型缺点在于其在各个作文集上的进步很不均匀 用跨提示且同分区间的作文数据训练模型能够避免模型在目标提示语数据上过拟合 预训练可以减少模型对于目标提示语作文数据量的需求 附1 - Multi-Stage Pre-training for Automated Chinese Essay Scoring “EMNLP 2020” 附2 - Attention-based Recurrent Convolutional Neural Network for Automatic Essay Scoring “CoNLL 2017” 附3 - ARC Model “keras-based” 附4 - Prompt 1: 尚先生把手机落在出租车上。他随后拨打那部手机，对方接听后立即挂断。 他又发短信表示，愿意出2000元“买”回手机。一小时后，尚先生收到回复，对方要归还手机。 捡到手机的人是一位年轻人。尚先生要酬谢他，但对方交还手机后就转身离去了。 当天晚上，记者联系到那位年轻人，年轻人说：“我本来无意归还，但看到手机里的照片和信息，发现机主刚刚给芦山地震灾区汇去一大笔捐款，很受感动。我不能见利忘义，不能用贪心对待爱心。我也要像尚先生那样多一些真诚和友善。” 附5 - Prompt 2: 两条小鱼一起游泳，遇到一条老鱼从另一方向游来，老鱼向他们点点头，说:“早上好，孩子们，水怎么样?” 两条小鱼一怔，接着往前游。游了一会儿，其中一条小鱼看了另一条小鱼一眼，忍不住说: “水到底是什么东西? ”看来，有些最常见而又不可或缺的东西，恰恰最容易被我们忽视；有些看似简单的事情，却能够引发我们深入思考? 附6 - Prompt 3: 中国自古有“学而知之”的说法，这里的“学”，通常被理解为从师学习。韩愈就说过：“人非生而知之者，孰能无惑？惑而不从师，其为惑也，终不解矣。” 随着时代的发展，我们获取知识、掌握技能或懂得道理的途径日趋多元。请结合你的心得和体验，在“ 而知之”中的横线处填入一字，构成题目，写一篇文章，不能以“学而知之”为题。 附7 - Prompt 4: 也许将来有这么一天，我们发明了一种智慧芯片，有了它，任何人都能古今中外无一不知，天文地理无所不晓。比如说，你在心里默念一声“物理”，人类有史以来有关物理的一切公式、定律便纷纷浮现出来，比老师讲的还多，比书本印的还全。你逛秦淮河时，脱口一句“旧时王谢堂前燕”，旁边卖雪糕的老大娘就接茬说“飞入寻常百姓家”，还慈祥的告诉你，这首诗的作者是刘禹锡，这时一个金发碧眼的小女孩说，诗名《乌衣巷》出自唐诗，这将是怎样的情形呀！读了以上材料，你有怎样的联想或思考？请就此写一篇文章。","categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"}]},{"title":"English automatic essay scoring method based on multi-level semantic features","slug":"English-automatic-essay-scoring-method-based-on-multi-level-semantic-features","date":"2022-02-08T03:59:39.000Z","updated":"2022-02-08T04:05:34.693Z","comments":true,"path":"2022/02/08/English-automatic-essay-scoring-method-based-on-multi-level-semantic-features/","link":"","permalink":"http://example.com/2022/02/08/English-automatic-essay-scoring-method-based-on-multi-level-semantic-features/","excerpt":"AES 以往的研究已经证明了将基于特征工程的浅层特征和基于深度学习的潜在语义特征结合起来就能够有效提高作文自动评分的性能，但多数深度学习研究只使用了单一的网络结构从而忽略了不同的网络结构能够从不同维度进行语义特征的提取，同时作文评分的复杂性又要求了一定的人工特征来辅助深度学习所获取的潜在语义特征。基于此，作者提出了一种基于多层次语义特征的英文作文自动评分方法，且在 Kaggle ASAP 的性能达到了 $\\mathcal{Avg.QWK} = 79.17%$ 。","text":"AES 以往的研究已经证明了将基于特征工程的浅层特征和基于深度学习的潜在语义特征结合起来就能够有效提高作文自动评分的性能，但多数深度学习研究只使用了单一的网络结构从而忽略了不同的网络结构能够从不同维度进行语义特征的提取，同时作文评分的复杂性又要求了一定的人工特征来辅助深度学习所获取的潜在语义特征。基于此，作者提出了一种基于多层次语义特征的英文作文自动评分方法，且在 Kaggle ASAP 的性能达到了 $\\mathcal{Avg.QWK} = 79.17%$ 。 Features 使用深度学习，且附带了一些人工特征 使用了 ASAP 数据集 建模了主题相关性 Multi-Level Semantic Features 特征层次 具体特征 特征描述 深层语义特征 局部语义特征 CNN + 注意力池化 以提取作文的局部信息 (词汇) 深层语义特征 全局语义特征 CNN + Bi-LSTM + 注意力池化 以提取作文的上下文信息 (连贯性) 主题层语义特征 主题相关性特征 Doc2Vec + 余弦相似度 以获取作文与提示语的主题相关性 浅层语义特征 语法错误特征 拼写错误、冠词误用、单复数误用等语法错误的数量 浅层语义特征 作文长度特征 单词的数量以及包含字符的数量 浅层语义特征 语义复杂度特征 单词长度的均值和方差 浅层语义特征 单词复杂度特征 字符长度大于 $6$ 的单词数量 浅层语义特征 句子复杂度特征 句子长度的均值和方差 浅层语义特征 从句数特征 作文中包含从句的个数 MLSN Model 如上图，模型提取出深层语义特征、主题层语义特征、浅层语义特征后，将这三个层次的特征融合后输入到全连接层，最后通过 Sigmoid 激活函数获得作文分数输出。 AnalysesConfigs 数据集：Kaggle ASAP (Automated Student Assessment Prize) 数据集 评估指标：QWK (Quadratic Weighted Kappa) 模型选择：$5$ 折交叉验证 Res Conclus 单独的神经网络模型不能很好的提取较长文档的语义特征并用于作文评分 简单的融合策略并不能在多个强学习算法的集成过程中起到较好的作用 人工特征是对语义特征的有益补充，能够对作文评分模型起到较好的作用 MLSN 能够从深层次提取作文的语义特征，从主题层次提取作文的主题和提示语的相似度特征，从浅层提取语法错误和语言丰富程度等语言学特征，因此本文提出的模型具有较好的泛化性且相比于基线模型，整体性能最优 局部语义特征和全语义特征能够更好地对作文的特征进行表征，能够有效地提高作文评分的性能 深度学习模型很难挖掘语法错误、语言丰富程度等语言学特征，该类人工特征是深度学习模型提取的语义特征的有益的补充，能够显著地提升作文评分模型的性能 主题特征能够提升作文评分模型的性能，但是目前的方法对模型的性能影响有限 **附1 - ** Doc2Vec “Doc2Vec模型能够使用单个低维稠密向量表示变长的文档” **附2 - ** Ensemble Learning “集成学习通过构建并结合多个机器学习器来完成学习任务” **附3 - ** 基于多层次语义特征的英文作文自动评分方法","categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"}]},{"title":"English Automated Essay Scoring Methods Based on Discourse Structure","slug":"English-Automated-Essay-Scoring-Methods-Based-on-Discourse-Structure","date":"2022-02-07T08:40:19.000Z","updated":"2022-02-07T08:43:19.706Z","comments":true,"path":"2022/02/07/English-Automated-Essay-Scoring-Methods-Based-on-Discourse-Structure/","link":"","permalink":"http://example.com/2022/02/07/English-Automated-Essay-Scoring-Methods-Based-on-Discourse-Structure/","excerpt":"该文主要介绍了一种基于篇章结构的为中国学生所作议论文自动评分的方法，作者指出 1) $E-rater$ 所使用的篇章结构评分模型由于其商业性而无法详知；2) 中国学生写的英文作文往往在风格上不同于母语为英语的学生学的作文，因此 $E-rater$ 对于中国学生的英文作文针对性不强；3) 句酷批改网虽然在国内使用较为广泛，但其对篇章结构的评分效果有限。因此，作者提出了一种基于细粒度篇章结构的作文自动评分方法。 AES 系统的发展历程中，存在着大量基于文章内容和语法的研究，但鲜有基于篇章结构的研究。而篇章结构是作文评分的重要方面，表示了作者对文章组织结构以及论证逻辑的掌握。在篇章结构的研究中，国内主要分析篇章的主次关系，对篇章成分的分析研究不足，而国外的研究对中国学生作文的针对性不强。","text":"该文主要介绍了一种基于篇章结构的为中国学生所作议论文自动评分的方法，作者指出 1) $E-rater$ 所使用的篇章结构评分模型由于其商业性而无法详知；2) 中国学生写的英文作文往往在风格上不同于母语为英语的学生学的作文，因此 $E-rater$ 对于中国学生的英文作文针对性不强；3) 句酷批改网虽然在国内使用较为广泛，但其对篇章结构的评分效果有限。因此，作者提出了一种基于细粒度篇章结构的作文自动评分方法。 AES 系统的发展历程中，存在着大量基于文章内容和语法的研究，但鲜有基于篇章结构的研究。而篇章结构是作文评分的重要方面，表示了作者对文章组织结构以及论证逻辑的掌握。在篇章结构的研究中，国内主要分析篇章的主次关系，对篇章成分的分析研究不足，而国外的研究对中国学生作文的针对性不强。 0. 本文特点 涉及大量特征工程，而没有使用深度学习技术 主要是针对作文的篇章结构进行评分，而缺乏对作文内容、语法等的评分 针对于中国学生所作的英文议论文进行评分 语料库为自建库 1. 篇章结构作者分析了大量托福、雅思以及 GRE 的 ‘Argument’ 官方范文和应考学生的文章，依托真实托福应考学生的 $300$ 篇议论文，总结出中国学生撰写议论文的特点；并结合议论文的 $5$ 个要素，即论点、理由、论据、结论和论证，将篇章成分划分为以下的 $9$ 个细粒度类别标签。 篇章成分 定义 BI 背景介绍 MC 主要论点 RS 论点理由，与论点构成因果关系 RI 论点理由的补充说明 ED 举例论证，与论点理由构成因果关系 CS 让步，承认反方观点的一部分而否定另一部分，消除论证的片面性 CC 总结全文 TST 承上启下，起过渡作用 IRL 与论证无关的句子 2. 语料库作者分析了从国内知名在线出国留学平台朗播网采集的真实托福应考学生的 $300$ 篇议论文，共 $6083$ 个句子，以句子为单位划分篇章单元，并为每个篇章单元标注篇章成分，且以段为单位对作文的篇章结构进行评分，以此自建语料库。其基本的统计信息如下： 3. 特征构成 特征类别 详细特征 特征描述 结构特征 词统计特征 篇章单元中单词的数量 结构特征 篇章单元位置特征 篇章单元所在段落在文章中的位置 + 篇章单元在段落中的位置 结构特征 标点符号特征 把篇章单元的结束标点符号转化为数值特征 (句号、问号、感叹号) 结构特征 上下文特征 篇章单元的前一个单元的篇章成分 (文章首个篇章单元则对应地补充为 $0$ ) 结构特征 段落成分特征 段落包含的 $9$ 类篇章成分的个数作为 $9$ 个数值特征 结构特征 段落上下文特征 段落前一段的篇章结构评分 (首段的前一段评分设置为 $-1$ ) 结构特征 篇章成分序列特征 段落的正确篇章成分序列与预测的篇章成分序列之间的最小编辑距离 词汇特征 n-grams 特征 $3$-gram 模型计算每个篇章单元的 $3$-gram 困惑度 词汇特征 指示词特征 从 PDTB 2.0 提取 $64$ 个指示词，将这些指示词的出现与否作为特征 词汇特征 人称代词特征 NLTK 标注的人称代词的词频 词汇特征 情态动词特征 NLTK 标注的情态动词的词频 词汇特征 专有名词特征 NLTK 标注的专有名词的词频 句法特征 句法树深度特征 Stanford 分析器构建的句法分析树的深度 句法特征 谓语时态特征 篇章单元的谓语时态 句法特征 句法树深度特征 Stanford 分析器构建的句法分析树的深度 4. 特征提取特征提取是将机器学习算法无法直接识别的原始数据转化为可识别的特征数据的过程。 使用特征提取程序提取语料库中每个句子与篇章结构相关的特征 利用特征整合程序将提取的特征整合成特征向量 将每个句子的特征向量与其对应的标签向量作为模型输入 识别每个句子的篇章成分类别 5. 作文自动评分基于篇章结构的作文自动评分方法分为篇章成分识别和 篇章结构评分两部分。 提取篇章成分特征，将篇章单元结构、词汇及句法等 $86$ 个特征转化成特征向量 基于 RF 构建篇章单元识别模型 DEI-RF，以 此预测测试集中每个篇章单元的篇章成分 基于 LR 构建篇章结构自动评分模型 DSS-LR，以此完成对文章段落的篇章结构评分 6. 实验结果 篇章成分识别模型中，DEI-RF 的性能表现普遍优于另外两种模型，以准确率为指标的结果如下： 篇章结构自动评分结果中，DSS-LR 模型对 Introduction 段、Argumentation 段及 Concession 段的评分效果较佳，而对 Conclusion 段的评分效果较差 (而判断一篇作文篇章结构的好坏，主要取决于 Introduction 段、Argumentation 段及 Concession 段)，其性能如下： **附 - ** 基于篇章结构的英文作文自动评分方法","categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"}]},{"title":"Attention-based Recurrent Convolutional Neural Network for Automatic Essay Scoring","slug":"Attention-based-Recurrent-Convolutional-Neural-Network-for-Automatic-Essay-Scoring","date":"2022-01-08T08:34:53.000Z","updated":"2022-01-08T08:36:32.662Z","comments":true,"path":"2022/01/08/Attention-based-Recurrent-Convolutional-Neural-Network-for-Automatic-Essay-Scoring/","link":"","permalink":"http://example.com/2022/01/08/Attention-based-Recurrent-Convolutional-Neural-Network-for-Automatic-Essay-Scoring/","excerpt":"$⭐\\ \\ \\ paper\\ \\ \\ link$ $⭐\\ \\ \\ code\\ \\ \\ link$ 神经网络模型被应用于自动作文评分任务，并展现出了有希望的性能表现。而现存的工作一般采用 $RNN$ 和 $CNN$ 来建模输入作文，然后基于单个作文的向量表示来评分。作文中的不同部分对于评分的贡献也都是不同的，这一点却没有被现存的模型意识到。为此，作者搭建了一个层次化的句子-篇章模型来表示作文，并使用注意力机制来自动地决定单词和句子在评分中的权重。试验结果表明作者的模型优于先前的 $SOTA$ 方法，并且注意力机制在 $AES$ 中的有效性也得以充分证明。","text":"$⭐\\ \\ \\ paper\\ \\ \\ link$ $⭐\\ \\ \\ code\\ \\ \\ link$ 神经网络模型被应用于自动作文评分任务，并展现出了有希望的性能表现。而现存的工作一般采用 $RNN$ 和 $CNN$ 来建模输入作文，然后基于单个作文的向量表示来评分。作文中的不同部分对于评分的贡献也都是不同的，这一点却没有被现存的模型意识到。为此，作者搭建了一个层次化的句子-篇章模型来表示作文，并使用注意力机制来自动地决定单词和句子在评分中的权重。试验结果表明作者的模型优于先前的 $SOTA$ 方法，并且注意力机制在 $AES$ 中的有效性也得以充分证明。 一、 引言传统的 $AES$ 模型基于特征工程，将稀疏特征作为评分的依据，这使其严重依赖于繁复的特征工程，并出现了数据的稀疏性。 神经网络模型被用于将词信息编码到单个稠密向量表示中去，该向量用以表示整个作文，然后，一个非线性神经基于该表示进行评分。由于不再依赖于人工特征，因此神经网络模型对跨领域的作文展现出了更好的鲁棒性。 众所周知，$CNN$ 结构可以有效地捕捉局部的 $n-grams$ 信息，而 $LSTM$ 则在长程建模方面占优。之前没有学者比较过 $RNN$ 和 $CNN$ 在 $AES$ 方向中的有效性。为此，作者采用了一个两层架构来比较 $CNN$ 和 $LSTM$ 对于句子和篇章建模的能力。结果表明：CNN 在句子建模上具有相对优势；LSTM 则在篇章建模上有相对优势。 句子之于评分并不等同，单词亦然。为此，作者采用了一个神经注意力模型来自动地计算卷积特征和隐藏状态的权重值。注意力机制可以直观地分辨出与提词更吻合或明显不正确的句子和语法，这一点算是共识了。 作者试验结果表明： 局部 $N-grams$ 信息与句子结构的评分更加相关 全局信息则于篇章级连贯性的评分息息相关 注意力能够显著地提供更准确的评分结果 作者的注意力模型取得了 $SOTA$ 的性能表现 二、Arc 模型1. 任务$AES$ 是一个有监督学习任务，可以分为以下三种： Classification 分类任务中，分数被划分为各个分类类别，然后通过分类模型来预测类别 Regression 回归任务中，每个分数都被认为是连续值，因此使用回归模型来预测该值 Ranking 评级任务中，又可以分为对级评级和集合级评级，先前的研究考虑的是对级评级，而后来的研究则旨在为整个作文集进行评级 本文中，作者的模型是一个线性回归模型。 2. 指标以下指标都可以用于 $AES$ 的评估中： Pearson’s correlation Spearman’s ranking correlation Kendall’s Tau quadratic weighted kappa 其中，QWK 作为 ASAP 竞赛的官方指标，应用得很普遍，因此作者也采用的是 QWK 指标。 Kappa 衡量的是不同评分者在定性项目上的一致性，即标注分数和模型预测之间的一致性；QWK 即采用二次权重的 Kappa 指数。$W$ 是平方权重矩阵，其中每个权重都与标注分数和预测分数的差值评分成正比；$O$ 是观察分数矩阵，其中 $O_{i, j}$ 表示的是标注分数为 $i$ 且预测分数为 $j$ 的作文数量；$E$ 是期望分数矩阵，它由标注分数和预测分数的直方图向量的外积计算而来，并且 $E$ 需要被规范化以与 $O$ 中的元素总个数保持一致。然后 QWK 由以下公式计算而来：$$\\Large \\mathcal{K} = 1 - \\frac{\\sum W_{i,j} O_{i, j}}{\\sum W_{i,j} E_{i, j}}$$ 3. 模型 如上所示，整个模型呈现出层次性，这类似于 sentence-document model，它们都将一篇作文看作为句子的组合而不是单纯的单词序列，但不同点在于作者使用 $LSTM$ 来捕捉全局的信息 。此外，注意力池化层同时被用于单词和句子上，旨在捕捉更相关的单词和句子 (即对最后评分贡献更大的那些单词和句子)。 基于字符的嵌入 作者在每个单词的字符上设置一个卷积层来捕捉字符特征，然后通过最大池化和平均池化层来获得两个池化表示，最后将这两个池化表示拼接起来作为单词的表示向量； 词嵌入 就是通过查询嵌入表来获得词嵌入； 句子表示 在词嵌入上通过卷积层和注意力池化来构造出句子表示，如上图(左)。对于每一个句子而言，卷积层会抽取词级特征而接下来的注意力池化层则会有区分性地将各个词信息池化到最终的句子表示中； 篇章表示 作者通过循环层来构造篇章级表示，使用 $LSTM$ 来从句子中学习长程依赖建模，并另外使用一个注意力池化层将 $LSTM$ 输出的隐藏表示序列池化成最终的作文表示向量。$LSTM$ 能够帮助我们学习全局性的文本信息，而注意力池化层则可以因地制宜地为各个句子分配权重。 线性层 最终，一个激活函数为 $Sigmoid$ 的线性层会基于该文本表示来为作文评分 三、结果","categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"}]},{"title":"Neural Automated Essay Scoring Incorporating Handcrafted Features","slug":"Neural-Automated-Essay-Scoring-Incorporating-Handcrafted-Features","date":"2022-01-07T07:46:16.000Z","updated":"2022-01-07T07:48:20.689Z","comments":true,"path":"2022/01/07/Neural-Automated-Essay-Scoring-Incorporating-Handcrafted-Features/","link":"","permalink":"http://example.com/2022/01/07/Neural-Automated-Essay-Scoring-Incorporating-Handcrafted-Features/","excerpt":"Neural Automated Essay Scoring Incorporating Handcrafted Features $AES$ 旨在自动为作文评分，而传统的 $AES$ 系统严重依赖于人工特征，而最近的 $AES$ 系统则转向了深度神经网络以缓解这种特征工程需求。于是，有学者进一步提出了混合方法，即将人工特征并入到一个深度神经 $AES$ 模型，该方法也取得了 $SOTA$ 的性能表现。最常用的混合方法之一就是将句子级的人工特征 (由额外的 $RNN$ 网络进行处理) 并入到深度神经 $AES$ 模型中去。但这种方法的缺陷有三，如下： 它无法并入有效地作文人工特征 它极大地增加了模型参数从而提高了训练难度 额外的 $RNN$ 处理句子级特征使得模型扩展起来复杂 为此，作者提出了一种新的混合方法来将作文人工特征并入到深度神经 $AES$ 模型中去。具体地来说，作者的方法就是将作文人工特征与分布式论文表示向量拼接起来，这个表示向量则来自于 $DNN-AES$ 模型的一个中间层。虽然作者的方法是 $DNN-AES$ 模型的一个简单扩展，但是它却起到了重要的性能提升","text":"Neural Automated Essay Scoring Incorporating Handcrafted Features $AES$ 旨在自动为作文评分，而传统的 $AES$ 系统严重依赖于人工特征，而最近的 $AES$ 系统则转向了深度神经网络以缓解这种特征工程需求。于是，有学者进一步提出了混合方法，即将人工特征并入到一个深度神经 $AES$ 模型，该方法也取得了 $SOTA$ 的性能表现。最常用的混合方法之一就是将句子级的人工特征 (由额外的 $RNN$ 网络进行处理) 并入到深度神经 $AES$ 模型中去。但这种方法的缺陷有三，如下： 它无法并入有效地作文人工特征 它极大地增加了模型参数从而提高了训练难度 额外的 $RNN$ 处理句子级特征使得模型扩展起来复杂 为此，作者提出了一种新的混合方法来将作文人工特征并入到深度神经 $AES$ 模型中去。具体地来说，作者的方法就是将作文人工特征与分布式论文表示向量拼接起来，这个表示向量则来自于 $DNN-AES$ 模型的一个中间层。虽然作者的方法是 $DNN-AES$ 模型的一个简单扩展，但是它却起到了重要的性能提升 Introduction写作测试能够考验受试者的逻辑思考能力、批判推理能力、创造性思维能力。但是人工评分的方式不仅耗时费力，而且存在着严重的主观性。自动作文评分 ($AES$) 旨在利用自然语言处理和机器学习技术来代替人类评分员，实现自动化作文评分以解决上述问题。$AES$ 方法可以被分为以下 $3$ 种： 特征工程方法：该方法通过人工特征来为作文预测分数；其优势在于可解释性和可说明性，而其缺陷在于严重依赖于特种工程。 神经网络方法：该方法通过深度神经网络模型来端到端地为作文评分，从而缓解了上述问题；其优势在于特种工程的移除，而其缺陷则是计算资源和花销的增加。 混合方法：简言之，该方法就是对上面的两种方法“全都要”；$NN$ 方法能够从词序列模式中抽取数据集特定的特征，而 $FE$ 方法则能够让模型学习到现存的一些有效特征 (这些人工特征往往难以从词序列模式中抽取到)。 Dasgupta 等人提出的混合方法是在 $DNN-AES$ 模型的基础上额外增加了一个 $RNN$ 模型来处理句子级人工特征。诚然它取得了 $SOTA$ 的成绩，但是其缺陷还是有的，如下： 它无法并入有效地作文人工特征 它极大地增加了模型参数从而提高了训练难度 额外的 $RNN$ 处理句子级特征使得模型扩展起来复杂 为此，作者提出了一种新的混合方法，即将作文人工特征并入到 $DNN-AES$ 模型中去。具体地来说，作者的方法就是将作文人工特征与分布式论文表示向量拼接起来，这个表示向量则来自于 $DNN-AES$ 模型的一个中间层。这种混合方法的优势有： 它能将现有的各种有效作文特征并入 其额外的参数量等于并入的作文特征数量，而没有额外的手工调整参数 它可以很轻易地扩展到各种 $DNN-AES$ 模型上，因为传统的模型一般总会有一个产生分布式作文表示向量的层 Methods作者所提出的方案说白了就是将作文特征直接加入到线性回归输入阶段，以下的 $3$ 张模型结构图，足以说明其工作原理了$8$： 作者声称其方法能够兼容现存的多种有效的作文特征，为此他实验了 $25$ 种作文特征，如下表所示： Experiments Dataset ASAP dataset，该数据集的统计数据见下图： Metrics 通过对每个 $prompt$ 的 $5-flold$ 交叉验证来评估分数预测的准确性 准确度指标则是平方加权的 Kappa 指数 (QWK) ，它可以考察预测分数和标注分数之间的一致性 Loss Function 最小化均方误差 (mean squared error， MSE) **附 - ** $k$ 折交叉验证 ($k$ -fold cross-validation)： 随机地将以给数据切分成 $k$ 个互不相交且大小相同的子集 利用 $k - 1$ 个子集的数据训练模型，并利用剩余的 $1$ 个子集测试模型 将这一过程对可能的 $k$ 种选择重复进行 最后选取 $k$ 次评测中平均测试误差最小的模型 Results 比较各提词的准确性，平均作文长度较短的提词数据集上的准确性往往高于那些长作文的提词数据集 比较传统的 $DNN-AES$ 模型，可知基于 $LSTM$ 且带有 $MoT$ ($mean-over-time$) 池化的模型具有更高的性能表现 $BERT$ 的性能表现往往优于基于 $LSTM$ 的模型 并入作文人工特征的方法极大地提高了所有基础 $DNN-AES$ 模型的准确性 所提出的方法在 $5%$ 的显著性水平上提高了基于 $LSTM$ 和 $BERT$ 模型的性能，在 $10%$ 的显著性水平上提高了传统混合模型的性能 将提出的方法与逻辑回归模型（一种特征工程方法）相比较，所有提出的方法都提供了更高的平均准确度 使用 $LSTM$ 与 $MoT$ 池化的方法和传统的混合模型的平均 $QWK$ 在 $5%$ 的显著性水平上更高，而基于 $BERT$ 的建议方法在 $1%$ 的显著性水平上更高 使用 $BERT$ 的模型提供了最高的平均精度 每个人工特征都有一定程度的贡献，而权重大的特征在不同的提词中都有不同的贡献","categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"}]},{"title":"Should You Fine-Tune BERT for Automated Essay Scoring?","slug":"Should-You-Fine-Tune-BERT-for-Automated-Essay-Scoring","date":"2022-01-06T14:33:52.000Z","updated":"2022-01-06T14:36:48.355Z","comments":true,"path":"2022/01/06/Should-You-Fine-Tune-BERT-for-Automated-Essay-Scoring/","link":"","permalink":"http://example.com/2022/01/06/Should-You-Fine-Tune-BERT-for-Automated-Essay-Scoring/","excerpt":"Should You Fine-Tune BERT for Automated Essay Scoring? 如今，大多数 NLP 研究都选择微调大型预训练模型来学习有监督分类任务。作者调研了深度神经模型是否是 AES 领域恰当的技术选择，并发现微调 BERT 所产生的性能类似于早期的经典模型，但是却需要巨大的额外花销。作者在最后总结了 AES 领域研究中的有希望的发展方向，其中 Transformer 模型的独有特点可能大有裨益。","text":"Should You Fine-Tune BERT for Automated Essay Scoring? 如今，大多数 NLP 研究都选择微调大型预训练模型来学习有监督分类任务。作者调研了深度神经模型是否是 AES 领域恰当的技术选择，并发现微调 BERT 所产生的性能类似于早期的经典模型，但是却需要巨大的额外花销。作者在最后总结了 AES 领域研究中的有希望的发展方向，其中 Transformer 模型的独有特点可能大有裨益。 BackgroundAES 旨在让计算机代替人类来为学生作文自动地评分，学生作文往往会以以下两种形式中的一种来进行打分： 针对总体质量的分数 ($holistic$) 针对某个维度的分数 ($trait$) 而这些分数一般都介于 $0$ 到 $10$ 以内，当然也不乏有 $60$ 个点这样的大区间存在。除此之外，学生作文又可以根据其给定 $prompt$ 的不同进行划分。 AES 方向的学者则倾向于使用一系列合理的变量来进行多变量回归。但这样的模型保留了变量和可识别的写作特征之间的直接映射，非常依赖于大量的特征工程。 我们先说一下很重要的一个概念 —— word representation， 它经历了以下四个阶段： N-grams 词袋特征 语料特定的嵌入表示 通用嵌入表示 上下文化嵌入表示 如今对预训练模型 (诸如 $BERT$) 进行下游任务微调的方法盛行于世。而 $BERT$ 当属 $contextual$ $word$ $embeddings$ 的中的佼佼者，但它的参数量都是亿级的，这就会大大增加模型训练的花销。 大量实验证明，以课程学习的方式控制模型学习率可以产生一个有效的微调过程 。然而，即使它的有效性是惊人且实际证明的，但是 $BERT$ 究竟学习到了什么仍然是未知数。 再说回 AES，神经模型在 AES 方向中的应用还处于起步阶段，而且主要是作为一种自动作文反馈的中间表示，端到端的 AES 系统恐怕离实际商业应用还差得远。 Question从特征工程转为深度神经网络模型对于 AES 而言，是否值得 是该论文主要研究的问题。 for AES specifically, is a move to deep neural models worth the cost? Experiments⭐ Bag-of-Words Representations 对于文本分类任务而言，词表示就是要抽取表面的 $1-grams$ 和 $2-grams$ 特征，并以 $one-hot$ 向量的二元值来表示其的存在与否。在 AES 中，该方法竟然十分有效，并还可以进一步提升其性能： 额外增加 $POS$ 标签来捕捉文本内容中的句法独立性 字符级的 $3-grams$ 和 $4-grams$ 来捕捉单词拼写问题以提升其鲁棒性 但是，这种高维表示法通常有一个截止阈值，在这个阈值下，那些罕见的标记将会被排除在外。该文实验中： 出现次数低于 $5$ 的那些 $N-grams$ 将会被去除 (即使经过这样的清理，它仍是一个有数千维的稀疏特征空间) 模型：Scikit-learn 中带有拉普拉斯平滑的朴素贝叶斯分类器 $POS$ 标记：来自 SpaCy ⭐ Word Embeddings 词级嵌入往往是一个最高 $300$ 维的密集实值特征向量表示。 该文实验中： 作者将每个文档表示为 GloVe 词嵌入向量的术语频率加权平均值 词嵌入：GloVe 模型：Scikit-learn 中带有利伯勒线性求解器和 $L2$ 正则化的逻辑回归分类器 ⭐ Fine-Tuning BERT 作者微调的是 Fast.ai 库中的 uncased BERT 模型，而 Fast.ai 推荐使用周期性学习率来进行微调，即学习率会有一个上界和一个下界，而学习率则会在这两个界之间起伏。其中，较大的学习率可以充当一个正则化项来避免模型在训练中过拟合或者遭遇局部鞍点；较小的学习率则能够让模型更好地找到局部最优点。 该文实验中： 学习率上界设置为 $1e-5$，而学习率下界设置为 $4e-7$ ，这也是 Fast.ai 所推荐的设置 除此之外，作者还实验了三种不同的学习率规划方案，如下： “default” policy：学习率会在一个 epoch 内由低升高，再由高降低 “2-rate” policy：学习率会在一个预设的 N epochs 后重新选择起伏区间 “1-cycle” policy：学习率会在整个训练期间进行一次由低升高和由高降低的起伏变化 作者还实现了早停机制，当验证集上的准确率（QWK）下降超过0.01时就停止训练 还有一个问题就是 BERT 只能处理长度不大于 512 的文本，故作者将 ASAP 数据集中超过这个长度的作文给去除了（它们主要都来自于 Prompt2） ⭐ Feature Extraction from BERT 显然，单个的 $BERT$ 模型再加上数据集，就已经需要近 $10$ G 的显存了。这对于低资源研究者而言是生命所不能承受之重，故我们也可以直接从预训练的 $BERT$ 中抽取语义化特征来进行分类。具体地，当文本输入到 $BERT$ 中后，它会产生两个输出结果，一个是文本的隐藏表示序列，还有一个就是 $[CLS]$ 标记所输出的分类隐藏表示( $768$ 维)。这个 $768$ 维的隐藏表示可以直接喂给一个线性分类器，从而实现文本的分类。 该文实验中： 作者抽取出该隐藏表示作为输入 模型：Scikit-learn 中带有利伯勒线性求解器和 $L2$ 正则化的逻辑回归分类器 ⭐ DistilBERT $DistilBERT$ 模型是由 $BERT$ 模型蒸馏出来的学生模型，它能够以很小的性能衰减博得参数量的降低并且实现推理时延的大大减少，这对于低资源和快速推理都是比较友好的。 该文实验中： $DistilBERT$ 模型只会实验 “1-cycle” policy Results 所有的机器学习方法都达到了由 $QWK$ 衡量的人类水平的 $IRR$ 在 $5$ 个 $prompt$ 数据集中的 $3$ 个 $prompt$ 上，没有任何公布的结果能够超过供应商的性能 而在所有情况下，一个朴素 $N-gram$ 方法居然比工业界和学术界的最先进水平只低了 $0.03 - 0.06$ 的 $QWK$ 分数 相对 $neural$ 或者 $N-gram$ 的 $representation$，**GloVe 嵌入所带来的性能较低，但却很少有出版物注意到了 GloVe 在 AES 领域的这种负面结果** 该现象的解释可能是：单个关键词对模型的性能有很大的影响。基于词汇的方法在 AES 任务中是有效的，缺乏对特定的基于单词的特征的获取可能会阻碍语义向量的表示 事实上，最近只有一篇有竞争力的关于 AES 的论文使用了非语境的词向量。他们使用了 word2vec，但并没有直接使用单词嵌入，而是首先将单词聚类到一组 $500$ 个 “嵌入集群” 中，然后将文本中出现的词作为该集群的中心点计入特征向量中，这实际上是创建了一个 $500$ 维的词包模型 微调预训练的 BERT 模型达到了与其他方法大致相同的性能水平，且略低于以前公布的结果 由于 $Transformer$ 模型的超参数优化和课程学习的复杂性，这很只能说是一个最差情况 通过较新的深度学习模型、逐步解冻、判别性微调或更大的参数化等复杂的方法，与默认的$BERT$ 实现相比，始终可以产生 $0.01 - 0.02$ 的 $QWK$ 分数上的提升 因此作者有信心，通过优化可以进一步提升这类方法的性能表现。但这些实验结果也表明了：AES 任务结果的上限降低了这种密集优化努力的价值。 即便是在 GPU 支持的云计算上的深度学习方法，与朴素方法相比，端到端的训练时间仍然增加了约 $30 - 100$ 倍 而在朴素方法中，大约有 $75%$ 的特征提取和模型训练时间是由于 $POS$ 标签标注所须的 将 $BERT$ 抽取的特征作为线性分类器输入是一个有趣的折衷方案，虽然在这些数据集上产生的性能略低，但在训练时间上只减慢了 $2$ 倍，且这都是在特征提取方面，其优势就在于这些特征可能保留有完整 $BERT$ 模型的一些语义知识 $prompt$ $2$ 数据集中的文章是较长的议论文，其平均长度为 $378$ 个单词，而 $prompt$ $3 - 6$ 数据集所对应的则是较短且基于来源内容知识的提词，其平均长度为 $98 - 152$ 个单词 在 $prompt 2$ 数据集上 $BERT$ 需要截断文章，而其他方法则不需要，这可能可以为其性能较差作由 微调 $BERT$ 的训练时间会随着 $epochs$ 数量和平均文章长度的增加而 (线性) 增加，这导致了 $prompt$ $2$ 数据集中长篇论文的训练时间更长，几乎与其他数据集的总和一样长 对于**基于内容的较短提词而言，微调 $BERT$ 的性能可以更快地收敛于人类评判员间的可靠性 ($IRR$)**，但在不过 $4$ 个 $epochs$ 后，模型性能就因为过拟合而开始下降；相较之下，对 $prompt$ $2$ 数据集的较长议论文，即使在我们的实验结束后也只有非常小的性能提升 吐了属于是 对于具有可靠评分和特定提词的训练数据，经典方法和神经方法都产生了类似的可靠性，它们与人类评分者之间的可靠性水平大致相同 预训练 $Transformer$ 并对其进行微调以达到这一性能所须的技术开销显著增加，坑爹的是与基线相比，其收益还特别小 对 NLP 学者来说，我们的教训就是：考虑到训练和推理时间的减慢，以及额外的硬件要求，单单使用深度学习进行评分是不可能合理的 但这并不是说 AES 仅限于评分，重点是那些神经模型已被证明具有高于基线优势的领域，此处我们优先考虑三个主要方面： 领域转移 风格 公平性 🔥 Domain Transfer AES 的一个主要挑战是：特定 prompt 的模型无法推广到新 prompt 的作文上去。而收集全新具有可靠分数的特定 $prompt$ 训练集仍然是在课程中扩展 AES 系统的主要障碍之一。相对来说，很少有学者在通用作文评分系统方面取得进展 Phandi 等人提出了一种贝叶斯回归方法，即先提取 $N-gram$ 特征，然后充分利用跨 $prompts$ 的相关特征 Jin 等人使用输入表面 $N-gram$ 和 $POS$ $N-gram$ 的 LSTM 架构模型，显示出了有希望的 $prompt$ 无关结果，但在所有 ASAP 数据集上都相对逊色于 $prompt$ 特定的模型但在实现过程中，大部分工作都是基于对 $prompt$ 特定模型的变通。虽然 $Transformer$ 对它们预训练的数据很敏感，但却很适合在大多数未见过的领域中进行迁移任务。这一点在历史文本的 $POS$ 标注、域外评论的情感分类以及新语境下的问题回答得以证明。最后一个对于基于内容的短文提示来说是有希望的。AES 在评分方面的公开挑战是：训练的模型能够基于知识和领域迁移，有意义地评估短的回应文，而不是记忆正确的、非领域的答案词汇。而早期结果表明，相关的知识已经嵌入到了 $BERT$ 的预训练模型中。这意味着，**$BERT$ 开辟了一条潜在可行的成功之路，而这是 $N-gram$ 模型根本不可能做到的** 🔥 Style &amp; Voice 修辞和作文学者对于 AES 在课堂上的实际使用提出了质疑，并且他们对 AES 系统在写作教学法中的作用表示担忧。事实上，我们在此强调的总结性评分的相对 “解决 “性质是这些专家特别关注的，他们注意到分数和字数等特征之间的高度相关性。现代课堂上对 AES 的使用超越了高考评分，如Project Essay Grade 或Turnitin Revision Assistant。在这里，适应作家的个性是当前的一个主要差距。Dixon-Roman 等人在 AES 的背景下对这些话题提出了一系列问题。对于 AES 来说，有一个未知的领域，以适应个别作家的风格，并根据个人的写作而不是提示特定的典范给予反馈。将这一领域推向个人表达，摆脱特定 $prompt$ 的数据集，可能是赋予 AES 合法性的一条道路，而 $Transformer$ 可能是使这些任务发挥作用的必要技术。 🔥 Fairness 几年前，研究人员提出，在 AES 系统中，人口偏见是值得检查的。但多年后，该领域主要报告了模拟数据的公平性实验，并分享了测量偏见的工具包，而不是真实世界的 AES 实施或高风险数据的结果。在社会科学家的推动下，NLP 研究人员已经看到了基于 Transformer 默认实现缺陷的公平性研究的复兴。目前，包括 AES 在内的学习分析软件的开发者被鼓励专注于学习结果的可扩展的实验性功效证据。而不是专注于具体的种族或性别偏见，或其他更难通过工程实现的公平结果。但 Transformer 架构的细微差别足以捕获巨大的世界知识，在 NLP 中产生快速增加的可解释性。在技术应用之外，但在更广泛的写作评估中，公平性也是一个丰富的话题，有很多文献可以借鉴。处于这两个领域交叉点的研究人员有一个巨大的开放机会，可以更好地理解公平背景下的 AES ，使用最新的工具不仅可以建立可靠的评分，而且可以推动社会变革。","categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"}]},{"title":"math formulas","slug":"math-formulas","date":"2022-01-04T14:44:21.000Z","updated":"2022-01-04T14:45:48.066Z","comments":true,"path":"2022/01/04/math-formulas/","link":"","permalink":"http://example.com/2022/01/04/math-formulas/","excerpt":"老师课上画的重要的公式","text":"老师课上画的重要的公式 将 $n$ 个 不同的的小球 放入 $r$ 个 不同的盒子 中，且 允许 存在空盒 将 $n$ 个 不同的的小球 放入 $r$ 个 不同的盒子 中，且 不允许 存在空盒 将 $n$ 个 不同的的小球 放入 $r$ 个 相同的盒子 中，且 允许 存在空盒 将 $n$ 个 不同的的小球 放入 $r$ 个 相同的盒子 中，且 不允许 存在空盒 将 $n$ 个 相同的的小球 放入 $r$ 个 不同的盒子 中，且 允许 存在空盒 将 $n$ 个 相同的的小球 放入 $r$ 个 不同的盒子 中，且 不允许 存在空盒 将 $n$ 个 相同的的小球 放入 $r$ 个 相同的盒子 中，且 允许 存在空盒 将 $n$ 个 相同的的小球 放入 $r$ 个 相同的盒子 中，且 不允许 存在空盒 分划性质 $$\\large S(n, 1) =$$ $$\\large S(n, 2) =$$ $$\\large S(n, r) =$$ $$\\large S(n, n-1) =$$ $$\\large S(n, n) =$$ 无序分拆性质 $$\\large S(n, 1) =$$ $$\\large S(n, 2) =$$ 错排 $$\\large D_n =$$ 禁止排列 $$\\large Q_n =$$ 容斥原理第三形式 $$\\large N(r) =$$ 非齐次特解 非齐次项 是否为特征根 非齐次特解 $\\large \\beta^{n}$ $\\beta$ 为 $m$ 重特征根 $\\large \\beta^{n}$ $\\beta$ 不为特征根 $\\large n^s$ $1$ 为 $m$ 重特征根 $\\large n^s$ $1$ 不为特征根 $\\large n^s \\beta^n$ $\\beta$ 为 $m$ 重特征根 $\\large n^s \\beta^n$ $\\beta$ 不为特征根 Fibonacci 数和 Catalan 数 $$\\large f(n) =$$ $$\\large C_n =$$ 生成函数的性质 $\\large (1)\\ \\ \\ \\ b_k = a_{k-l}\\ \\ \\ when\\ \\ \\ k \\geqslant l\\ \\ \\ else\\ \\ \\ = 0$ $\\large (2)\\ \\ \\ \\ b_k = a_{k+l}$ $\\large (3)\\ \\ \\ \\ b_k = \\sum_{i=0}^{k} a_i$ $\\large (4)\\ \\ \\ \\ b_k = \\sum_{i=k}^{\\infty} a_i$ 且 $\\sum_{i=0}^{k} a_i$ 是收敛的 $\\large (5)\\ \\ \\ \\ b_k = k a_k$ $\\large (6)\\ \\ \\ \\ b_k = \\frac{a_k}{k+1}$ $\\large (7)\\ \\ \\ \\ c_k = \\alpha a_k + \\beta b_k$ $\\large (8)\\ \\ \\ \\ c_k = a_0 b_k + a_1 b_{k-1} + \\dots + a_k b_0$ 常用生成函数 $$\\large (1)\\ \\ \\ \\ G{1} =$$ $$\\large (2)\\ \\ \\ \\ G{a^k} =$$ $$\\large (3)\\ \\ \\ \\ G{k} =$$ $$\\large (4)\\ \\ \\ \\ G{k(k+1)} =$$ $$\\large (5)\\ \\ \\ \\ G{k(k+1)(k+2)} =$$ $$\\large (6)\\ \\ \\ \\ G{k^2} =$$ $$\\large (7)\\ \\ \\ \\ G{\\frac{1}{k!} } =$$ $$\\large (8)\\ \\ \\ \\ G{\\binom{\\alpha}{k} } =$$ $$\\Large (9)\\ \\ \\ \\ G{\\binom{n+k}{k} } =$$ 附注:$$\\large G{k} =$$ $$\\large G{k(k+1)} =$$ $$\\large G{k(k+1)(k+2)} =$$ $$\\large G{k(k+1)(k+2)(k+3)} =$$ $$\\large G{k} =$$ $$\\large G{k^2} =$$ $$\\large G{k^3} =$$ $$\\large G{k^4} =$$ 指数型生成函数性质 $\\large e(x)e(y) =$ $\\large e(x)e(-x) = $ $\\large e(-x) =$ $\\Large \\frac{e(x) + e(-x)}{2} =$ $\\Large \\frac{e(x) - e(-x)}{2} =$","categories":[{"name":"Combinatorial Mathematices","slug":"Combinatorial-Mathematices","permalink":"http://example.com/categories/Combinatorial-Mathematices/"}],"tags":[{"name":"Combinatorial Mathematices","slug":"Combinatorial-Mathematices","permalink":"http://example.com/tags/Combinatorial-Mathematices/"}]},{"title":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","date":"2021-12-26T09:30:32.000Z","updated":"2021-12-26T09:44:14.807Z","comments":true,"path":"2021/12/26/Automated-Essay-Scoring/","link":"","permalink":"http://example.com/2021/12/26/Automated-Essay-Scoring/","excerpt":"1. Background1.1 Problems基于人工评分的方法具有以下的缺点 耗时耗力 主观性强 误差性大 过程漫长","text":"1. Background1.1 Problems基于人工评分的方法具有以下的缺点 耗时耗力 主观性强 误差性大 过程漫长 1.2 Corresponding Information AES Systems $\\Delta$ e-rater Intelligent Essay Assessor Competition $\\Delta$ Kaggle 的 The Hewlett Foundation: Automated Essay Scoring Metrics $\\Delta$ Quadratic Weighted Kappa (QWK) “ASAP competition adopted QWK as the official evaluation metric“ Mean Absolute Error (MAE) Mean Square Error (MSE) Pearson’s Correlation Coefficient (PCC) Spearman’s Correlation Coefficient (SCC) Evaluation Schemas in-domain : 在同一提词上训练和评估，并且模型的总体性能由所有提词的平均性能所衡量 cross-domain : 在不同的提词上训练和评估，尤其适用于那些使用迁移学习技术的系统 2. Essay Quality Dimensions Difficulty Dimension Description 1 Grammaticality 文章语法 2 Usage 实词与虚词的使用 3 Mechanics 单词拼写、标点符号以及大小写的正误 4 Style 选词与句子结构的多样性 5 Relevance 内容与提词的相关性 6 Organization 文章结构的组织性 7 Development 以例子印证文章观点 8 Cohesion 过渡短语的使用恰当与否 9 Coherence 文章观点间的衔接恰当与否 10 Thesis Clarity 文章主题的阐述如何 11 Persuasiveness 文章中心论点的信服力如何 3. Features3.1 Length-based features因为长度已被证明和文章的整体评分呈现高度正相关的关系，所以基于长度的特征是 AES 系统中最重要的特征类型之一。这类特征**将文章的长度信息进行编码(句子/单词/字符的数量)**。 3.2 Lexical features n-grams 这类特征包含了词的一元文法、二元文法以及三元文法信息。这些单词 n-grams 信息十分有用，因为它们所编码的语法、语义、篇章信息对于 AES 系统都是大有裨益的。n-grams 作为特征的关键优势在于它们是语言独立的，其缺点就是通常需要大量训练数据来学习哪些单词 n-grams 是有用的。 statistics 这类特征包含了基于词 n-grams 而计算出来的统计数据(尤其是一元文法)。 3.3 Embeddings嵌入可以被视为一种 n-grams 特征的变体，可以说是一种语义上比 n-grams 更佳的(词/短语)表示，主要分为以下三类 第一类包含了基于在大语料上预训练词嵌入计算而来的特征 第二类包含了基于 AES 特定的嵌入表示计算而来的特征 第三类包含了可被训练的 one-hot 初始词向量 3.4 Word category features词类特征是基于包含了属于某一特定词汇、句法、语义类别的所有单词构成的词表或词典计算而来的。文章中某些类别的选词可以揭示作者组织思想、对提词作出连贯且一致的反应、掌握标准英语的能力。直观的来说，较高的单词级别表明词汇的使用更为复杂。词类特征有助于概括词的 n-grams 特征，但只在少量训练数据的情况下特别有用。 3.5 Prompt-relevant features提词相关性特征会编码文章内容与其提词的相关性，而一篇不符合提词的文章是不能得到高分的。不同的相似性衡量方法被用来计算一篇文章与其提词的相关性，如：单词重叠率及其变体、单词的主题性、通过随机索引测量的语义相似性等方法。 3.6 Readability features可读性特征需要编码一篇文章有多难读，该特征主要依赖于文章的选词。简单来说，一篇好的文章，需要使用广泛的词汇和多样的句子结构，但又不能晦涩难懂。该特征既可以使用可读性指标(如: Flesch-Kindcaid Reading Ease)来衡量，也可以使用简单的指标(如: type-token ratio)来衡量。 3.7 Syntactic features句法特征编码文章的句法信息，主要可以分为以下三类 Part-of-speech (POS) tag sequences 这类特征需要提供单词 n-grams 的句法概括并编码文章得非语法性和风格 Parse Trees 解析树的深度编码句子中句法结构的复杂性，短语结构规则编码不同语法结构的存在，依存关系计算标题与其从属关系之间的句法距离 Grammatical error rates 这类特征编码文章中语法错误出现的频率，可以使用语言模型计算或者直接从标注的语法错误类型中获得 3.8 Argumentation features论证特征基于文章的论证结构计算而来，因此该特征仅适用于那些具有论证结构的文章。一篇文章的论证结构是一颗树，其节点为论证成分(如: 断言、假设)，而边则表示对应论证成分间的关系(如: 支持、反对)。具体来说，论证特征基于论证结构及其具体内容来计算得出。 3.9 Semantic features语义特征编码文章中不同单词间的词汇语义关联性，主要分为以下两种语义特征 Histogram-based features 计算文章中的每对词之间的点状互信息(PMI)，它会根据共现性来衡量两个词之间的关联程度 通过对PMI值进行分档构建直方图，其中每个档对应的值表示具有该档内PMI值的词对所占的百分比 基于直方图计算出语义特征 直观地，高关联的配对比例越高，可能表明话题的延申越好，而低关联的配对比例越高，可能表明对语言的使用越有新意。 Frame-based features 该特征基于 FrameNet 中的语义框架计算而来。简言之，一个框架可以描述一句话中发生的事件，而框架的事件元素可以是参与相应事件的人或物。而知道文章中的某个论点是由作者还是他人表达的，这对文章论题的清晰度打分有益，因为论题的清晰度应该根据作者自己的论点来衡量。 3.10 Discourse features语篇特征编码文章的语篇结构，主要可以分为以下四种来源 Entity grids 实体网格是一种语篇表征，用于捕捉基于Centering Theory的文本局部连贯性 Discourse parse trees 基于Rhetorical Structure Theory构造的语篇解析树可以对文本的层次结构进行了编码(如: 一个语段是对另一个语段的阐述，还是与另一个语段有对比关系)，它可以用来捕捉文章的局部和整体的一致性特征 Lexical chains 词链是语篇中相关词汇的序列，可用于评估文本的内聚力。一篇包含了很多词链的文章，尤其是那些词链的开头和结尾覆盖了很大跨度的文章，往往具有更强的内聚力 discourse function label 语篇功能标签是定义在一个句子或段落上的，它表明了该句子或段落在特定文章中的语段功能(如: 一个段落是引言还是结论，一个句子是否是文章的论题) 这些标签可用于推导出文章组织评分所需的特征。 4. Corpora Corpora Essay Types Writer’s Language Level Essays Numbers Prompts Numbers CLC-FCE 议论文+记叙文+评论文+建议文+书信 非英语母语者+ESOL考生 1244 篇文章 10 个题词 ASAP 议论文+回应文+记叙文 7 到 10 年级的英国学生 17450 篇文章 8 个题词 TOEFL11 议论文 非英语母语者+托福考生 1100 篇文章 8 个题词 ICLE 议论文 非英语母语者+大学生 1003 篇文章 12 个题词 ICLE 议论文 非英语母语者+大学生 830 篇文章 13 个题词 ICLE 议论文 非英语母语者+大学生 830 篇文章 13 个题词 ICLE 议论文 非英语母语者+大学生 1000 篇文章 10 个题词 AAE 议论文 线上社区 essayforum2 102 篇论文 101 个题词 Corpora Scoring Task Score Range Additional Annotations CLC-FCE 整体评分 分数在 1 到 40 内 额外标注了语言学错误 (约 80 种错误类型) ASAP 整体评分 小则 0 到 3 之间，大则 0 到 60 之间 无额外标注 TOEFL11 整体评分 三个等级：Low，Medium，High 无额外标注 ICLE 文章组织评分 1 到 4 之间以 0.5 增长 无额外标注 ICLE 主题阐述评分 1 到 4 之间以 0.5 增长 无额外标注 ICLE 提词关联性评分 1 到 4 之间以 0.5 增长 无额外标注 ICLE 论点信服力评分 1 到 4 之间以 0.5 增长s 无额外标注 AAE 论点信服力评分 1 到 6 分之间 额外标注了影响信服力的因素 5. Tasks Task Description Additions Holistic scoring 基于文章的整体内容为其进行分数裁定的任务 AES 方向的主要任务 Dimension-specific scoring 针对文章质量的某个维度进行分数裁定 $11$ 种维度见 2 6. Approaches6.1 Neural NetworksA Neural Approach to Automated Essay Scoring “EMNLP 2016” 该论文是第一个使用神经网络进行自动文章评分的，它有效地缓解了模型对繁复的特征工程的需求，其大致处理流程如下 将 one-hot 向量作为输入 convolution 层来捕捉局部文本依赖 recurrent 层则用于捕捉文章中的长程依赖关系 将不同时间步的隐藏表示向量拼接起来以预测文章得分 6.2 Score-Specific Word EmbeddingsAutomatic Text Scoring Using Neural Networks “ACL 2016” 由于低信息词相较于高信息词而言，对文章评分的影响更小。出于此，该论文提出训练一个得分特定的词嵌入表示作为输入，而不是使用第一篇论文的 one-hot 向量。即，该模型能够依据词嵌入辨别高低信息化的用词。 6.3 Document StructureAutomatic Features for Essay Scoring – An Empirical Study “EMNLP 2016” 前面说的两篇论文都是将文章看作为单词的线性序列，因此该论文提出通过建模文章的等级结构来提升 AES 性能，其大致流程如下 词级卷积层接收独热词向量并独立抽取每个句子内的 n-gram 级特征信息 池化层将这些 n-gram 特征抽取出来并压缩为一个句子向量 句子级卷积层接收句子向量并抽取不同句子间的 n-gram 级特征信息 简言之: 将 words 融合以表示 sentences + 将 sentences 融合以表示 document。 6.4 Attention PoolingAttention-based Recurrent Convolutional Neural Network for Automatic Essay Scoring “CoNLL 2017” 为了能够自动地识别文章中重要的字符、单词、句子，该论文将注意力机制引入其中。即，使用一个注意力池化层来代替简单池化。具体地来讲就是，每个注意力池化层接收相应的卷积层输出作为输入，并充分利用可训练的权重矩阵来为输入向量进行加权组合。 6.5 CoherenceSkipFlow: Incorporating neural coherence features for end-to-end automatic text scoring “arXiv 2017” 因为连贯性使文章质量的一个重要维度，因此该论文假设通过计算和利用文章的连贯性得分可以提升整体性评分的性能，其大致流程如下 首先，LSTM 层负责建模文章内依赖关系 其次，额外的层来接收 LSTM 两个位置上的输出并为其计算相似度 (作者将其称为神经连贯性特征，因为直觉上连贯性是和相似度呈正相关的) 然后，神经连贯性特征会增强 LSTM 的输出向量 最后，模型基于神经连贯性特征增强的表示向量来进行整体评分 6.6 Transfer Learning理论上，训练提词特定的 AES 系统可以获得更准确的评分，因为测试相同的题词时，模型可以充分利用训练时学习到的该题词所特定的所有知识。然而，几乎对于任意特定的目标提词而言，其所对应的充足的训练数据都是不存在的。所以，许多 AES 系统都是以一种提词独立的方式训练出来的，这也就意味着少量的目标提词文章和大量的无目标提词文章被用于模型训练。于是，源提词词表和目标提词词表之间潜在的不一致就很可能会导致这些系统的性能下降。 Frustratingly Easy Domain Adaptation “ACL 2007” EasyAdapt 是一种简单而有效的迁移学习算法，它基于假设：”输入数据只可能是源提词或者目标提词的“。首先，我们知道不使用迁移学习的模型会采用一个被源提词和目标提词共享的特征空间，而 EasyAdapt 通过对这个特征空间中的特征进行三次复制来增强这种特征集。 第一次复制会存储两个领域共享的信息 第二次复制会存储源提词的信息 第三次复制会存储目标提词的信息 在这样的特征空间中，目标提词信息的重要性将会是源提词信息的 $2$ 倍，故模型能够更好地适应目标提词的信息。 Flexible Domain Adaptation for Automated Essay Scoring Using Correlated Linear Regression “EMNLP 2015” 该论文将 EasyAdapt 扩展到相关贝叶斯线性岭回归(Correlated Bayesian Linear Ridge Regression)，这使得**目标提词信息的权重可以被学习，而不是固定为 $2$**。 Constrained Multi-Task Learning for Automated Essay Scoring “ACL 2016” 该论文应用 EasyAdapt 来增强特征空间，此外还训练一个对级评级器(pairwise ranker)来评级同一提词的文章对。 TDNN: A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring “ACL 2018” 上述的几个系统还认为有少量的目标提词存在于训练数据中，而该论文则基于训练数据中不存在目标提词的假设来使用迁移学习技术，具体可分为以下两阶段 Stage 1 - 旨在识别测试集中质量极端的目标提词文章(极差或极佳)。作者使用提词无关特征(如语法错误、拼写错误等)在源提词文章上训练一个模型，之后使用该模型来为测试文章进行评分。这一步基于的假设是：质量极差或极佳的文章仅通过通用的特征就可以识别出来了。 Stage 2 - 旨在为剩余的测试文章评分。首先，作者将在 Stage 1 阶段选出的极端质量文章进行自动化标注，极佳为 $1$，极差为 $0$。然后，作者使用提词特定的特征在这些自动标注的文章上训练一个回归器，这一步基于的假设是：这些特定的特征需要正确地捕捉那些质量不极端文章的意义。最后，作者使用这个回归器来为剩余的测试文章进行评分，其分值区间为 $(0, 1)$。 7. State Of The Art Corpus System Scoring Task Approach CLC-FCE Modeling coherence in ESOL learner texts Holistic Ranking ASAP Automated essay scoring with string kernels and word embeddings “in-domain” Holistic Regression ASAP Automated essay scoring with string kernels and word embeddings “cross-domain” Holistic Regression TOEFL11 Investigating the Role of Linguistic Features Holistic Regression ICLE Using Argument Mining to Assess the Argumentation Quality of Essays Organization Regression ICLE Modeling Thesis Clarity in Student Essays Thesis Clarity Regression ICLE Modeling Prompt Adherence in Student Essays Prompt Adherence Regression ICLE Using Argument Mining to Assess the Argumentation Quality of Essays Persuasiveness Regression AAE Modeling Attributes Affecting Argument Persuasiveness in Student Essays Persuasiveness Regression (Neural) Corpus Features QWK PCC MAE CLC-FCE 5 - $0.749$ - ASAP 1 $0.785$ - - ASAP 1 $0.661$ ; $0.779$ ; $0.788$ ; $0.649$ - - TOEFL11 5 - $0.800$ $0.400$ ICLE 6 - - $0.315$ ICLE 4 - - $0.483$ ICLE 4 - $0.360$ $0.348$ ICLE 6 - - $0.378$ AAE 4 - $0.236$ $1.035$ 注: Kaggle Leaderboard 上的最新 SOTA 成绩为 $0.81407$。 Attachments Cambridge Learner Corpus-First Certificate in English exam “CLC-FCE” Automated Student Assessment Prize “ASAP” Automated Essay Scoring for Swedish “Corpora in Swedish” Automated Essay Scoring: A Survey of the State of the Art “IJCAI 2019” Code for “A Neural Approach to Automated Essay Scoring” “Based on Python 2.7 &amp; TensorFlow 1.X” Code for “Automatic Text Scoring Using Neural Networks” “Based on TensorFlow 1.9 &amp; Keras 2.2.2” Code for “SkipFlow: Incorporating neural coherence features for end-to-end automatic text scoring” “No Environment Information”","categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"}]},{"title":"Math","slug":"Math-1-3","date":"2021-12-24T15:42:33.000Z","updated":"2021-12-28T03:04:13.289Z","comments":true,"path":"2021/12/24/Math-1-3/","link":"","permalink":"http://example.com/2021/12/24/Math-1-3/","excerpt":"考试在即，组合数学知识点整理","text":"考试在即，组合数学知识点整理 § 第一章 排列、组合、二项式定理$Ⅰ$ 分配问题 将 $n$ 个 不同的的小球 放入 $r$ 个 不同的盒子 中，且 允许 存在空盒 $$\\large r^n$$ 解: 对于第 $i$ 个小球来说, 它都有 $r$ 个盒子可以放 $(i = 1, 2, \\dots)$所以一共有 $r^n$ 种分配方案。 将 $n$ 个 不同的的小球 放入 $r$ 个 不同的盒子 中，且 不允许 存在空盒 $$\\large r! S(n, r)$$ 解: 因为小球各不相同且每个盒子又不能为空所以, 可以看作是对小球的集合 $M$ 进行 $r$ 分划, 即 $S(n, r)$又因为盒子也各不相同所以还需要对分划后的 $r$ 个小球子集进行排列, 即 $r!$综上所述, 一共有 $r! S(n, r)$ 种分配方案。 将 $n$ 个 不同的的小球 放入 $r$ 个 相同的盒子 中，且 允许 存在空盒 $$\\large \\sum_{i=1}^{r} S(n, i)$$ 解: 因为允许盒子为空且盒子都是一样的, 那么空盒的个数就可能为 $r-1, r-2, \\dots, 0$对于非空盒为 $i\\ \\ (i = 1, 2, \\dots)$ 个的情况, 那么我们只需要对不同小球集合 $M$ 进行 $i$ 分划即可又因为盒子完全相同, 所以无须排列综上所述, 一共有 $\\sum_{i=1}^{r} S(n, i)$ 种分配方案 将 $n$ 个 不同的的小球 放入 $r$ 个 相同的盒子 中，且 不允许 存在空盒 $$\\large S(n, r)$$ 解: 因为盒子完全相同且不允许存在空盒所以直接将不同小球集合 $M$ 进行 $r$ 分划即可因此, 一共有 $S(n, r)$ 种分配方案 将 $n$ 个 相同的的小球 放入 $r$ 个 不同的盒子 中，且 允许 存在空盒 $$\\large \\binom{n+r-1}{r-1} $$ 解: 相当于求 $x_1 + x_2 + \\dots + x_r = n$ 的非负整数解个数此时将 $r-1$ 个 $0$ 作为分隔符插入到 $n$ 个 $1$ 里面, 那个结果字串就可以表示一种非负整数解因此原题又相当于多重集合 $M = {n \\times 1, (r-1) \\times 0}$ 的全排列数由书上定理可知, 其全排列数为 $\\frac{(n+r-1)!}{n! (r-1)!} = \\binom{n+r-1}{r-1} $因此, 一共有 $\\binom{n+r-1}{r-1}$ 种分配方案 将 $n$ 个 相同的的小球 放入 $r$ 个 不同的盒子 中，且 不允许 存在空盒 $$\\large \\binom{n-1}{r-1} $$ 解: 相当于求 $x_1 + x_2 + \\dots + x_r = n$ 的正整数解个数令 $ y_i = x_i - 1$, 则有 $y_1 + y_2 + \\dots + y_r = n - r$ 且 $y_i \\nless 0$ $(i = 1, 2, \\dots, r)$因此相当于求 $y_1 + y_2 + \\dots + y_r = n - r$ 的非负整数解个数此时将 $r-1$ 个 $0$ 作为分隔符插入到 $n-r$ 个 $1$ 里面, 那个结果字串就可以表示一种非负整数解因此原题又相当于多重集合 $M = {(n-r) \\times 1, (r-1) \\times 0}$ 的全排列数由书上定理可知, 其全排列数为 $\\frac{[(n-r)+(r-1)]!}{(n-r)! (r-1)!} = \\binom{n-1}{r-1} $因此, 一共有 $\\binom{n-1}{r-1}$ 种分配方案 将 $n$ 个 相同的的小球 放入 $r$ 个 相同的盒子 中，且 允许 存在空盒 $$\\large \\sum_{i=1}^{r} B(n, i)$$ 解: 因为允许盒子为空且盒子都是一样的, 那么空盒的个数就可能为 $r-1, r-2, \\dots, 0$对于非空盒为 $i\\ \\ (i = 1, 2, \\dots)$ 个的情况, 那么我们只需要对 $n$ 进行 $i$ 无序分拆即可综上所述, 一共有 $\\sum_{i=1}^{r} B(n, i)$ 种分配方案 将 $n$ 个 相同的的小球 放入 $r$ 个 相同的盒子 中，且 不允许 存在空盒 $$\\large B(n, r)$$ 解: 因为盒子完全相同且不允许存在空盒所以直接将 $n$ 进行 $r$ 无序分拆即可因此, 一共有 $B(n, r)$ 种分配方案 $Ⅱ$ 多重集合排列和组合的证明 多重集合 $M = {k_1 \\cdot a_1, k_2 \\cdot a_2, \\dots, k_n \\cdot a_n}$ 的全排列数 $$\\Large \\frac{(k_1 + k_2 + \\dots + k_n)!}{k_1! k_2! \\cdots k_n!}$$ 证: $M$ 的全排列中, $a_1$ 占了 $k_1$ 个又因为所有的 $a_1$ 都完全相同, 所以不需要排列于是, 对 $a_1$ 有 $\\binom{k_1 + k_2 + \\dots + k_n}{k_1}$ 种方案类似地, 在 $M$ 剩余的 $k_2 + k_3 + \\dots + k_n$ 位置中, $a_2$ 占了 $k_2$ 个又因为 $a_2$ 完全相同, 所以不用排列于是, 对 $a_2$ 有 $\\binom{ k_2 + k_3 + \\dots + k_n}{k_1}$ 种方案以此类推并由乘法原则可知一共有$$\\large \\binom{k_1 + k_2 + \\dots + k_n}{k_1} \\cdot \\binom{ k_2 + k_3 + \\dots + k_n}{k_1} \\cdots \\binom{k_n}{k_n}$$ $= $ $$\\large \\frac{(k_1 + k_2 + \\dots + k_n)!}{k_1!(k_2 + k_3 + \\dots + k_n)!} \\cdot \\frac{(k_2 + k_3 + \\dots + k_n)!}{k_2!(k_3 + k_4 + \\dots + k_n)!} \\cdots \\frac{k_n!}{k_n!} $$ $= $ $$\\Large \\frac{(k_1 + k_2 + \\dots + k_n)!}{k_1! k_2! \\cdots k_n!}$$ 多重集合 $M = {\\infty \\cdot a_1, \\infty \\cdot a_2, \\dots, \\infty \\cdot a_n}$ 的 $r$ 组合数 $$\\Large \\binom{r + n - 1}{r}$$ 证: 假设 $x_i$ 表示 $a_i$ 被选取的个数 ($i = 1, 2, \\dots, n$)那么就相当于求 $x_1 + x_2 + \\dots + x_n = r$ 的非负整数解个数我们将 $n-1$ 个 $0$ 插入到 $r$ 个 $1$ 串中去, 那么就会被划分出 $n$ 个 $1$ 的字串, 而第 $i$ 的字串中 $1$ 的个数就表示 $x_i$ 的取值大小先定义多重集合 $N = {r \\times 1, (n-1) \\times 0}$因此, 我们可以知道一种上述的 $10$ 串就可以唯一对应上述方程的一个非负整数解即, 多重集合 $N$ 的 $r$ 组合数与方程的非负整数解一一对应故原题又相当于求多重集合 $N = {r \\times 1, (n-1) \\times 0}$ 的全排列数由书上定理可知, 其全排列数为 $\\frac{[r+(n-1)]!}{r! (n-1)!} = \\binom{r+n-1}{r} $因此, 一共有 $\\binom{r+n-1}{r}$ 种分配方案 $Ⅲ$ 二项式定理$$\\large (x+y)^n = \\binom{n}{0} y^n + \\binom{n}{1} x^1 y^{n-1} + \\dots + \\binom{n}{n-1} x^{n-1} y^1 + \\binom{n}{n} x^n$$ 即 $$\\large (x+y)^n = \\sum_{i=0}^{n} \\binom{n}{i} x^i y^{n-i}$$ $Ⅳ$ 分划数的性质 $$\\large S(n, 1) = 1$$ $$\\large S(n, 2) = 2^{n-1} - 1$$ $$\\large S(n, r) = S(n-1, r-1) + r S(n-1, r)$$ $$\\large S(n, n-1) = \\binom{n}{2}$$ $$\\large S(n, n) = 1$$ $Ⅴ$ 无序分拆数的性质 $$\\large B(n, 1) = 1$$ $$\\large B(n, 2) = \\lfloor \\frac{n}{2} \\rfloor$$ $$\\large B(n) = \\sum_{k=1}^{n} B(n, k)$$ $$\\large B(n+k, k) = B(n, 1) + B(n, 2) + \\dots + B(n, k)$$ $$\\large B(n, n) = 1$$ § 第二章 容斥原理与鸽巢原理$Ⅰ$ 容斥原理的三个表示 设 $S$ 是一有限集合, $P_1, P_2, \\dots, P_m$ 是同集合 $S$ 有关的 $m$ 个性质, 设 $A_i$ 为 $S$ 中具有性质 $P_i$ 的元素构成的集合 ($1 \\leqslant i \\leqslant m$), $\\bar{A_i}$ 是 $S$ 中不具有性质 $P_i$ 的元素构成的集合 ($1 \\leqslant i \\leqslant m$), 则 $S$ 中不具有全部 $m$ 个性质的元素个数为 $$\\large |\\bar{A_1} \\cap \\bar{A_2} \\cap \\dots \\cap \\bar{A_m}| = |S| - \\sum_{i=1}^{m}|A_i| + \\sum_{1 \\leqslant i \\ne j \\leqslant m}^{m} |A_i \\cap A_j| - \\dots + (-1)^{m} |A_1 \\cap A_2 \\cap \\dots \\cap A_m|$$ 设 $S$ 是一有限集合, $P_1, P_2, \\dots, P_m$ 是同集合 $S$ 有关的 $m$ 个性质, 设 $A_i$ 为 $S$ 中具有性质 $P_i$ 的元素构成的集合 ($1 \\leqslant i \\leqslant m$), $\\bar{A_i}$ 是 $S$ 中不具有性质 $P_i$ 的元素构成的集合 ($1 \\leqslant i \\leqslant m$), 则 $S$ 中至少具有一个性质的元素个数为 $$\\large |A_1 \\cup A_2 \\cup \\dots \\cup A_m| = \\sum_{i=1}^{m}|A_i| - \\sum_{1 \\leqslant i \\ne j \\leqslant m}^{m} |A_i \\cap A_j| + \\dots + (-1)^{m-1} |A_1 \\cap A_2 \\cap \\dots \\cap A_m|$$ 设集合 $S$ 中具有性质集合 $P = {P_1, P_2, \\dots, P_m}$ 中恰有 $r$ 个性质的元素个数为 $N(r)$, 则 $$\\large N(r) = w(r) - \\binom{r+1}{r} w(r+1) + \\binom{r+2}{r} w(r+2) - \\dots + (-1)^{m-r} \\binom{m}{r} w(m)$$ 其中, $w(0) = |S|$, $w(r) = \\sum_{1 \\leqslant i_1 \\leqslant \\dots \\leqslant i_r \\leqslant m} N(P_{i_1}, P_{i_2}, \\dots, P_{i_r})$, 而 $N(P_{i_1}, P_{i_2}, \\dots, P_{i_r})$ 表示 $S$ 中具有性质 $P_{i_1}, P_{i_2}, \\dots, P_{i_r}$ 的元素个数 $Ⅱ$ 容斥原理应用 求任意多重集合 $M$ 的 $r$ 组合数 设 $S_{\\infty} = { \\infty \\cdot a_i }$ 取集合 $A$ 为 $S_{\\infty}$ 的 $r$ 组合全体 定义性质集合 $P$ 假设 $a_1$ 元素在 $M$ 中的个数为 x, 那么 $P_1$ 应该指 $r$ 组合中 $a_1$ 的个数大于等于 $x+1$ 取集合 $A_i$ 为满足性质 $P_i$ 的 $r$ 组合全体 计算容斥原理中剩余的各个项 使用容斥原理求解答案 错排问题 $$\\large D_n = n! (1 - \\frac{1}{1!} + \\frac{1}{2!} - \\dots + (-1)^n \\frac{1}{n!})$$ 有禁止模式的错排问题 $$\\large Q_n = n! - \\binom{n-1}{1} (n-1)! + \\binom{n-1}{2} (n-2)! - \\dots + (-1)^{n-1} \\binom{n-1}{n-1} 1! = D_n + D_{n-1}$$ $Ⅲ$ 鸽巢原理 如果把 $n+1$ 个物体放入 $n$ 个盒子里, 那么至少有一个盒子里有两个或更多的的物体 设 $a_1, a_2, \\dots, a_n$ 都是正整数, 如果把 $a_1 + a_2 + \\dots + a_n - n + 1$ 个物体放入 $n$ 个盒子中, 那么至少存在一个 $i \\in [1, n]$, 使得第 $i$ 个盒子至少包含 $a_i$ 个物体 $Ⅳ$ 鸽巢原理应用 寻找和题目条件相关的数值组合、区间等 对数据进行分组 因为须要从 $m$ 个组中选择 $m+1$ 个数 所以, 由鸽巢原理可证原题关系 § 第三章 递推关系$Ⅰ$ 非齐次特解 非齐次项 是否为特征根 非齐次特解 $\\beta^{n}$ $\\beta$ 为 $m$ 重特征根 $\\alpha \\cdot n^m \\cdot \\beta^n$ $\\beta^{n}$ $\\beta$ 不为特征根 $\\alpha \\cdot \\beta^n$ $n^s$ $1$ 为 $m$ 重特征根 $n^m \\cdot (b_s \\cdot n^s + b_{s-1} \\cdot n^{s-1} + \\dots + b_1 \\cdot n + b_0)$ $n^s$ $1$ 不为特征根 $b_s \\cdot n^s + b_{s-1} \\cdot n^{s-1} + \\dots + b_1 \\cdot n + b_0$ $n^s \\beta^n$ $\\beta$ 为 $m$ 重特征根 $n^m \\cdot (b_s \\cdot n^s + b_{s-1} \\cdot n^{s-1} + \\dots + b_1 \\cdot n + b_0) \\beta^n$ $n^s \\beta^n$ $\\beta$ 不为特征根 $(b_s \\cdot n^s + b_{s-1} \\cdot n^{s-1} + \\dots + b_1 \\cdot n + b_0) \\beta^n$ $Ⅱ$ 迭代法 解题步骤 求解 $f(n) = F(f(\\le n))$ 的迭代关系: $f(n) = \\dots = F(?) + f(1)$ 数学归纳法证明之 证明 $k=1$ 时, 上式成立 假设 $n=k$ 时, 上式成立 证明 $n=k+1$ 时, 上式成立 转换递推关系 将变系数的一阶线性递推关系转换为常系数的线性递推关系 将一阶高次递推关系通过变量代换为一阶线性递推关系式 $Ⅲ$ Fibonacci 数和 Catalan 数 Fibonacci 数的性质 $$\\large f(n) = f(n-1) + f(n-2)\\ \\ and\\ \\ f(1)=1, f(0)=1$$ $$\\large f(n) = \\binom{n}{0} + \\binom{n-1}{1} + \\dots + \\binom{n-k}{k}\\ \\ and\\ \\ k = \\lfloor \\frac{n}{2} \\rfloor$$ Catalan 数 $n$ 个 $+1$ 和 $n$ 个 $-1$ 构成的 $2n$ 项 $a_1, a_2, \\dots, a_{2n}$, 其部分和满足 $a_1 + a_2 + \\dots + a_k \\geqslant 0$ ($k = 1, 2, \\dots, 2n$) 的数列的个数等于第 $n$ 个 Catalan 数 $$\\large C_n = \\frac{1}{n+1} \\binom{2n}{n}$$ § 第四章 生成函数$Ⅰ$ 形式幂级数 一般情况下, 形式幂级数中的 $x$ 只是一个抽象符号, 并不需要对其赋予具体的数值, 因此 无须考虑它的收敛性。 形式导数 $DA(x)$ 的规则 $D(\\alpha \\cdot A(x) + \\beta \\cdot B(x)) = \\alpha \\cdot DA(x) + \\beta \\cdot DB(x)$ $D(A(x) \\cdot B(x)) = A(x) \\cdot DB(x) + DA(x) \\cdot B(x)$ $D(A^n(x)) = n \\cdot A^{n-1}(x) \\cdot DA(x) $ $Ⅱ$ 生成函数的性质 $b_k = a_{k-l}\\ \\ \\ when\\ \\ \\ k \\geqslant l\\ \\ \\ else\\ \\ \\ = 0$ $$\\large B(x) = x^l \\cdot A(x)$$ 解: 由题意可知, $A(x) = \\sum_{n=0}^{\\infty} a_n \\cdot x^n$, $B(x) = \\sum_{n=0}^{\\infty} b_n \\cdot x^n$而因为当 $k$ 小于 $l$ 时有 $b_k = 0$, 所以 $B(x) = \\sum_{n=l}^{\\infty} a_{n-l} \\cdot x^n$此时令 $m = n - l$, 就有 $B(x) = \\sum_{m=0}^{\\infty} a_{m} \\cdot x^{m+l} = x^l \\cdot \\sum_{m=0}^{\\infty} a_{m} \\cdot x^{m} = x^l \\cdot A(x)$因此 $B(x) = x^l A(x)$ $b_k = a_{k+l}$ $$\\large \\frac{1}{x^l} \\cdot [A(x) - \\sum_{k=0}^{l-1} a_k \\cdot x^k]$$ 解: 因为 $b_k = a_{k+l}$, 所以 $B(x) = \\sum_{n=0}^{\\infty} b_n \\cdot x^n = \\sum_{n=0}^{\\infty} a_{n+l} \\cdot x^n$此时对右边乘 $x^l$ 再除以 $x^l$ 则有, $B(x) = \\frac{1}{x^l} \\sum_{n=0}^{\\infty} a_{n+l} \\cdot x^{n+l}$然后令 $m = n+l$ 并加上缺失项减去缺失项, 则有 $B(x) = \\frac{1}{x^l} \\cdot [\\sum_{m=0}^{\\infty} a_{m} x^m - \\sum_{k=0}^{l-1} a_k x^k]$因此 $B(x) = \\frac{1}{x^l} \\cdot [A(x) - \\sum_{k=0}^{l-1} a_k x^k]$ $b_k = \\sum_{i=0}^{k} a_i$ $$\\Large B(x) = \\frac{A(x)}{1-x}$$ 解: 因为 $b_k = \\sum_{i=0}^{k} a_i$, 则有$b_0 = a_0$$b_1 x = (a_0 + a_1) x$$b_2 x^2 = (a_0 + a_1 + a_2) x^2$$\\dots$$b_n x^n = (a_0 + a_1 + \\dots + a_n) x^n$$\\dots$将所有上式两边分别相加, 可得 $B(x) = a_0 (1 + x + x^2 + \\dots) + a_1 x (1 + x + x^2 + \\dots) + a_2 x^2 (1 + x + x^2 + \\dots) + \\dots$ 不难发现 $1 + x + x^2 + \\dots$ 为公共项, 于是提取之后可得 $B(x) = (a_0 + a_1 x + a_2 x^2 + \\dots)(1 + x + x^2 + \\dots) = A(x) \\cdot \\frac{1}{1-x}$ 因此 $B(x) = \\frac{A(x)}{1-x}$ $b_k = \\sum_{i=k}^{\\infty} a_i$ 且 $\\sum_{i=0}^{k} a_i$ 是收敛的 $$\\large \\frac{A(1) - xA(x)}{1-x}$$ 解: 因为 $A(1) = \\sum_{k=0}^{\\infty} a_k$ 是收敛的, 所以 $b_k = \\sum_{i=k}^{\\infty} a_i$ 是存在的, 于是有$b_0 = a_0 + a_1 + \\dots = A(1)$$b_1 x = (a_1 + a_2 + \\dots) x = A(1) x - a_0 x$$\\dots$$b_n x^n = (a_n + a_{n+1} + \\dots) x^n = A(1) x^n - (a_0 + a_1 + \\dots + a_{n-1}) x^n$$\\dots$将所有上式两边分别相加, 可得$B(x) = A(1) (1 + x + x^2 + \\dots) - [a_0 x (1 + x + x^2 + \\dots) + a_1 x^2 (1 + x + x^2 + \\dots) + \\dots]$ $= A(1) \\frac{1}{1-x} - x (a_0 + a_1 x + a_2 x^2 + \\dots)(1 + x + x^2 + \\dots)$ $= A(1) \\frac{1}{1-x} - x A(x) \\frac{1}{1-x} $ 因此 $B(x) = \\frac{A(1) - x A(x)}{1-x}$ $b_k = k a_k$ $$\\large B(x) = x A’(x)$$ 解: $B(x) = \\sum_{n=0}^{\\infty} b_n x^n = \\sum_{n=0}^{\\infty} n a_n x^n $ $ = x \\sum_{n=0}^{\\infty} n a_n x^{n-1} = x \\sum_{n=0}^{\\infty} a_n (x^n)’ $ 因此 $B(x) = x A’(x)$ $b_k = \\frac{a_k}{k+1}$ $$\\large B(x) = \\frac{1}{x} \\int_{0}^{x} A(t) dt$$ 解: $B(x) = \\sum_{n=0}^{\\infty} b_n x^n = \\sum_{n=0}^{\\infty} \\frac{a_n}{n+1} x^n$ 因为 $\\frac{x^{n+1}}{n+1}$ 是积分 $\\int t^n dt$ 的一个原函数, 所以上式变为 $B(x) = \\frac{1}{x} \\sum_{n=0}^{\\infty} a_n \\int_{0}^{x} t^n dt = \\frac{1}{x} \\int_{0}^{x} \\sum_{n=0}^{\\infty} a_n t^n dt $ 因此, $B(x) = \\frac{1}{x} \\int_{0}^{x} A(t) dt$ $c_k = \\alpha a_k + \\beta b_k$ $$\\large C(x) = \\sum_{n=0}^{\\infty} c_k x^k = \\alpha A(x) + \\beta B(x)$$ $c_k = a_0 b_k + a_1 b_{k-1} + \\dots + a_k b_0$ $$\\large C(x) = A(x) B(x)$$ $Ⅲ$ 常用生成函数 $$\\large G{1} = \\frac{1}{1-x}$$ 证: $G{1} = \\sum_{k=0}^{\\infty} 1 \\cdot x^k = \\frac{1}{1-x}$ $$\\large G{a^k} = \\frac{1}{1-ax}$$ 证: $G{a^k} = \\sum_{k=0}^{\\infty} a^k \\cdot x^k = \\sum_{k=0}^{\\infty} (ax)^k = \\frac{1}{1-ax}$ $$\\large G{k} = \\frac{x}{(1-x)^2}$$ 证: $G{k} = \\sum_{k=0}^{\\infty} k \\cdot x^k = x \\sum_{k=0}^{\\infty} (x^k)’ = x (\\sum_{k=0}^{\\infty} x^k)’ = x (\\frac{1}{1-x}) = \\frac{x}{(1-x)^2}$ $$\\large G{k(k+1)} = \\frac{2x}{(1-x)^3}$$ 证: $G{k(k+1)} = \\sum_{k=0}^{\\infty} k(k+1) x^k = x \\sum_{k=0}^{\\infty} (x^{k+1})’’ = x (\\sum_{k=0}^{\\infty} x^{k+1})’’ = x (\\frac{1}{1-x} - 1)’’ = \\frac{2x}{(1-x)^3}$ $$\\large G{k(k+1)(k+2)} = \\frac{6x}{(1-x)^4}$$ 证: $G{k(k+1)(k+2)} = \\sum_{k=0}^{\\infty} k(k+1)(k+2) x^k = x \\sum_{k=0}^{\\infty} (x^{k+2})^{(3)} = x (\\sum_{k=0}^{\\infty} x^{k+2})^{(3)} = x (\\frac{1}{1-x} - 1 - x)^{(3)} = \\frac{6x}{(1-x)^4}$ $$\\large G{K^2} = \\frac{x(1+x)}{(1-x)^3}$$ 证: $G{k^2} = \\sum_{k=0}^{\\infty} k^2 x^k = \\sum_{k=0}^{\\infty} k(k+1) x^k - \\sum_{k=0}^{\\infty} k x^k = \\frac{2x}{(1-x)^3} - \\frac{x}{(1-x)^2} = \\frac{x(1+x)}{(1-x)^3}$ $$\\large G{\\frac{1}{k!} } = e^x$$ 证: $G{\\frac{1}{k!} } = \\sum_{k=0}^{\\infty} \\frac{1}{k!} x^k = e^x$ $$\\large G{\\binom{\\alpha}{k} } = (1+x)^{\\alpha}$$ 证: $G{\\binom{\\alpha}{k} } = \\sum_{k=0}^{\\infty} \\binom{\\alpha}{k} x^k = \\sum_{k=0}^{\\alpha} \\binom{\\alpha}{k} x^k = (1+x)^{\\alpha}$ $$\\Large G{\\binom{n+k}{k} } = \\frac{1}{(1-x)^{n+1}}$$ 证: 由牛顿二项式定理可知, $(1-x)^{-n} = \\frac{1}{(1-x)^n} = \\sum_{k=0}^{\\infty} \\binom{(n-1)+k}{k} x^k$ 因此 $G{\\binom{n+k}{k} } = \\sum_{k=0}^{\\infty} \\binom{n+k}{k} x^k = \\frac{1}{(1-x)^{n+1}}$ 附注:$$\\large G{k} = \\frac{x}{(1-x)^2}$$$$\\large G{k(k+1)} = \\frac{2x}{(1-x)^3}$$$$\\large G{k(k+1)(k+2)} = \\frac{6x}{(1-x)^4}$$$$\\large G{k(k+1)(k+2)(k+3)} = \\frac{24x}{(1-x)^5}$$ $$\\large G{k} = \\frac{x}{(1-x)^2}$$$$\\large G{k^2} = \\frac{x}{(1-x)^3} \\cdot (1 + x)$$$$\\large G{k^3} = \\frac{x}{(1-x)^4} \\cdot (1 + 4x + x^2)$$$$\\large G{k^4} = \\frac{x}{(1-x)^5} \\cdot (1 + 11x + 11x^2 + x^3)$$ $Ⅳ$ 生成函数求解递推关系 令 $A(x) = \\sum_{n=0}^{\\infty} f(n) x^n$; 将 $f(n)$ 的递推关系转化为 $A(x)$ 的递推关系; 求解 $A(x)$ 的展开表达式, 其中 $x^n$ 的系数就是所求的 $f(n)$; $Ⅴ$ 生成函数求计数问题 组合数 从 $n$ 元集合 $S = {a_1, a_2, \\dots, a_n }“$中取 $k$ 个元素, 并且限定元素 $a_i$ 的出现次数集合为 $M_i$ 把 $k$ 个相同的小球放入 $n$ 个不同的盒子 $a_1, a_2, \\dots, a_n$ 中, 限定盒子 $a_i$ 的容量集合为 $M_i$ $$\\Large \\prod_{i=1}^{n} \\Bigl ( \\sum_{m \\in M_i} x^m \\Bigr)$$ 排列数 多重集合 $M = {\\infty \\cdot a_1, \\infty \\cdot a_2, \\dots, \\infty \\cdot a_n}$ 的 $k$ 排列中, 若限定元素 $a_i$ 出现的次数集合为 M_i 把 $k$ 个不同的小球 $1, 2, \\dots, k$ 放入 $n$ 个不同的盒子 $a_1, a_2, \\dots, a_n$ 中, 限定盒子 $a_i$ 的容量集合为 $M_i$ $$\\Large \\prod_{i=1}^{n} \\Bigl ( \\sum_{m \\in M_i} \\frac{x^m}{m!} \\Bigr)$$ 数列 ${1, 1, \\dots, 1}$ 的指数型生成函数 $e(x) = \\sum_{n=0}^{\\infty} \\frac{x^n}{n!}$ 具有与指数函数相似的性质 $e(x)e(y) = e(x+y)$ $e(x)e(-x) = e(0) = 1$ $e(-x) = \\frac{1}{e(x)}$ $\\large \\frac{e(x) + e(-x)}{2} = 1 + \\frac{x^2}{2!} + \\frac{x^4}{4!} + \\dots $ $\\large \\frac{e(x) - e(-x)}{2} = x + \\frac{x^3}{3!} + \\frac{x^5}{5!} + \\dots $","categories":[{"name":"Combinatorial Mathematices","slug":"Combinatorial-Mathematices","permalink":"http://example.com/categories/Combinatorial-Mathematices/"}],"tags":[{"name":"Combinatorial Mathematices","slug":"Combinatorial-Mathematices","permalink":"http://example.com/tags/Combinatorial-Mathematices/"}]},{"title":"How does Length Prediction Influence the Performance of Non-Autoregressive Translation?","slug":"How-does-Length-Prediction-Influence-the-Performance-of-Non-Autoregressive-Translation","date":"2021-12-17T15:29:54.000Z","updated":"2021-12-17T15:36:21.944Z","comments":true,"path":"2021/12/17/How-does-Length-Prediction-Influence-the-Performance-of-Non-Autoregressive-Translation/","link":"","permalink":"http://example.com/2021/12/17/How-does-Length-Prediction-Influence-the-Performance-of-Non-Autoregressive-Translation/","excerpt":"EMNLP 2021 : How Length Prediction Influence the Performance of Non-Autoregressive Translation? 长度预测(length prediction)是 NAT 模型的一项特殊任务，旨在(预先/动态)决定目标序列的长度。然而长度预测性能由何决定又如何与翻译质量相联系这个问题却鲜有人关注，因此作者基于 CMLM 模型进行了大量的实验以研究如下两个问题： 影响长度预测性能的因素是什么？ 长度预测是如何影响翻译质量的？","text":"EMNLP 2021 : How Length Prediction Influence the Performance of Non-Autoregressive Translation? 长度预测(length prediction)是 NAT 模型的一项特殊任务，旨在(预先/动态)决定目标序列的长度。然而长度预测性能由何决定又如何与翻译质量相联系这个问题却鲜有人关注，因此作者基于 CMLM 模型进行了大量的实验以研究如下两个问题： 影响长度预测性能的因素是什么？ 长度预测是如何影响翻译质量的？ 引言Transformer 模型实现了训练时的并行化，而 Vanilla NAT 模型则实现了推理时的并行化。但 NAT 基于的条件独立性假设使其翻译质量远不如对应的 AT 模型，因而各种方法被提出以提升 NAT 模型性能，包括了基于插入的方法、基于迭代的方法、基于隐变量的方法等等。这些方法都希望将目标序列依赖关系建模施加到 NAT 模型推理阶段，但除此之外，NAT 模型的长度预测问题也是很重要的。之前的学者所提出的方法大致可以分为以下四类： 基于对齐 Gu 等人通过繁殖力来隐式地预测目标长度；但这可能会引入噪声，因为我们并没有完全正确的对齐关系；当然 Gu 等人还提出了 NPD 来提升模型性能； 基于统计 模型在训练集上学习一个比例 α 来讲源句长度映射为目标长度；显然，这种频次统计 α 值无法兼顾各条数据样本的差异性； 基于回归 让分类器基于源端表示来预测出目标长度；这个方法直观上比之前的好，但是这种固定长度仍然会抑制推理的灵活性； 基于插入 通过动态地执行 tokens 插入操作，模型就可以在推理期间动态地调整目标序列的长度；其缺陷就是大大降低了 NAT 模型的推理速度； 实验分析基于统计的长度预测分析 De 和 Ro 的句子长度普遍要长于 En 的句子; 对于所有的语言对，随着源端长度的增长，长度比率 $\\alpha$ 也会随之变化 (通常都是不断衰减)，这说明单一的线性模型是不充分的； 训练数据集上的 $\\alpha$ 与测试数据集上的 $\\alpha$ 值存在着明显的差值(gap)，这可能会导致长度预测的错误发生； 对于 En-de 语言对，原始数据上的 $\\alpha$ 的标准差和均值范围始终大于蒸馏数据集，这说明蒸馏数据集更加的干净和简单；对于 En-Ro 语言对，这种差距没有那么明显。 总的来说，对于基于统计的方法 $L_y = \\alpha L_x$ 而言，其优点是简单且时间效率较高；其缺陷在于统计值 $\\alpha$ 过于笼统而忽视了各条数据样本间的差异；进一步来讲，目标长度并不是仅仅依赖于源端长度的，它还依赖于各种语言属性(如：句法、语义等) 长度预测错误分析 NAT 模型，尤其是训练在蒸馏数据集上的，明显优于 AT 教师模型，这说明了 NAT 模型能够明确地建模目标长度分布，可能的原因为：相较于 AT 模型的隐式预测，NAT 模型的明确预测能够获得更好的性能； 虽然直觉上来说，因为原始数据集中的训练数据集和测试数据集同分布，所以训练在原始数据集上的 NAT 模型应该会优于在蒸馏数据集上的模型；但事实却不然；这可能是因为：蒸馏数据集更加的干净和单调，这就使得 NAT 模型更容易符合目标长度分布，尽管它的长度错误是更大的； 大部分长度错误都在 5 个 tokens 以内，并且在原始数据集上训练的 NAT 模型($R$)、在蒸馏数据集上训练的 NAT 模型($D$)以及在原始数据集上训练的 AT 模型($KD$)所对应的长度预测错误都有着明显不同的分布； 长度预测与翻译质量的关联分析① 源句长度源句长度的影响不大，可以不将其考虑在内。 ② 长度错误随着长度错误的增加，模型的翻译质量几乎呈线性衰减，这表明 AT 和 NAT 模型都存在着这样的关系：长度错误与翻译质量成负相关。 ③ 翻译上界除了 Ro-En 结果，真实长度能够明确地提升翻译质量，这是因为预测的长度可以被视为一种离散的隐变量，所以目标长度建模和标记预测之间存在着强联系；而在 Ro→En 上，只有原始数据集上出现了相反的现象，这可能是因为过拟合训练集；如果模型能够在长度候选中推断出其最佳的目标长度，那么翻译质量还能够得到潜在的提升。 ④ 迭代改进 较少的迭代会将真实长度 $L$ 打上错误的得分*； 较少的几轮迭代所产生的翻译句子中通常都存在着许多重复标记； 首轮迭代中，真实长度 L* 反而产生了更多的重复标记，表明了即使给定了真实目标长度 L*，模型也未必能够善加利用； 经过更多的迭代改进之后，真实长度下的翻译句子质量会更好，说明了更多的迭代起了十分重要的作用。 总结 目标长度是 NAT 模型中的一个重要隐变量，且其与翻译质量之间存在着强关联，因此应该得到重视； 准确地预测到真实的目标长度虽然有益于提升模型性能，但是如果模型在长度束中能够推理出其最佳目标长度时，模型的翻译质量还可以获得进一步提升以达到性能上界； 在改善 NAT 翻译质量方面，灵活的解码策略比竭力追求准确的长度预测更有效，这是因为语言具有复杂性和多样性，基本上不存在 gold 目标长度；","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"Non-Autoregressive Text Generation with Pre-trained Language Models","slug":"Non-Autoregressive-Text-Generation-with-Pre-trained-Language-Models","date":"2021-12-10T05:31:31.000Z","updated":"2021-12-10T05:33:30.098Z","comments":true,"path":"2021/12/10/Non-Autoregressive-Text-Generation-with-Pre-trained-Language-Models/","link":"","permalink":"http://example.com/2021/12/10/Non-Autoregressive-Text-Generation-with-Pre-trained-Language-Models/","excerpt":"AG 模型因其链式条件概率建模而饱受高时延之痛；NAG 模型又因其强条件独立性假设而深陷低性能的泥沼。因此，作者提出使用预训练 BERT 模型来作为 NAG 模型的骨干从而提升其性能，并设计了一种简单却不失优雅的解码机制来缓解 NAG 模型的两个通病。为了进一步提升 NAG 模型在推理速度上的优势，作者额外引入了 $ratio$-$first$ 解码策略来加快模型解码速度。在三个文本生成任务($text\\ \\ summarization$、$sentence\\ \\ compression$、$machine\\ \\ translation$)上进行了实验，结果表明了作者的模型确实优于其他 NAG 模型。","text":"AG 模型因其链式条件概率建模而饱受高时延之痛；NAG 模型又因其强条件独立性假设而深陷低性能的泥沼。因此，作者提出使用预训练 BERT 模型来作为 NAG 模型的骨干从而提升其性能，并设计了一种简单却不失优雅的解码机制来缓解 NAG 模型的两个通病。为了进一步提升 NAG 模型在推理速度上的优势，作者额外引入了 $ratio$-$first$ 解码策略来加快模型解码速度。在三个文本生成任务($text\\ \\ summarization$、$sentence\\ \\ compression$、$machine\\ \\ translation$)上进行了实验，结果表明了作者的模型确实优于其他 NAG 模型。 现存问题 NAG 模型具备高并行化能力因而能够更快地推理，但其生成质量常常落后于对应的 AG 模型； 之前的 NAG 模型需要在推理输出序列前就预测好目标序列的长度，这就使得其不得不额外增加一个长度预测模块； 而长度预测模块所预测到的最佳长度有很可能不是正确的目标长度，这使其又不得不采取长度束再评分方法，该方法大大削弱了 NAG 模型的快速推理优势； 现存的 NAG 模型都**需要强条件独立性以支持其并行解码范式，这就会导致其翻译出不符合语法规则的语段(重译问题)**； NAG-BERT 针对 NAG 模型性能问题，作者引入预训练 BERT 模型来提升模型性能； 针对长度预测问题，作者设计了一个基于模型决策的解码机制来动态地调整目标长度； 针对条件独立性假设，作者在损失函数中额外引入一项损失以缓解重译问题； 针对 NAG 的低延时优势，作者设计了一个解码策略 $ratio$-$first$ 来进一步提升解码速度。 Model Architecture 整个模型就是在 BERT 架构上添加一个 CRF 网络，而其中的 BERT 模型部分都是用预训练的 BERT 参数来进行初始化。 参照 BERT 模型，作者首先将 、 这两个特殊标记添加到输入序列中去。然后使用 标记来讲输入序列的长度填充到最长，这样可以保证输入序列的长度一定不小于目标长度。接下来就是简单的多头自注意力网络和位置级前馈神经网络的堆栈而已(和 BERT 或者说 Transformer encoder 是一样的)。 经过 BERT 编码后的隐藏表示 $H$ 会被喂给一个线性链 CRF 网络，长度为 $T’$ 目标序列 $Y$ 的似然函数如下所示：$$\\large P_{CRF} (Y \\mid X) = \\frac{e^{S (X, Y)}}{\\sum_{Y’} e^{S (X, Y’)}} = \\frac{1}{Z (X)} exp (\\sum_{i=1}^{T’} \\Phi_{y_i} (h_i) + \\sum_{i=2}^{T’} t(y_{i-1}, y_i))$$ 其中，$Z(X)$ 是归一化因子；$\\Phi_{y_i} (h_i)$ 代表了 $y_i$ 在位置 $i$ 上的得分；神经网络参数 $\\Phi$ 会将 BERT 输出的隐藏表示映射到目标词表空间中去；$t(y_{i-1}, y_i)$ 表示从标签 $y_{i-1}$ 转移到标签 $y_i$ 的转移得分；$T$ 就是转移矩阵 又因为目标词表很大(32k)，所以直接建模转移矩阵 $T$ 和归一化因子 $Z(X)$ 是不切实际的，故作者选择采用 Sun 等人的方法： 对于 $T$ 矩阵，通过分解的方法，将 $T$ 分解为 $E_1$ 和 $E_2$ 两个较小的矩阵，其中 $T = E_1 × E_2^T$； 对于归一化因子 $Z(X)$，与其搜索所有可能的路径，倒不如使用预先定义的搜索束进行剪枝； Length Prediction作者的基本想法就是希望模型能够通过生成一个特殊标记 () 来动态地停止序列生成。为此，作者将两个连续的 标记添加到目标序列的末尾，进而模型能够在两个 之间学习到一个确定的转移行为，即 → 。这是因为在训练期间，模型压根看不到任何一个从 转移回回目标词表的转移行为。 推理期间，结果 $Y$ 需要能够最大化 CRF 的评分函数，该评分函数可分解为如下形式：$$\\large S (X, Y’) = \\sum_{i=1}^{T} \\Phi_{y_i’} (h_i) + \\sum_{i=2}^{T} t(y_{i-1}’, y_i’) = \\Phi_{y_1’} (h_1) + \\sum_{i=2}^{T} {\\Phi_{y_i’} (h_i) + t(y_{i-1}’, y_i’)}$$ 一旦解码路径上出现了一个 标记，那么接下来的所有转移行为都将在 和 中进行。简言之，翻译句子的后续子序列仅由 构成，并且会被删除。 Ratio-First DecodingBERT 的输出其实可以分为两段，第一段为第一个 之前看的输出标记序列，第二段则为剩余部分。如上图所示，y1 y2 y3 y4 为第一段，剩余为第二段。不难发现，第二段对于最终输出结果毫无作用，故而去除之。这足以说明只考虑 BERT 输出的前半段可以提升模型的解码速度。因此，对于那些已知目标序列短于源序列的任务，作者只使用 BERT 输出的前 $[\\alpha \\cdot T]$ 长度的子序列来进行推理。其中，$T$ 是源序列长度；$\\alpha$ 是数据统计值；$[\\cdot]$ 是整数舍入法。正式的公式如下所示：$$\\large \\hat{Y} = \\underset{Y’}{\\operatorname {argmax}} \\mathcal{F} (X, Y’, \\alpha) = \\underset{Y’}{\\operatorname {argmax}} { \\sum_{i=1}^{[\\alpha \\cdot T]} \\Phi_{y_i’} (h_i) + \\sum_{i=2}^{[\\alpha \\cdot T]} t(y_{i-1}’, y_i’) }$$ 值得注意的是： 当 α = 1.0 时，该解码策略就退化为了标准的解码策略； ratio-first 虽然会减短目标序列的最大长度，但是其实际输出长度仍然由模型动态地决定； ratio-first 能够在保留生成质量的同时显著提高推理速度； Learning ObjectiveNAG 因其在输出标记上的条件独立性近似而偏爱重复单词，引入目标端明确的依赖关系可缓解之。作者提出在 NAG 的上下文中使用无可能表示(unliklihood formulation)，即：将否定的候选词集定义为在预设的上下文窗口 $c$ 内的邻近标记。具体的公式如下所示：$$\\large \\mathcal{L}{CA} (Y \\mid X) = - \\sum{i=1}^{T’} {\\log p_{\\theta} (y_i \\mid h_i; X) + l_{CA} (i)}$$$$\\large l_{CA} (i) = \\sum_{j = i - c,\\ y_j \\ne y_i}^{j = i + c} \\log (1.0 - p_{\\theta} (y_j \\mid h_i; X))$$ 其中，$l_{CA} (i)$ 被用来放大位置 $i$ 邻近重复标记的损失值；$\\mathcal{L}_{CA}$ 则被用来最小化训练损失。于是，该策略会打消模型产生邻近的重复标记的积极性。 Results 作者在机器翻译任务上得出的结论有： NAG-BERT 模型在性能和速度上都实现了有效的提升； ratio-first 解码策略在损失一点性能的同时，将解码速度提高到 13.92 倍。","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation","slug":"ENGINE-Energy-Based-Inference-Networks-for-Non-Autoregressive-Machine-Translation","date":"2021-12-09T06:40:14.000Z","updated":"2021-12-09T06:41:30.409Z","comments":true,"path":"2021/12/09/ENGINE-Energy-Based-Inference-Networks-for-Non-Autoregressive-Machine-Translation/","link":"","permalink":"http://example.com/2021/12/09/ENGINE-Energy-Based-Inference-Networks-for-Non-Autoregressive-Machine-Translation/","excerpt":"知识蒸馏是一种用于将 AT 模型知识迁移到 NAT 模型的方法，它能够有效地帮助 NAT 模型缓解多模问题(multi-modality problem)。准确的来说，通过一个已训练的 AT 模型来翻译原始语料中的源语言句子，然后将 AT 模型预测的翻译句子取代源预料中的 ground true，从而构造出蒸馏数据集(distill dataset)，最后 NAT 模型将在蒸馏数据集上进行训练。","text":"知识蒸馏是一种用于将 AT 模型知识迁移到 NAT 模型的方法，它能够有效地帮助 NAT 模型缓解多模问题(multi-modality problem)。准确的来说，通过一个已训练的 AT 模型来翻译原始语料中的源语言句子，然后将 AT 模型预测的翻译句子取代源预料中的 ground true，从而构造出蒸馏数据集(distill dataset)，最后 NAT 模型将在蒸馏数据集上进行训练。 但是，这种方法将会导致 NAT 模型只能够学习到 AT 模型在固定的数据集上的知识，从而导致 NAT 模型的性能落后于 AT 模型。为此，作者将 NAT 模型视作一个推理网络(inference network)， 该网络被训练以最小化 AT 教师模型的能量(energy)。作者认为这能让 NAT 模型学习到额外的能量信息。为了能够让推理网络最小化 AT 能量，这个能量就必须关于推理网络的输出是可微的，这样就可以进行基于梯度的优化。 Energy对于一个自回归的神经机器翻译系统而言，由链式条件分解有：$$\\large \\log p_{\\Theta} (y \\mid x) = \\sum_{t=1}^{\\mid y \\mid} \\log p_{\\Theta} (y_t \\mid y_{0:t-1}, x)$$ 这个模型就可以被视作一个基于能量的模型(energy-based model)，而其**能量函数(Energy Function)**为：$$\\large E_{\\Theta} (x, y) = - \\log p_{\\Theta} (y \\mid x)$$ 给定训练的参数 $\\Theta$，模型进行推理时就是需要找到一个能够最小化能量(energy)的翻译句子：$$\\large \\hat{y} = \\underset{y}{\\operatorname {argmin}} E_{\\Theta} (x, y)$$ 想要找到最小化能量的翻译句子就会涉及到组合搜索(combinatorial search)，作者训练一个推理网络来近似这种组合搜索。其想法就是：用一个被训练来生成近似最优预测的网络的输出来代替测试时的组合搜索，尤其是被应用在结构化预测中的组合搜索。而这个推理网络 $A_{\\Psi}$ 需要将输入序列 $x$ 映射为翻译序列 $y$，其训练目标函数为 $A_{\\Psi} \\approx \\underset{y}{\\operatorname {argmin}} E_{\\Psi} (x, y)$，而推理网络参数 $\\Psi$ 的训练方法如下：$$\\large \\hat{\\Psi} = \\underset{\\Psi}{\\operatorname {argmin}} \\sum_{\\left\\langle x, y \\right\\rangle \\in \\mathcal{D}} E_{\\Theta} (x, A_{\\Psi} (x))$$ 值得注意的是，推理网络 $A_{\\Psi}$ 的模型结构可以不同于 Energy Function 的模型结构，因此，作者将 NAT 模型作为推理网络，而 AT 模型作为功能函数。其目标就是：兼取 AT 模型的高质量和 NAT 模型的低时延。 Energy Function为了能够对推理网络的参数 $\\Psi$ 进行基于梯度的优化，作者将 $y$ 定义为翻译句子上的词分布序列，具体形式如下：$$\\large E_{\\Theta} (x, y) = \\sum_{t=1}^{\\mid y \\mid} e_t (x, y)$$$$\\large e_t (x, y) = - y_t^T \\log p_{\\Theta} (\\ \\dot\\ \\mid y_0, y_1, \\dots, y_{t-1}, x)$$上式中的 $p_{\\Theta} (\\ \\dot\\ \\mid y_0, y_1, \\dots, y_{t-1}, x)$ 表示词上的完整分布。 通过使用独热分布来代替单词，作者又恢复了原始能量；而为了训练一个推理网络以最小化这种能量，作者所需要的一个能够生成词分布序列的网络结构，NAT 模型则恰好满足。因为原始能量所涉及的分布是独热的，对推理网络来说，输出独热或近似独热的分布也是有利的。作者将推理网络视作生成一个长度为 $T$ 的预测向量 $z_t$ 序列的模块，然后再考虑两个操作 $O_1$ 和 $O_2$，它们会被用于将这些预测向量映射成词分布以用于计算能量。模型总览如下图所示： Operators如上所述，作者需要两个操作 $O_1$ 和 $O_2$ 来管理推理网络和能量函数之间的接口，其中：$O_1$ 需要调节推理网络输出的预测向量 $z_t$，然后将其喂给能量函数中的解码器；$O_2$ 需要决定如何使用词分布 $p_{\\Theta}$ 来计算单词 $y$ 的对数概率。最终，能量函数的局部被改写为如下形式：$$\\large e_t (x, y) = - O_2 (z_t)^T \\log p_{\\Theta} (\\ \\dot\\ \\mid O_1 (z_0), O_1 (z_1), \\dots, O_1 (z_{t-1}), x)$$ 通过实验比较，作者最终将 $Softmax$ 作为 $O_1$ 操作，而将 $Straight-Through$ 作为 $O_2$ 操作。 Results 通过实验，作者得出一下结论： 能量函数加上推理网络始终优于使用蒸馏数据集的 NAT 模型； 10 次迭代后，ENGINE 在 WMT16 Ro-En 数据集上优于 CMLM； 1 次迭代的情况下，ENGINE 优于很多其他的迭代式 NAT 模型。","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation","slug":"Rejuvenating-Low-Frequency-Words-Making-the-Most-of-Parallel-Data-in-Non-Autoregressive-Translation","date":"2021-12-05T06:13:23.000Z","updated":"2021-12-05T06:14:44.980Z","comments":true,"path":"2021/12/05/Rejuvenating-Low-Frequency-Words-Making-the-Most-of-Parallel-Data-in-Non-Autoregressive-Translation/","link":"","permalink":"http://example.com/2021/12/05/Rejuvenating-Low-Frequency-Words-Making-the-Most-of-Parallel-Data-in-Non-Autoregressive-Translation/","excerpt":"虽然，数据蒸馏(Knowledge Distillation)被广泛地用于构造合成数据来训练 NAT 模型，但是蒸馏数据集和原始数据集在低频词上存在着不一致，进而导致了 NAT 模型预测低频词时会出现更多的错误。 换言之，知识蒸馏可能会丢失原始数据中的某种重要信息，从而导致了预测低频词时出现更多错误。针对此，Ding 等人提出的方法严重依赖于外部资源和人工先验知识，这就限制了其应用的范围。","text":"虽然，数据蒸馏(Knowledge Distillation)被广泛地用于构造合成数据来训练 NAT 模型，但是蒸馏数据集和原始数据集在低频词上存在着不一致，进而导致了 NAT 模型预测低频词时会出现更多的错误。 换言之，知识蒸馏可能会丢失原始数据中的某种重要信息，从而导致了预测低频词时出现更多错误。针对此，Ding 等人提出的方法严重依赖于外部资源和人工先验知识，这就限制了其应用的范围。 作者利用预训练将原始数据直接暴露给 NAT 模型而不需要大量修改模型体系结构。此外，作者还根据两个对齐方向来分析蒸馏数据中的双向连接(对齐关系)，进而发现知识蒸馏使得源低频词词更确定地对齐到目标词，而目标低频词则难以对齐源端词语。作者将其归结于知识蒸馏所造成的信息缺失，并基于上述问题提出了逆知识蒸馏(reverse KD)来召回目标低频词的对齐，之后又将两种蒸馏数据拼接起来以同时让模型拥有确定性知识和低频词信息。为了充分利用原始数据和合成数据，作者最终将原始预训练、双语蒸馏训练、知识蒸馏微调这三个互补的方法结合起来以进一步提升 NAT 模型的性能表现。 Pretraining with Raw Data $\\overrightarrow{KD}$ 能够减少训练数据上的模式从而减低内在不确定性和学习难度，这就使得 NAT 模型学习起来更加简单； $\\overrightarrow{KD}$ 加剧了训练数据中高频词和低频词间的不平衡并且还会导致某种重要信息的缺失。Ding 等人也揭示了 $\\overrightarrow{KD}$ 的副作用：蒸馏数据集引起了 NAT 模型在低频词上的词汇选择错误。 预训练可以迁移知识和数据分布从而提升模型的鲁棒性，作者希望能够将损失信息的分布(低频词)迁移到翻译模型中去，因此提出了如下的预训练方法： raw data + pretraining 在原始数据集上预训练 NAT 模型，这是因为原始数据能够保持数据的原始分布，尤其是针对那些低频词。尽管对于 NAT 来说是困难的，但是模型通过预训练能够获得真实数据的通用知识，这或许将有助于模型更好更快地去学习后续任务。作者选择在模型获得了原始数据最佳 BLEU 得分的 90% 时就早停预训练。 distill data + continuous training 在蒸馏数据集上持续训练 NAT 模型，这是为了保持低模(low-modes)的优势。 Bidirectional Distillation Training$\\overrightarrow{KD}$ 通过将高频词代替低频词简化了训练数据，这虽然促成了更简单的源端到目标端对齐，从而导致了双语覆盖率高。但作者认为 $\\overrightarrow{KD}$ 使得目标低频词没什么机会去对齐到源单词。为了验证上述想法，他们提出了一个定量分析双向双语连接的方法。 实验结果表明：1. 在 source 到 target 方向上，$\\overrightarrow{KD}$ 数据召回了更多 LFW 对齐关系，并且其对齐准确率较高。因此从双语对齐来看，$\\overrightarrow{KD}$ 对于 NAT 模型是有效的。2. 在 target 到 source 方向上，$\\overrightarrow{KD}$ 数据召回了更少的 LFW 对齐关系，而且对齐准确率较低。因此 $\\overrightarrow{KD}$ 会因目标低频词的缺失而损害 NAT 模型。 基于上述的实验结果，使用逆知识蒸馏是一个自然而然的想法；因此，作者提出在双语蒸馏数据集(将两种不同方向的蒸馏数据集拼接起来)上训练 NAT 模型。 Knowledge Distillation $\\overrightarrow{KD}$ 同之前的方法一样，将 AT 模型翻译的目标句作为 ground true。 Reverse Knowledge Distillation $\\overrightarrow{KD}$ 使用一个反向 AT 教师模型来根据目标句子翻译出源句，并将其取代原始的源句。 具体流程如下： 同时用两个 AT 模型来生成正逆蒸馏数据集 拼接这两个数据集为一个蒸馏数据集以互补 在新的蒸馏数据集上训练 NAT 模型 通过上述方法，作者希望：蒸馏数据能够保持低模优势的同时；双语蒸馏能够找回更多的 LFW 对齐连接并且伴随着更高质量的对齐，从而为模型带来总体的性能提升。 Low-Frequency Rejuvenation (LFR)作者将上述的两种方法结合起来以进一步提升模型性能，又考虑到预训练效果和干净微调，故而将结合的流水线作为最佳训练策略，即 **$Raw \\Rightarrow \\overrightarrow{KD} + \\overleftarrow{KD} \\Rightarrow \\overrightarrow{KD}$**。 Results 作者通过实验得出的结论大概有： 显著且普遍地提升了 NAT 模型的翻译性能 该性能提升主要依赖于低频词翻译准确率的提高 在不同语系的数据集上，依旧大大地提升了 NAT 模型的性能 在跨领域测试时，显示出了 LFR 方法能够增强 NAT 模型的领域鲁棒性 在不同规模的数据集上，始终提升 NAT 模型性能的同时，在大规模数据集上显示出了更好的性能 能够与 Ding et al. 的数据级方法互补","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"Context-Aware Cross-Attention for Non-Autoregressive Translation","slug":"Context-Aware-Cross-Attention-for-Non-Autoregressive-Translation","date":"2021-12-01T14:38:43.000Z","updated":"2021-12-01T14:40:15.007Z","comments":true,"path":"2021/12/01/Context-Aware-Cross-Attention-for-Non-Autoregressive-Translation/","link":"","permalink":"http://example.com/2021/12/01/Context-Aware-Cross-Attention-for-Non-Autoregressive-Translation/","excerpt":"因为 NAT 模型缺失了对目标序列中依赖关系建模的能力，所以其十分依赖于交叉注意力网络。但作者发现 NAT 模型的交叉注意力网络存在着一种局部认知缺陷(localness perception problem)，即该交叉注意力网络难以充分地捕捉到源句上下文中的依赖信息。","text":"因为 NAT 模型缺失了对目标序列中依赖关系建模的能力，所以其十分依赖于交叉注意力网络。但作者发现 NAT 模型的交叉注意力网络存在着一种局部认知缺陷(localness perception problem)，即该交叉注意力网络难以充分地捕捉到源句上下文中的依赖信息。 Localness Perception Problem作者分析得到：缺失自回归分解的缘故，NAT 解码器难以充分地捕捉到源端上下文信息。而 Li 等人则发现：相较于 AT 模型，NAT 模型的交叉注意力分布更加的模糊。这两个发现不谋而合，于是作者构造了实验以证明上述的发现。简言之，$LE$ 表示语料级局部交叉熵损失(locality entropy)，该值越高则表明注意力分布越分散；反之则表示注意力分布越集中。该实验结果如下，不难发现： 模型性能和 $LE$ 呈现负相关； NAT 模型的 $LE$ 明显高于 AT 模型，而作者提出的方法的确能够缓解这种问题； 作者的方法不能将 NAT 模型的 $LE$ 拉低到与 AT 模型接近的水平。 CCAN为了缓解 NAT 的局部认知缺陷，作者提出了一种结合局部和全局交叉注意力的方法。下图为一个例子： Global Cross-Attention 不难发现，原始 NAT 或者 AT 的交叉注意力就是全局的，也就是说解码器端的查询 Query 能够关注到源端的所有位置 如上图则表示，”socializing” 能够关注到 【”弗兰克” “在” “跟” “一个” “女孩” “交往”】。 Local Cross-Attention 在源端对齐的单词 $s_j$ 周围构造一个窗口，那么该窗口就是局部注意力能够关注到的范围。而对齐的话，作者将原始注意力权重最高的单词作为对齐单词，这样就无需引入外部对齐信息。 在此之上，作者进一步提出了一个插值门机制以将全局注意力和局部注意力结合起来。通过一个权重 $g = \\sigma (W Q_i)$ 来控制全局和局部的比例。 值得注意的是，唯一引入的额外参数只有上述的 $W$，它会基于解码器端查询 Query 来估计全局交叉注意力的重要性，从而将两个注意力结合起来。作者让每个注意力头都共享这个参数，这样就又能缩减模型参数量了。 如上图则表示，”dating” 只能关注到 【”一个” “女孩” “交往”】。 Results 作者通过实验得到了如下结论： CCAN 能够有效地提升模型性能； CCAN 通过局部和全局注意力更好地利用了源端上下文信息； CCAN 强化了 NAT 模型学习句法和语义信息的能力。","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"LAT-NAT","slug":"LAT-NAT","date":"2021-12-01T05:16:03.000Z","updated":"2021-12-01T05:17:29.252Z","comments":true,"path":"2021/12/01/LAT-NAT/","link":"","permalink":"http://example.com/2021/12/01/LAT-NAT/","excerpt":"NAT 模型能够完全并行化，但是其翻译质量却下跌地厉害，为此之前的学者提出了迭代式解码策略来提高模型性能。本文作者通过将局部自回归翻译(local autoregressive translation)机制融入到非自回归翻译模型中去从而实现了目标句子中局部依赖关系的捕捉能力，因而提高了 NAT 模型的性能。","text":"NAT 模型能够完全并行化，但是其翻译质量却下跌地厉害，为此之前的学者提出了迭代式解码策略来提高模型性能。本文作者通过将局部自回归翻译(local autoregressive translation)机制融入到非自回归翻译模型中去从而实现了目标句子中局部依赖关系的捕捉能力，因而提高了 NAT 模型的性能。 **对于每一个解码位置 $i$ 而言，作者不再预测单个 token $t_i$，而是通过局部翻译器来翻译出长度为 $K$ 的翻译片段(translation piece)**，如下图所示(K=3)： 不难发现，这样就会导致后续位置上出现多个预测 tokens 的情况。如上图，$i=2$ 位置上的预测 tokens 包括了 ${like, like}$，而 $i=3$ 的位置上预测 tokens 包括了 ${birds, birds, birds}$。作者正是利用这一特性，采取了一种对齐归并算法来将所有的翻译片段归并成为完全的翻译输出。简言之，归并算法通过逐步地对齐和归并邻接翻译片段来构造输出结果，最终的结果则是由归并算法动态地决定，这也就意味着模型对于翻译长度具有更高的灵活性。作者将这种局部自回归翻译机制搭建在 CMLM 模型之上，并且也采用了迭代式解码策略来提升模型性能。 Model在 CMLM 解码器输出之上，作者搭建了一个轻量级的 LSTM 顺序解码器作为局部翻译器(local translator)来自回归地生成翻译片段。具体地，对于某个目标端位置 $i$，CMLM 解码器会生成一个隐藏表示 $pos_i$，随后局部翻译器基于该表示自回归地预测出一个翻译片段 $[t_i^1, t_i^2, \\dots, t_i^K]$。此处的 $K$ 是一个超参数以控制翻译片段的长度，而出于推理速度的考虑，作者将其设置为 3。 推理期间，特殊标记 被喂给局部翻译器作为自回归翻译的初始标记，当所有翻译片段生成结束后，作者再使用归并算法将其归并成一个完全的翻译输出。上文说到本模型也采取了迭代式解码，而作者完全遵循 Mask-Predict 的迭代方式。但除此之外，作者将特殊标记 喂给编码器以预测目标端的长度 $N$，但值得注意的是归并算法能够动态地规划长度，因此该模型对长度并没有那么敏感。 模型训练则主要分为如下几步： 从 $[1, N]$ 上的均匀分布中采样 mask 的数量； 随机地选择翻译单词进行 mask； 对于每个目标位置，作者都采用 teacher-forcing styled 训练方法来手机预测翻译片段的交叉熵损失 (作者简单地通过顺延方法来构造翻译片段的 ground true)。 最终作者将所有位置都考虑在损失内，只不过那些 unmasked 位置上的损失权重较小，这样模型还是能够主要处理 masked 单词预测。其 token 预测损失如下 (其中，unmasked 单词的损失权重 $\\alpha$ 被设置为了 0.1)：$$\\Large \\mathcal{L} = - \\sum_{i=1}^N \\sum_{j=1}^K \\mathbb{1} \\big{ t_i^j \\in T_{mask} \\big} \\log \\big( p(t_i^j) \\big) - \\sum_{i=1}^N \\sum_{j=1}^K \\mathbb{1} \\big{ t_i^j \\not\\in T_{mask} \\big} \\alpha \\log \\big( p(t_i^j) \\big)$$ 除此之外，作者还通过随机地删除输入序列中的某些位置来使模型学习如何进行插入式操作。模型训练的最终损失为 token 预测损失和 length 预测损失之和：$$\\large \\mathcal{L}{model} = \\mathcal{L}{tokens} + \\mathcal{L}_{length}$$ Merging Algorithm如上所述，作者采用一个归并算法来逐段渐进地构造输出。该算法基于如下假设： 如果局部翻译器是 well-trained 的话，那么 其自回归翻译片段也应该是 well-translated 并且具备流畅性； 各个翻译片段之间存在着重叠的单词，而这些词可以作为归并时的对齐点。 首先，通过下图的例子来说明归并算法是如何归并两个邻接翻译片段 $s1$ 和 $s2$ 的： 使用最长共同子序列(LCS)算法找出两个翻译片段之间的对齐点(如上假设，所找到的最长共同子序列应该承担着作为归并对齐点的任务)，也就是上图中的 study； 如果没有匹配到对齐点，那么就简单地将两个翻译片段拼接起来； 否则，找到的对齐点至多划分出两个冲突段中段，即上图的 going to &amp; will 和 here &amp; in the； 比较两个段中段的置信分，将高者作为冲突结果复制到归并翻译中去，即上图中的 going to 和 in the； 当处理完所有的冲突段中段后，归并翻译就构造完成了。 上述过程中，冲突段中段的置信分该如何定义呢？作者将每个预测 token 的对数概率作为其模型得分，然后简单地将段中段内所有 tokens 得分的均值作为段中段的置信分。还有一个问题，那就是如果冲突段中段的一个段中段为空时，其置信分是否直接为0呢，作者将该值设置为一个恒定值 $\\log 0.25$，因为如此就能让模型(归并算法)学习到删除式操作了。 对于所有的翻译片段，作者自左至右地逐段归并翻译片段；而在每个归并操作中，作者也只对 $2K$ 个 tokens 进行归并($s1$ 有 $K$ 个，$s2$ 也有 $K$ 个)，这样能够保证归并的局部性从而减轻错误对齐点的负面影响。 这样的归并算法是能够直接应用在迭代解码过程中的，但因为归并算法可以动态地调节翻译长度，所以作者进一步采用了一种中间迭代长度调整策略来缓解上述问题。简言之，通过增加或删除 标记的数量就可以保证归并后的翻译长度不会偏离预测长度太多。 值得注意的是，归并算法整体是类似于自回归的方式，但其并不涉及神经计算，所以并不会带来很多速度损耗，但该算法的动态长度特性(选择空段中段，即删除；选择更长的段中段，即插入)还能使得模型输出更具灵活性。 Results作者在 WMT14 En-De、WMT16 En-Ro 以及 IWSLT14 De-En 数据集上进行了实验，其主要结果如下图所示： 作者得出的结论有： LAT-NAT 模型能够显著地提升 CMLM 基线模型的性能； LAT-NAT 模型能够显著地减少 CMLM 基线模型的迭代； LAT-NAT 模型能够更好的泛化到更长的句子上去。","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"RewriteNAT","slug":"RewriteNAT","date":"2021-11-30T08:16:23.000Z","updated":"2021-11-30T08:18:14.155Z","comments":true,"path":"2021/11/30/RewriteNAT/","link":"","permalink":"http://example.com/2021/11/30/RewriteNAT/","excerpt":"NAT 模型因其性能的衰减而无法与 AT 模型同日而语，CMLM 提出了一种 conditional mask language model 来缓解 NAT 模型的性能衰减问题。实际上，CMLM 将 Heuristic Mask 和 Iterative Predict 相结合以实现模型性能的提升，但是这其中最大的问题就在于所谓的启发式规则是否真的能够有效地定位到错误单词。简言之，如果迭代时纠正的错误单词并不错误，那么整个模型的性能不仅无法提高而且会造成推理速度的衰减。而之前所提出的一些迭代式 NAT 模型已经暴露出了其识别错误单词能力不足的问题，因此作者设计了 RewriteNAT 模型以克服这个问题。","text":"NAT 模型因其性能的衰减而无法与 AT 模型同日而语，CMLM 提出了一种 conditional mask language model 来缓解 NAT 模型的性能衰减问题。实际上，CMLM 将 Heuristic Mask 和 Iterative Predict 相结合以实现模型性能的提升，但是这其中最大的问题就在于所谓的启发式规则是否真的能够有效地定位到错误单词。简言之，如果迭代时纠正的错误单词并不错误，那么整个模型的性能不仅无法提高而且会造成推理速度的衰减。而之前所提出的一些迭代式 NAT 模型已经暴露出了其识别错误单词能力不足的问题，因此作者设计了 RewriteNAT 模型以克服这个问题。 对标 CMLM 模型首先，作者列举了 CMLM 模型的三宗罪： 识别错误单词的能力不足：CMLM 模型可能会保留错误单词而重写正确单词； 迭代造成了推理速度下降：在上述条件下的迭代会损害 NAT 与生俱来的高推理速度而没有什么性能提升； 训练与推理之间的不一致：训练时的目标端能够偷看到很大一部分 ground true 单词，而推理时却不能。 基于上述的缺点，作者采用了以下措施来进一步改进 NAT 模型，从而提出了 RewriteNAT 模型： 使用 revisor 模块和 locator 模块来代替 CMLM 的 Mask-Predict 策略； 使用动态止停策略来动态地结束迭代而不是永远恒定迭代次数； 使用 ground true 来监督上述的 revisor 模块和 locator 模块。 我们可以先对比一下 RewriteNAT 模型与 CMLM 模型在大体结构上的差异，如下图： 不难发现，主要的区别就是启发式规则被一个 locator 模块给代替了，而这个 locator 似乎能够更准确地捕捉到错误单词。在两个模型的右下角，我们又可以知道 locator 模块会基于编码器输出，或者是源句上下文来进行判断，这在直觉上是要比启发式规则强很多。 RewriteNATRewriteNAT 模型总体上由三部分组成： encoder：编码器和其他的 MT 模型编码器一样，负责将输入序列编码成上下文化的隐藏表示； revisor：重写器则类似于 CMLM 的解码器，负责将 masked 单词重新预测以生成正确的单词； locator：定位器则取代了 CMLM 的启发式规则，负责定位出 revisor 已生成的偏翻译中的错误单词。 Revisorrevisor 会基于源句上下文将 masked 单词重写改正；初始时，它会接收一个仅由 [MASK] 组成的输入序列，即重写输入序列上的每一个单词。具体流程如下： 给定前一迭代步 t-1 时 locator 所生成的偏翻译 $Y_{t-1}^r$，首先通过 transformer 块堆栈基于源句表示 $H^e$ 来生成出其对应的隐藏表示 $H_t^r$； 随后，包含着 [MASK] 标记的隐藏表示 $H_t^r$ 会被喂给 revisor classifier $\\pi^l$ 以生成翻译单词，而这些新生成的翻译单词会取代 [MASK] 标记，从而组成一个新的偏翻译 $Y_t^l$； 最后，生成的偏翻译 $Y_t^l$ 会被喂给 locator 模块。 Locatorlocator 会基于源句上下文来辨别整个偏翻译中的错误单词；由于它在 revisor 模块后面，所以它总是接收 revisor 输出的偏翻译即可。对于给定的偏翻译，locator 需要为其中的每一个单词都标注上标签；若该标签为 0 (keep)，那么这个单词在输出的偏翻译中将被保留；若该标签为 1 (revise)，那么这个单词在输出的偏翻译中将会被 [MASK] 标记所取代。其具体流程如下： 给定当前迭代步 t 时 revisor 所生成的偏翻译 $Y_t^l$，首先通过 transformer 块堆栈基于源句表示 $H^e$ 来生成出其对应的隐藏表示 $H^l$； 随后，将该隐藏表示喂给 locator classifier $\\pi^r$ 以生成对应的标注序列，如上将被标注 revise 的单词以 [MASK] 代替，从而组成一个新的偏翻译 $Y_t^r$； 最后，生成的偏翻译 $Y_t^r$ 会喂给 revisor 模块。 下面是一个 RewriteNAT 解码的例子，假设源句输入为 “Thank you .”；目标翻译为 “Vielen Dank .”；迭代次数为 2，则其流程如下图所示： Training训练期间，作者为这两个模块定义了两种不同的监督信号以使其能够更准确地定位并重写错误单词： revisor 的监督信号 $q(\\hat{Y}_t^r)$ $q(\\hat{Y}_t^r)$ 是一个权重向量，它将那些 masked 位置权重置为 1，其余的则置为 0；也就是说该信号旨在优化 revisor 在 masked 单词上的重写损失，其定义如下： $$ \\large q_i(\\hat{Y}t^r) = 1\\ \\ if\\ \\ \\hat{Y}{t_i}^r == [MASK]\\ \\ else\\ \\ 0 $$ locator 的监督信号 $z(\\hat{Y}_t^l)$ $z(\\hat{Y}_t^l)$ 向量则是统计了偏翻译对比 ground true 而言的错误单词位置，也就是说该信号旨在优化 locator 在整个偏翻译序列上的定位损失，其定义如下： $$ \\large z_i(\\hat{Y}t^l) = 1\\ \\ if\\ \\ \\hat{Y}{t_i}^l \\ne Y_i\\ \\ else\\ \\ 0 $$ 作者通过上述的两个监督信号来指导 revisor 和 locator 更加精确地去学习如何重写和定位错误单词，从而进一步提升模型性能，具体的训练目标函数如下：$$\\Large \\mathcal{L}(\\theta) = \\sum_{m=1}^M \\Big{ q(\\hat{Y}t^r) \\log \\pi{\\theta}^l (Y \\mid \\hat{Y}t^r, X) + \\log \\pi{\\theta}^r (z(\\hat{Y}_t^l) \\mid \\hat{Y}_t^l, X) \\Big}$$ Inference在训练期间，RewriteNAT 必须翻译出和 ground true 一样长的偏翻译才能够比较得出上述的两个监督信号；但在推理期间，RewriteNAT 无法预先得知目标序列的长度。因此，作者选择使用一个长度分类器来预测目标端长度，而且还能够实现长度束搜索解码。 除此之外，作者采用动态止停技术来使得 RewriteNAT 模型具备动态迭代能力从而缓解迭代损害速度问题。具体来说，当 locator 为整个偏翻译都标注上 keep 或者 revisor 无法再生成新单词时，迭代解码结束。当然，为了避免模型无休止地迭代，作者还设定了一个迭代上限。 Results作者在 WMT14 En-De 和 WMT16 En-Ro 数据集上的实验结果如下所示： 简单来说，作者得到了以下的模型优点： 模型取得了 SOTA 的 BLEU 得分； 模型大大降低了解码时迭代的次数； 模型对于长句子的泛化能力更强。","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"MHPLSTM","slug":"MHPLSTM","date":"2021-11-29T14:00:57.000Z","updated":"2021-11-29T14:03:36.353Z","comments":true,"path":"2021/11/29/MHPLSTM/","link":"","permalink":"http://example.com/2021/11/29/MHPLSTM/","excerpt":"自注意力网络因其序列级并行化能力而风靡，但值得注意的是 SAN 的计算复杂度是 $O(n^2)$，而 LSTM 的计算复杂度则只有 $O(n)$。而之所以 LSTM 远慢于 SAN 是因为每一时刻 LSTM 的状态都依赖于前一时刻的输出，从而使得其不得不顺序地进行 $n$ 次的计算 (假设输入序列长度为 $n$)。而在这些顺序计算中，LSTM 的 gate 计算和 state 计算是通过矩阵乘法实现的线性变换，其他则是由一些元素级运算实现，因此 LSTM 的 gate 和 state 计算是造成其速度过慢的主要矛盾。","text":"自注意力网络因其序列级并行化能力而风靡，但值得注意的是 SAN 的计算复杂度是 $O(n^2)$，而 LSTM 的计算复杂度则只有 $O(n)$。而之所以 LSTM 远慢于 SAN 是因为每一时刻 LSTM 的状态都依赖于前一时刻的输出，从而使得其不得不顺序地进行 $n$ 次的计算 (假设输入序列长度为 $n$)。而在这些顺序计算中，LSTM 的 gate 计算和 state 计算是通过矩阵乘法实现的线性变换，其他则是由一些元素级运算实现，因此 LSTM 的 gate 和 state 计算是造成其速度过慢的主要矛盾。 不难看出顺序的线性变换计算问题是问题的关键，因此为了使得 LSTM 能够获得序列级并行化能力，作者通过使用当前输入的嵌入表示和之前输入的词袋表示来计算 gate 和 state 以近似完全的 LSTM 上下文建模。这使得 LSTM 能够并行地计算每一个输入步，从而避免了笨拙的顺序线性变换问题，该模型被作者称为 Highly Parallelized LSTM (简称 HPLSTM)。除此之外，作者类似于 Transformer 中的多头注意力网络那样将整个 HPLSTM 进一步分解成多个更小的 HPLSTM，从而实现进一步的并行化能力以及模型参数量的有效缩减。 LN-LSTM首先，回顾一下层归一化增强的 LSTM 模型的工作流程，如下图所示： 首先，需要强调的是 LN-LSTM 在解码时获得了比 Transformer 更好的性能表现，而本文的模型就是基于 LN-LSTM 而设计的。LN-LSTM 的处理流程大概如下： 将前一时刻的输出表示 $o^{t-1}$ 和当前时刻的输入表示 $i^t$ 拼接成当前输入 $v^t$：$$\\large v^t = i^t \\mid o^{t-1}$$ 通过当前输入数据 $v^t$ 来计算输入门 $i_g^t$、输出门 $o_g^t$、遗忘门 $f_g^t$ 以及当前隐藏表示 $h^t$：$$\\large i_g^t = \\sigma(LayerNorm(W_i \\cdot v^t + b_i))$$$$\\large o_g^t = \\sigma(LayerNorm(W_o \\cdot v^t + b_o))$$$$\\large f_g^t = \\sigma(LayerNorm(W_f \\cdot v^t + b_f))$$$$\\large h^t = \\alpha(LayerNorm(W_h \\cdot v^t + b_h))$$ 通过输入门 $i_g^t$ 和遗忘门 $f_g^t$ 以及当前隐藏表示 $h^t$ 来更新记忆单元 $cell$ 中的内容：$$\\large c^t = c^{t-1} * f_g^t + h^t * i_g^t$$ 最后通过输出门 $o_g^t$ 和当前记忆单元内容 $c^t$ 来得到当前时刻的输出表示 $o^t$：$$\\large o^t = c^t * o_g^t$$ HPLSTM 使用累加运算(cumulative sum operation)求出**输入序列的词袋表示(bag-of-words)并令 $s^1 = [0, \\dots, 0] $**：$$\\large s^t = \\sum_{k=1}^{t-1} i^k$$ **对词袋表示进行层归一化以防止出现梯度爆炸问题，然后将其代替上述的 $o^{t-1}$ 与输入表示 $i^t$ 拼接起来获得 $v^t$**：$$\\large v^t = i \\mid LayerNorm(s^t)$$ 使用当前输入数据 $v^t$ 来计算出输入门 $i_g^t$、遗忘门 $f_g^t$：$$\\large i_g^t = \\sigma(LayerNorm(W_i \\cdot v^t + b_i))$$$$\\large f_g^t = \\sigma(LayerNorm(W_f \\cdot v^t + b_f))$$ 使用两层前馈网络来计算隐藏状态 $h^t$ 以缓解词袋表示的权重均等问题：$$\\large h^t = W_{h2} \\alpha(LayerNorm(W_{h1} \\cdot v^t + b_{h1})) + b_{h2}$$ 使用输入门 $i_g^t$ 和遗忘门 $f_g^t$ 以及隐藏表示 $h^t$ 来更新记忆单元内容：$$\\large c^t = c^{t-1} * f_g^t + h^t * i_g^t$$ **由于词袋表示远远不如当前记忆单元中所存储的隐藏表示，因此将记忆表示 $c^t$ 和输入表示 $i$ 拼接以获得输出门 $o_g^t$**：$$\\large o_g^t = \\sigma(LayerNorm(W_o \\cdot (i^t \\mid c^t) + b_o))$$ 最后输出门 $o_g^t$ 控制当前位置 t 上的输出表示 $o^t$：$$\\large o^t = c^t * o_g^t$$ 此过程中，虽然词袋表示 $s$ 不如原始 LSTM 中所依赖的 $o$，但是它使得模型获得了序列级并行化能力，而且上述过程中采用了两个手段来缓解词袋表示所带来的性能影响。此外，除了 $cell$ 的计算是顺序的，其他都是序列级并行化处理，而 $cell$ 的计算是由元素级运算实现的，因此 HPLSTM 模型的训练和解码速度可以得到大大提升。 Multi-Head HPLSTM上述的方法虽然带来的并行化能力，但是也使得 HPLSTM 模型涉及更多的参数量和计算量，为了限制模型的参数量，作者效仿 Multi-Head 将整个 HPLSTM 模型分成 $n$ 个更小的 HPLSTM 模型来并行地进行计算。 通过线性变换将输入数据 $i$ 映射到 $n$ 个不同的嵌入空间中去，并将变换后的输入分割成 $n$ 个段：$$\\large i_1 \\mid i_2 \\mid \\dots \\mid i_n = W_s \\cdot i + b_s$$ 将第 k 段的表示 $i_k$ 喂给对应的 HPLSTM 子网络，并经过网络处理后得到对应输出 $o_k$：$$\\large o_k = HPLSTM_k (i_k)$$ 将第 1 段到第 k 段的输出结果拼接起来后，对其再施加一次线性变换：$$\\large o = W_m \\cdot (o_1 \\mid o_2 \\mid \\dots \\mid o_n) + b_m$$ ExperimentsMHPLSTM 解码器模型不仅显著地提高了 BLEU 得分，而且大大加快了训练和解码速度。 除此之外，作者还发现： MHPLSTM 并不适合承担 encoder 这一角色； MHPLSTM 的长程建模能力远不如 Self-Attention (当长度大于 15 后)； MHPLSTM 解码器中的 FFN 层也起着一定的作用，不能去除。","categories":[{"name":"Neural Machine Translation","slug":"Neural-Machine-Translation","permalink":"http://example.com/categories/Neural-Machine-Translation/"}],"tags":[{"name":"NMT","slug":"NMT","permalink":"http://example.com/tags/NMT/"}]},{"title":"DSLP","slug":"DSLP","date":"2021-11-27T12:14:39.000Z","updated":"2021-11-27T13:03:24.841Z","comments":true,"path":"2021/11/27/DSLP/","link":"","permalink":"http://example.com/2021/11/27/DSLP/","excerpt":"之前拜读了 “Non-Autoregressive Translation with Layer-Wise Prediction and Deep Supervision” 这篇论文，最近发现它开源了代码，因此进行了如下实践。","text":"之前拜读了 “Non-Autoregressive Translation with Layer-Wise Prediction and Deep Supervision” 这篇论文，最近发现它开源了代码，因此进行了如下实践。 过程源码 可以直接在 DSLP 仓库打包下载 也可以使用 wget 下载 环境首先，进入 DSLP 文件目录下，然后依次执行如下指令： 123456pip install -e .pip install tensorflow tensorboard sacremoses nltk Ninja omegaconfpip install &#x27;fuzzywuzzy[speedup]&#x27;pip install hydra-core==1.0.6pip install sacrebleu==1.5.1pip install git+https://github.com/dugu9sword/lunanlp.git 然后，就需要配置 ctcdecode 了，如果条件允许的话，直接运行如下代码： 12git clone --recursive https://github.com/parlance/ctcdecode.gitcd ctcdecode &amp;&amp; pip install . 如果你出现如下问题，那么就算网再好，也不可能安装好的： 1234567891011121314151617181920212223242526Traceback (most recent call last): File &quot;setup.py&quot;, line 26, in &lt;module&gt; download_extract( File &quot;setup.py&quot;, line 16, in download_extract urllib.request.urlretrieve(url, dl_path) File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/urllib/request.py&quot;, line 247, in urlretrieve with contextlib.closing(urlopen(url, data)) as fp: File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/urllib/request.py&quot;, line 222, in urlopen return opener.open(url, data, timeout) File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/urllib/request.py&quot;, line 525, in open response = self._open(req, data) File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/urllib/request.py&quot;, line 542, in _open result = self._call_chain(self.handle_open, protocol, protocol + File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/urllib/request.py&quot;, line 502, in _call_chain result = func(*args) File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/urllib/request.py&quot;, line 1397, in https_open return self.do_open(http.client.HTTPSConnection, req, File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/urllib/request.py&quot;, line 1358, in do_open r = h.getresponse() File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/http/client.py&quot;, line 1348, in getresponse response.begin() File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/http/client.py&quot;, line 316, in begin version, status, reason = self._read_status() File &quot;/home/wbxu/anaconda3/envs/dslp/lib/python3.8/http/client.py&quot;, line 285, in _read_status raise RemoteDisconnected(&quot;Remote end closed connection without&quot;http.client.RemoteDisconnected: Remote end closed connection without response 请按照如下步骤进行： 打开 google colab 并创建一个新的 notebook，运行如下代码： 12git clone --recursive https://github.com/parlance/ctcdecode.gitcd ctcdecode &amp;&amp; pip install . 然后按照它输出的提示信息，找到如下一个目录和两个文件： 123ctcdecode/~/third_party/openfst-1.6.7.tar.gz~/third_party/boost_1_67_0.tar.gz 将这两个文件添加到 ctcdecode/third_party 目录下，然后下载整个 ctcdecode 目录： 1ThreadPool kenlm utf8 openfst-1.6.7.tar.gz boost_1_67_0.tar.gz 将 ctcdecode 上传至 DSLP 目录下，运行如下指令： 12cd ctcdecodepip install . 数据fairseq 提供了蒸馏数据集，我们直接下载即可 运行 Preprocess 123456789TEXT=wmt14_ende_distillpython3 fairseq_cli/preprocess.py --source-lang en --target-lang de \\ --trainpref $TEXT/train.en-de \\ --validpref $TEXT/valid.en-de \\ --testpref $TEXT/test.en-de \\ --destdir data-bin/wmt14.en-de_kd \\ --workers 40 \\ --joined-dictionary Run 123456789101112131415161718192021222324252627282930313233343536373839404142434445python3 train.py data-bin/wmt14.en-de_kd --source-lang en --target-lang de \\ --save-dir checkpoints --eval-tokenized-bleu \\ --keep-interval-updates 5 --save-interval-updates 500 \\ --validate-interval-updates 500 \\ --maximize-best-checkpoint-metric \\ --eval-bleu-remove-bpe \\ --eval-bleu-print-samples \\ --best-checkpoint-metric bleu \\ --log-format simple \\ --log-interval 100 \\ --eval-bleu \\ --eval-bleu-detok space \\ --keep-last-epochs 5 \\ --keep-best-checkpoints 5 \\ --fixed-validation-seed 7 \\ --ddp-backend=no_c10d \\ --share-all-embeddings \\ --decoder-learned-pos \\ --encoder-learned-pos \\ --optimizer adam \\ --adam-betas &quot;(0.9,0.98)&quot; \\ --lr 0.0005 \\ --lr-scheduler inverse_sqrt \\ --stop-min-lr 1e-09 \\ --warmup-updates 10000 \\ --warmup-init-lr 1e-07 \\ --apply-bert-init \\ --weight-decay 0.01 \\ --fp16 \\ --clip-norm 2.0 \\ --max-update 300000 \\ --task translation_glat \\ --criterion glat_loss \\ --arch glat_sd \\ --noise full_mask \\ --concat-yhat \\ --concat-dropout 0.0 \\ --label-smoothing 0.1 \\ --activation-fn gelu \\ --dropout 0.1 \\ --max-tokens 8192 \\ --glat-mode glat \\ --length-loss-factor 0.1 \\ --pred-length-offset Evaluation 12345678910fairseq-generate data-bin/wmt14.en-de_kd --path checkpoints \\ --gen-subset test \\ --task translation_lev \\ --iter-decode-max-iter 0 \\ --iter-decode-eos-penalty 0 \\ --beam 1 \\ --remove-bpe \\ --print-step \\ --batch-size 100","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"Connectionist Temporal Classification","slug":"Connectionist-Temporal-Classification","date":"2021-11-26T07:21:26.000Z","updated":"2021-11-26T07:24:49.772Z","comments":true,"path":"2021/11/26/Connectionist-Temporal-Classification/","link":"","permalink":"http://example.com/2021/11/26/Connectionist-Temporal-Classification/","excerpt":"CTC 的提出就是旨在解决输入和输出之间没有先验对齐的情况。 本文来自Sequence Modeling With CTC，非常好的文章，中文是我自己翻译，如有错误还望赐教！","text":"CTC 的提出就是旨在解决输入和输出之间没有先验对齐的情况。 本文来自Sequence Modeling With CTC，非常好的文章，中文是我自己翻译，如有错误还望赐教！ 正式地来说，输入序列 $X = [x_1, x_2, \\dots , x_T]$ 和输出序列 $Y = [y_1, y_2, \\dots , y_U]$，我希望找到 $X$ 到 $Y$ 之间的映射。而我们所面对的困难有3点： $X$ 和 $Y$ 的长度都是可变的； $X$ 和 $Y$ 的长度间的比例是可变的； 我们并没有 $X$ 和 $Y$ 之间的词级先验对齐。 CTC 算法能够克服上述的几点缺陷。对于给定的输入序列 $X$，CTC 将会为我们提供所有可能 $Y$ 上的一个输出分布。借助这个输出分布，我们可以推理出一个可能的输出 $Y$ 或者评估某个给定输出的概率值。 对于给定的输入 $X$，我们希望训练我们的模型以最大化正确输出 $Y$ 的概率，而为此我们就需要能够有效的计算条件概率 $p(Y|X)$，不仅如此该条件概率必须是可微的从而能够使用梯度下降(gradient descent)算法来进行优化。 模型训练结束之后，我们将会使用该模型基于给定的 $X$ 来推理出一个可能的输出 $Y$，如贪婪解码 $\\hat{Y} = \\underset{Y}{\\operatorname {argmax}} p(Y \\mid X)$。如果我们使用 CTC 则可以获得一个近似解而不付出太大的代价。 AlignmentCTC 算法不需要输入和输出之间的先验对齐，它通过求和所有可能对齐的概率从而计算得到给定输入下一个输出的概率。首先我们举个例子，假设输入序列 $X$ 的长度为 6，而输出序列 $Y = [c, a, t]$，那么一种可能的对齐方式如下 (每个位置都会有一个预测输出，最终通过删除重复还获得 $Y$)，如下图所示： 这种方法有两个弊端： 一般，强迫每个输入单词都必须对齐某个输出是没有道理可言的，如翻译中的虚词； 我们无法产生连续而又正确的输出序列，如 $Y = [h, e, l, l, o]$。 为了克服上述问题，CTC 引入了一个额外的 token 来表示对应位置上没有任何输出 (或对齐空)，这个 token 称为 blank token，此处用 $\\epsilon$ 来表示。于是，CTC 允许任何能够映射到 $Y$ 的对齐，如下： 让我们再把目光放回到 cat 的例子上，我们就会发现能够映射到 $Y$ 的对齐映射关系存在着多种，下图举出了一些例子： 通过上面的例子，我们可以发现 CTC Alignments 具备几个值得注意的属性： $X$ 和 $Y$ 之间可能的对齐关系是单调的，即我们预测下一个方格时，要么就是与之前的方格预测结果相同，要么就是将结果也推进一位； $X$ 和 $Y$ 之间的对齐关系是多对一映射(Many-to-One Map)，这也就使得输入序列的长度必须不小于输出序列，即 $\\mid X \\mid \\ge \\mid Y \\mid$。 Loss FunctionCTC alignments 为我们提供了一种自然的概率计算方法，如下所示： 准确地讲，CTC 对于单个输入输出对 $(X, Y)$ 的目标函数如下：$$\\Large p(Y \\mid X)\\ \\ =\\ \\ \\sum_{A \\in \\mathcal{A}{X, Y}}\\ \\ \\ \\prod{t=1}^{T} p_t(a_t \\mid X)$$ 其中，$p(Y \\mid X)$ 是 CTC 条件概率；$\\sum_{A \\in \\mathcal{A}{X, Y}}$ 是边际化所有合法对齐映射所组成的集合；$\\prod{t=1}^{T} p_t(a_t \\mid X)$ 是逐个时间步地计算单个对齐映射的概率。我们当然可以直接求和所有合法的对齐映射的概率，但问题在于对齐数量会随着输入序列长度的增加而呈指数式增长，因此这种计算所付出的时间代价太大。万幸的是动态规划能够有效地解决这个问题，其关键在于如果存在两个对齐在同一个时间步预测出了同样的输出，那么我们就可以将它们合并起来： ​ $\\Longrightarrow$ 因为 CTC 在对齐映射中引入了 $\\epsilon$，所以我们自然而然地定义序列 $Z = [\\epsilon, y_1, \\epsilon, y_2, \\dots, \\epsilon, y_U, \\epsilon]$ (即在每个输出 label 之间以及整个序列的首尾都插入一个 $\\epsilon$)。接着，我们将 $\\alpha_{s, t}$ 表示时间步 $t$ 时的偏序列 $Z_{1:s}$ 的 CTC 得分，那么要使用动态规划算法，就得构造出状态转移方程，具体可分为如下情况： 预测重复标签 举上图为例 (显然有 $Z_s = Z_{s-2}$)，我们不能从 $Z_{1:s-2}^{t-1} = \\dots a$ 直接转移到 $Z_{1:s}^{t} = \\dots aa$，因为需要被预测出的重复标签 $aa$ 显然会变成单个标签 $a$。在这种情况下，我们只能从 $Z_{s}^{t-1}$ 或 $Z_{s-1}^{t-1}$ 转移而来 (如上图的两个实线箭头)；因此有下式：$$\\Large \\alpha_{s, t} = (\\alpha_{s-1, t-1} + \\alpha_{s, t-1}) \\cdot p_t(z_s \\mid X)$$ 其中，$\\alpha_{s-1, t-1} + \\alpha_{s, t-1}$ 是时刻 $t-1$ 能够转移到正确结果 $Z_s^t$ 的两个合法偏序列的得分和；$p_t(z_s \\mid X)$ 则是当前标签 $Z_s$ 在时刻 $t$ 上的概率。 预测不同标签 如上图 (显然有 $Z_s \\ne Z_{s-2}$ 且 $Z_{s-1} = \\epsilon$)，此时我们可以从 $Z_{s-2}^{t-1}$ 或 $Z_{s-1}^{t-1}$ 或 $Z_{s}^{t-1}$ 转移而来，因此有下式：$$\\Large \\alpha_{s, t} = (\\alpha_{s-2, t-1} + \\alpha_{s-1, t-1} + \\alpha_{s, t-1}) \\cdot p_t(z_s \\mid X)$$ 其中，$\\alpha_{s-2, t-1} + \\alpha_{s-1, t-1} + \\alpha_{s, t-1}$ 是时刻 $t-1$ 能够转移到正确结果 $Z_s^t$ 的三个合法偏序列的得分和；$p_t(z_s \\mid X)$ 则是当前标签 $Z_s$ 在时刻 $t$ 上的概率。 结合上述的两种情况，假定 $Z = [\\epsilon, a, \\epsilon, b, \\epsilon]$，那么动态规划得到的所有合法对齐映射路径 (一个对齐映射在图中表现为一条从起始节点到终止节点的路径) 如下图所示： 从上图中不难发现，起始节点有两个 ($\\epsilon$ 或 $a$)，而终止节点亦有两个 ($b$ 或 $\\epsilon$)，那么整个所有可能对齐映射的概率和就只需要将两个终止节点 ($b$ 或 $\\epsilon$) 的得分 $\\alpha$ 相加即可，即：$$\\Large p(Y \\mid X) = \\alpha_{2U, T} + \\alpha_{2U+1, T}$$ 因为每一步的动态规划都只涉及到乘法和加法，所以我们的损失函数是可微的，进一步来说模型可以通过梯度反向传播算法来训练。当然了，最大化条件概率也就可以说成最小化负的条件概率，因此我们的目标函数 (最小化负对数似然) 如下：$$\\large \\sum_{(X,Y) \\in \\mathcal{D}} - log p(Y \\mid X)$$ Inference当我们训练完模型之后，我们就得拿它来推理结果了，这是就需要解决如下问题：$$\\Large Y^{*} = \\underset{Y} {\\operatorname {argmax}} p(Y \\mid X)$$ 最佳路径解码 采用贪心策略，每个时间步上都选择概率最高的单词作为输出以期使得整个对齐映射概率最高，如下：$$\\Large A^{*} = \\underset{A} {\\operatorname {argmax}} \\prod_{t=1}^{T} p_t(a_t \\mid X)$$ 然后通过 collapse function 就可以根据得到的 $A^{*}$ 来获得输出序列 $Y$。这种贪心策略实际上能够应用且有效的场景少之甚少，它并不能保证我们所获得的 $Y$ 一定是最优的 $Y$。更进一步来说，它完全没有考虑到一个 $Y$ 可能对应着多个 $A$ 这个事实。 举个例子，$p(A_1 = [a, a, a] \\mid X) = 0.32$、$p(A_2 = [a, a, \\epsilon] \\mid X) = 0.18$ 以及 $p(A_3 = [\\epsilon, b, \\epsilon] \\mid X) = 0.42$，那么就要有 $Y = [b]$，因为对齐 $A_3$ 是概率最高的那个。但是我们要知道 $[a, a, a] \\Rightarrow [a]$ 且 $[a, a, \\epsilon] \\Rightarrow [a]$，所以 $p(Y = [a] \\mid X) = 0.32 + 0.18 = 0.5$，故 $Y = [a]$ 才更有可能是正确输出结果。 前缀搜索解码 该算法可以是作为束搜索(beam search)的变体，那么我们首先来看一下标准的束搜索是如何运作的，如下图(束大小为3)： 我们为什么不能使用标准的束搜索呢，正是因为我们的偏序列中存在着重复和 $\\epsilon$ tokens，因此到头来可能计算的只是同一个输出 $Y$ 的不同对齐映射罢了。正是因为如此，我们在每个时间步中都执行 collapse function 来将偏对齐转换成偏输出，这里称之为对应输出的前缀(prefix)，于是我们的“束”不再是“对齐束”而是“前缀束”。具体如下图所示： 从上图详细的流程中我们不难发现如下规律： 当前缀中的最后一个标签与当前标签不同时，我们只需要将正在处理的节点得分并入到前缀得分中去； 当前缀中的最后一个标签与当前标签相同时，我们需要将当前处理节点以及其上面的 $\\epsilon$ 节点一起并入前缀得分中去； 某一个得到前缀并不一定就只能从一个之前的前缀得到，有可能有多条路径都能构成该前缀。 注：CTC 计算的一种实现方法 像是在语音识别领，整个模型还会涉及一个语言模型来提升准确率，为此，我们可以将语言模型视作推理中的一个项：$$\\Large Y^{*} = \\underset{Y} {\\operatorname {argmax}} p(Y \\mid X) \\cdot p(Y)^\\alpha \\cdot L(Y)^\\beta$$ 其中，$L(Y)$ 会计算目标序列 $Y$ 的长度并充当一个词插入红利(word insertion bonus)。语言模型则只会在前缀被扩展时被包括在内，也容易理解，毕竟一个不变的东西再去算一次属实浪费时间了。如若这般，就很容易导致搜索更偏向于较短的前缀，此时上述的词插入红利将会帮助我们减缓这个问题。一般来说，参数 $\\alpha$ 和 $\\beta$ 会通过交叉验证(cross-validation)来设定。 CTC PropertiesConditional Independence它广为人诟病的缺陷就是条件独立性假设，但这一点与非自回归翻译模型却不谋而合。即，模型假设：对于给定的输入，每一个输出都条件独立于其他输出，自不必说，这是一个bad假设，尤其对于 Seq2Seq 问题来说。 例如，给定一个语音输入，其预测的输出序列可以是 $Y = tripple\\ \\ A$ 或 $Y = AAA$。那么，当第一个字符输出为 $A$ 时，下一个字符为 $A$ 的可能性应该因此而提升；当第一个字符输出为 $t$ 时，下一个字符为 $i$ 的概率就应该因此而升高。但是条件独立性假设移除了这种前后向的依赖关系，从而导致上述行为无法发生。 因此，使用 CTC 的模型性能都不高，因为它很难学习到一个像样的语言模型。但条件独立性假设并非全是坏处，它有助于学习到的语言知识能够快速迁移至其他领域。 Alignment PropertiesCTC 算法是无需先验对齐的，它的目标函数会边际化所有可能的对齐。尽管 CTC 对 $X$ 和 $Y$ 之间的对齐映射的排列形式做了强假设，但是它们之间的分布概率对于模型是不可知的。有些任务上，CTC 会给单个对齐排列分配绝大部分的概率，这是不合理的。 正如之前所说的，CTC 的对齐是单调的，这对于语音识别而言是行之有效的，但是对于机器翻译而言是不合理的 (因为一个当前翻译单词很可能就对齐到了源句中相对位置在其之前好几个单词位置的单词)。 CTC 对齐的另一特点就是它们是多对一的对齐关系，且必定是 Many-to-One Map 的对齐。这种映射关系对于一些情况是合理的，但显然对于所有情况一概视之是不合理的。简言之，CTC 不允许一对多映射关系的存在是不合理的。 从上述多对一映射，我们又引出令一问题：输入序列长度一定要大于等于输出序列长度。这会导致其难以处理输入序列长度小于输出序列长度的样本，而这在机器翻译中并非鲜有。即使是通过某种方法拉伸输入序列长度，那么长度预测又成为一个新问题，当长度预测过长时，冗余的计算量又成为一个问题。 CTC ContextHMMsCTC 其实和 HMM 十分相似，接下来我们就来分析一下。假设输入序列 $X$ 其长度为 $T$、输出序列 $Y$ 其长度为 $U$，一种学习 $p(Y \\mid X)$ 的策略就是应用贝叶斯法则：$$\\Large p(Y \\mid X) \\propto p(X \\mid Y) \\cdot p(Y)$$ 其中，$p(Y)$ 是任意的某个语言模型，我们需要关注的是 $p(X \\mid Y)$。如上所述，我们依旧让 $\\mathcal{A}$ 表示 $X$ 和 $Y$ 之间所有合法对齐排列的集合，然后我们边缘化这些对齐则有：$$\\Large p(X \\mid Y) = \\sum_{A \\in \\mathcal{A}} p(X, A \\mid Y)$$ 我们在这儿定义两个假设：(1). 给定前一个状态 $a_{t-1}$，状态 $a_t$ 条件独立于所有历史状态；(2). 给定当前状态 $a_t$，观察结果 $x_t$ 完全条件独立，(如下图所示) 于是我们有如下的标准隐马尔科夫模型：$$\\Large p(X) = \\sum_{A \\in \\mathcal{A}} \\prod_{t=1}^{T}\\ \\ \\ p(x_t \\mid a_t) \\cdot p(a_t \\mid a_{t-1})$$ 其中，$\\sum_{A \\in \\mathcal{A}} \\prod_{t=1}^{T}$ 表示边际化所有可能的对齐映射；$p(x_t \\mid a_t)$ 是输出概率(emission probability)；$p(a_t \\mid a_{t-1})$ 则是翻译概率(translation probability)。 接下来，我们首先假设翻译概率 $p(a_t \\mid a_{t-1})$ 是均匀分布，那么就有：$$\\Large p(X) \\propto \\sum_{A \\in \\mathcal{A}} \\prod_{t=1}^{T}\\ \\ \\ p(x_t \\mid a_t)$$ 比较两式，我们不难发现如下两点不同： 一个是给定 $X$ 去学习 $Y$，另一个则是给定 $Y$ 去学习 $X$； 集合 $\\mathcal{A}$ 的生成方法。 我们应用贝叶斯公式并重写模型：$$\\Large \\begin{aligned}p(X) &amp; \\propto \\sum_{A \\in \\mathcal{A}} \\prod_{t=1}^{T}\\ \\ \\ p(x_t \\mid a_t) \\&amp; \\propto \\sum_{A \\in \\mathcal{A}} \\prod_{t=1}^{T}\\ \\ \\ \\frac{p(a_t \\mid x_t) \\cdot p(x_t)}{p(a_t)} \\&amp; \\propto \\sum_{A \\in \\mathcal{A}} \\prod_{t=1}^{T}\\ \\ \\ \\frac{p(a_t \\mid x_t)}{p(a_t)}\\end{aligned}$$ 此时，我们为状态 $a$ 假设一个先验的均匀分布并使其基于输入序列 $X$ 而不是单个时刻的 $x_t$，则有：$$\\Large p(X) \\propto \\sum_{A \\in \\mathcal{A}} \\prod_{t=1}^{T}\\ \\ \\ p(a_t \\mid X)$$ 如果假设 $\\mathcal{A}$ 是一样的，那么上述式子实质上就是 CTC loss 函数；而实际上 HMM 框架并不会指定 $\\mathcal{A}$ 应该包含什么，它可由具体任务具体分析得到。在许多情况下，模型并不依赖于 $Y$ 并且 $\\mathcal{A}$ 包含了所有长度为 $T$ 的可能输出序列，此时 HMM 可以被表示为如下的遍历图： 而在我们的这个例子中，模型的转移与 $Y$ 是密切相关的，我们可以通过一个简单的线性状态转移图来表示之： CTC 通过 $\\epsilon$ 来增强输出词典，HMM 则允许一个从左至右的转移子集，而 CTC HMM 就会有两个开始状态和两个接受状态。虽然对于每个独立的 $Y$，状态图都是不同的，但是估计观察和转移概率的函数是共享的。CTC 模型能够给出可能对齐的集合从而为一些问题提供好的先验知识。并且 CTC 是辨别的，它直接建模 $p(Y \\mid X)$，而辨别训练能够让我们使用更强大的学习算法。 Enc-Dec Models编码器-解码器是用于序列建模的神经网络通用框架，其中编码器编码输入序列 $X$，而解码器则输出目标序列上的一个分布，我们可以表示成如下式子：$$\\Large p(Y \\mid X) = Decoder(Encoder(X))$$ Encoder CTC 模型的编码器和我们所见过的 Enc-Dec 架构中的编码器并无二样，但其有一个特殊的约束：输入序列长度大于目标序列长度。 Decoder CTC 模型的解码器可以视作一个简单的线性变换再加上一个 Softmax 激活函数。","categories":[{"name":"Speech Recognition","slug":"Speech-Recognition","permalink":"http://example.com/categories/Speech-Recognition/"}],"tags":[{"name":"CTC","slug":"CTC","permalink":"http://example.com/tags/CTC/"}]},{"title":"Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine Translation","slug":"Multi-Task-Learning-with-Shared-Encoder-for-Non-Autoregressive-Machine-Translation","date":"2021-11-25T07:20:39.000Z","updated":"2021-11-25T08:34:17.206Z","comments":true,"path":"2021/11/25/Multi-Task-Learning-with-Shared-Encoder-for-Non-Autoregressive-Machine-Translation/","link":"","permalink":"http://example.com/2021/11/25/Multi-Task-Learning-with-Shared-Encoder-for-Non-Autoregressive-Machine-Translation/","excerpt":"机器翻译具有两大解码范式：自回归解码和非自回归解码。自回归解码方式取得了 SOTA 的性能表现却其推理时延却很高；而非自回归解码方式大大提升了模型推理速度却饱受翻译质量偏低之苦。这正是因为 NAT 解码器中目标句内上下文依赖关系的缺失所致，而之前的学者大都采用知识蒸馏(KD)来实现迁移学习以缓解该问题。","text":"机器翻译具有两大解码范式：自回归解码和非自回归解码。自回归解码方式取得了 SOTA 的性能表现却其推理时延却很高；而非自回归解码方式大大提升了模型推理速度却饱受翻译质量偏低之苦。这正是因为 NAT 解码器中目标句内上下文依赖关系的缺失所致，而之前的学者大都采用知识蒸馏(KD)来实现迁移学习以缓解该问题。 本文中，作者大胆假设“AT 模型的 encoder 和 NAT 模型的 encoder 会捕捉输入句子中不同的语言特征”并通过探究实验(probing tasks)实证之。而基于上述假设，作者提出使用多任务学习并共享编码器来将 AT 模型的知识迁移到 NAT 模型中去从而提升 NAT 模型翻译的质量。确切地说，作者通过采用一个额外的 AT 模型作为辅助并让 AT 与 NAT 共享编码器参数以结合 AT encoder 和 NAT encoder 所能够捕捉到的不同语言特征从而提高 NAT 模型的翻译质量。 在 WMT14 En-De 和 WMT16 En-Ro 数据集上的实验结果表明 Multi-task NAT 相较于基线模型取得了显著的性能提升；在大规模数据集 WMT19 &amp; WMT20 En-De 上的结果也进一步验证了 Multi-task Learning 框架的有效性。除此之外，作者从实验结果中揭示多任务学习是知识蒸馏的互补方法。 1. Probing Tasks探究任务可以定量地衡量模型表示中嵌入的语言知识，作者借助该任务来实证上述的假设。具体的任务如下 SeLen : 预测句子的长度 WC : 在给定句子及其嵌入表示下预测出现的单词 TrDep : 检查编码器表示是否推理出了句子的等级结构 ToCo : 衡量句子节点下紧接着的顶层成分序列 BShif : 预测句子中的连续单词是否被反转 Tense : 预测主句中动词的时态 SubN : 预测主句中主语的个数 ObjN : 预测主句中直接宾语的个数 SoMo : 辨别每个句子的动词或名词是否被更改过 Coln : 辨别每个并列从句是否被修改过 最终的 probing tasks 结果如下图所示： 从上表中不难发现，AT encoder 和 NAT encoder 的确趋向于捕捉源句中不同的语言特征。总体上 AT encoder 更擅长于捕捉源句中的语义特征，而 NAT encoder 则更擅长于捕捉源句中的表面特征。值得一提的是，length prediction 对于 NAT 而言是至关重要的，所以 NAT encoder 在 SeLen 任务上的表现远超 AT encoder。于是，作者就此得出结论：AT encoder 和 NAT encoder 会捕捉源句中的不同语言属性特征，从而为作者的编码器共享结构提供了现实依据。 2. Multi-Task NAT 模型结构 作者采取了硬参数共享方法(hard parameter sharing method)来共享编码器参数，于是整体模型包含了以下三个部分： 共享编码器 (shared encoder) 自回归解码器 (AT decoder) 非自回归解码器 (NAT decoder) 这三个部分的参数会被联合优化(jointly optimize)以最小化多任务损失函数值。 损失函数 Multi-Task NAT 模型的损失函数定义为 AT 模型损失和 NAT 模型损失的加权和，具体如下式：$$\\large \\mathcal{L} = \\lambda_t \\mathcal{L}{nat}(X, Y; \\theta{enc}, \\theta_{dec}^{nat}) + (1-\\lambda_t) \\mathcal{L}{at} (X, Y; \\theta{enc}, \\theta_{dec}^{at})$$ 之前的学者已经表明了任务权重对于多任务学习而言是尤为重要的一环，因此作者提出了一种退火策略以动态地降低 AT loss 的权重。由上式不难发现，$\\lambda_t$ 不是一个常数，而是与当前时间步 t 相关的，如果 $\\lambda_t$ 随着 $t$ 而不断增长，此消彼长，那么 AT loss 的权重就会不断减小。 权重退火 作者将其权重退火成为重要性退火(importance annealing)策略，将 $\\lambda_t$ 定义为一个关于当前时间步 $t$ 和总时间步 $T$ 的线性函数，如下：$$\\large \\lambda_t = \\frac{t}{T}$$ 从上式不难发现有这样的关系成立 $t : 0 \\rightarrow T \\Longrightarrow \\lambda_t : 0 \\rightarrow 1 \\Longrightarrow (1 - \\lambda_t) : 1 \\rightarrow 0$ 。这是因为 AT loss 只是一个辅助项，并不应该直接影响 NAT 模型的实际推理，所以作者希望 AT loss 的权重在训练中不断减小直至归零。 训练推理 训练期间 作者将输入序列 $X$ 输入到共享编码器中，并将输出序列 $Y$ 分别输入到 AT 解码器和 NAT 解码器中去。此时的 $Y$ 既可以是原始数据集的 groundtrue 也可以是蒸馏数据集的 groundtrue。 推理期间 作者在本文中只使用共享编码器和 NAT 解码器来进行模型翻译任务，如此便能保持 NAT 的快速推理优点。 3. 实验结果作者在很多数据集上进行了实验，其结果说明多任务学习与编码器共享框架能够有效地提高 NAT 魔性的翻译质量，如下图所示：","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"Align Tools (Baby dataset)","slug":"Align-Tools-Baby-dataset","date":"2021-11-24T08:06:58.000Z","updated":"2021-11-29T14:07:19.251Z","comments":true,"path":"2021/11/24/Align-Tools-Baby-dataset/","link":"","permalink":"http://example.com/2021/11/24/Align-Tools-Baby-dataset/","excerpt":"1. 引言鉴于第一次运行的 fast align 和 Giza++ 是在一个大数据集上进行的，不仅耗时巨大，而且结果多的反而让我眼花缭乱，再加之有一些细节地方并没有得以实现，所以我在一个小数据集上再次进行了实验。而为了能够直观地解释对齐关系，我选用了如下数据集： 数据集：newstest2017 ch-en dataset which contains 2001 examples. 预处理：因为是直接从实验室找的，所以所有文本已经被 tokenize 过了。","text":"1. 引言鉴于第一次运行的 fast align 和 Giza++ 是在一个大数据集上进行的，不仅耗时巨大，而且结果多的反而让我眼花缭乱，再加之有一些细节地方并没有得以实现，所以我在一个小数据集上再次进行了实验。而为了能够直观地解释对齐关系，我选用了如下数据集： 数据集：newstest2017 ch-en dataset which contains 2001 examples. 预处理：因为是直接从实验室找的，所以所有文本已经被 tokenize 过了。 2. Fast Align1. 实验过程参考 Align Tools 即可。 2. 错误解决 – Could NOT find SparseHash (missing: SPARSEHASH_INCLUDE_DIR) 即使去安装 sparsehash 也是失败的，所以暂时没找到解决方法 但好像不解决这个也能够继续下去 如何检查自己的 fast align 是否已安装完成 输入下列指令 /data1/wbxu/fast_align/build/fast_align 返回如下结果即表明安装完成123456789101112131415Usage: /data1/wbxu/fast_align/build/fast_align -i file.fr-en Standard options ([USE] = strongly recommended): -i: [REQ] Input parallel corpus -v: [USE] Use Dirichlet prior on lexical translation distributions -d: [USE] Favor alignment points close to the monotonic diagonoal -o: [USE] Optimize how close to the diagonal alignment points should be -r: Run alignment in reverse (condition on target and predict source) -c: Output conditional probability table Advanced options: -I: number of iterations in EM training (default = 5) -q: p_null parameter (default = 0.08) -N: No null word -a: alpha parameter for optional Dirichlet prior (default = 0.01) -T: starting lambda for diagonal distance parameter (default = 4) -s: print alignment scores (alignment ||| score, disabled by default) 3. 结果分析1. 执行输出日志分析12345678910111213141516ITERATION 5 (FINAL) log_e likelihood: -203555 log_2 likelihood: -293667 cross entropy: 6.49534 perplexity: 90.2178 posterior p0: 0 posterior al-feat: 0ITERATION 5 (FINAL) log_e likelihood: -223478 log_2 likelihood: -322411 cross entropy: 6.07383 perplexity: 67.3607 posterior p0: 0 posterior al-feat: 0 size counts: 728 虽然，随着 EM 算法的执行，整个对齐的困惑度在不断下降，但是无论是前向对齐还是逆向对齐，它们**最终的困惑度都很高 (≈90, 67)**； 最终的的负对数似然也很高； 其他的参数尚不明了，因此不作分析。 2. 句内对齐分析 fast align 的输出结果是以位置间对齐映射关系的形式存储的，这虽然对于机器是友好的，但是对于人类是极其不友好的，如下：12(Source-Target) 0-0 0-1 2-2 4-3 5-4 6-5 7-6 9-7 11-8 11-9(Source-Target) 9-0 2-1 2-2 5-3 12-4 7-5 0-6 1-7 5-8 12-9 25-10 14-11 15-12 12-13 17-14 22-15 21-16 23-17 20-18 23-19 我们同时执行了源语言到目标语言的对齐和目标语言到源语言的对齐，而这两个方向的对齐并不是完全可逆的；En→Zh 方向上，’28’对齐到’岁’，而 Zh→En 方向上，’岁’对齐到‘@-@’，如下：12(Source-Target) 28-28 28-岁 Year-厨师 Old-被 Chef-发现 Found-死 Dead-于 San-旧金山 Mal-一家 Mal-商 (Target-Source) 28-28 岁-@-@ 厨师-Year 岁-@-@ 厨师-Old 发现-Chef 死-Found 死-Dead 旧金山-at 旧金山-San 旧金山-Francisco 商-Mal 我们发现tokenized数据会对 fast align 的对齐过程增加噪声；‘@-@’ 是一个子词表示，而正常来说，同一个单词的所有子词应该是对齐同一个单词或同样的几个单词的，如下：1(Source-Target) recently-近日 @-@-刚 @-@-搬 old-至 San-旧金山 who-的 a-一位 28-28 old-岁 San-厨师 week-本周 was-被 found-发现 在同一个句子内，诚然 fast align 建模了一些正确的对齐关系，但是错误的对齐映射关系也不少，甚至在有些句子中错误的数量超过了正确的数量，如下：12(Source-Target) to-针对 government-政府 &amp;apos;s-的 silence-沉默 to-态度 ,-， JDC-初级 JDC-医生 JDC-委员会 JDC-执行 JDC-委员会 has-已 today-今日 formal-正式 request-要求 BMA-英国 BMA-医学 BMA-协会 a-理事会 meeting-召开 special-特别 meeting-会议 authorise-批准 to-旨在 programme-九月初 beginning-开始 of-升级 industrial-劳工 action-行动 of-的 a-一项 (Source-Target) rejected-参与 rejected-投票 by-的 58-成员 of-中 of-, members-58% rejected-反对 the-该 ballot-合同 ballot-交易 不难发现，fast align 会将同一个单词对应多个目标单词，而这里面可能只有一个是对的，甚至一个对的都没有，给人一种蒙的感觉，如下：12(Target-Source) 该-the 数字-number 已-has 大幅-slumped 大幅-by 近-almost 90%-90 90%-% (Source-Target) her-拜 her-尔斯 in-因此 puts-跻身 like-像 puts-迈克尔 @-@-- league-菲尔 league-普斯 @-@-( @-@-Michael @-@-Phelps @-@-) like-一样 @-@-的 a-“ @-@-几十年 @-@-一遇 athletes-运动员 @-@-” who-的 Michael-行列 their-他们 to-将 Phelps-各自 their-的 sports-体育项目 sports-提升 to-到 new-新 heights-高度 3. 所有对齐分析首先，我们需要编写python程序来统计某个单词一共对齐哪些单词，我写的代码如下 (注：由于欠考虑原始的’-‘符号等原因，结果中会存在一定的错误，但是总体来说，已经能够反映出 fast align 的对齐情况了)： 123456789101112131415161718192021222324252627def mapping(file, output): text = [] with open(file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f: content = f.readlines() length = len(content) for i in range(length): text.append(content[i].split()) mappings = &#123;&#125; for i in range(length): for j in range(len(text[i])): for k in range(len(text[i][j])): if text[i][j][k] == &quot;-&quot;: key = text[i][j][:k] value = text[i][j][k+1:] break if key not in mappings.keys(): mappings[key] = [value, ] elif value not in mappings[key]: mappings[key].append(value) with open(output, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f: for key, value in mappings.items(): f.write(str(key) + str(value) + &quot;\\n&quot;) 上述程序执行之后就可以得到所有单词的所有对齐情况了，于是我们又发现了如下的问题： fast align 能够建模正确的对齐关系，如下：12recently [&#x27;近日&#x27;, &#x27;最近&#x27;, &#x27;连日来&#x27;, &#x27;日前&#x27;, &#x27;近期&#x27;]八个 [&#x27;eight&#x27;] 但是，在某个单词的所有对齐关系中存在着很多的不正确对齐关系，如下：12friends [&#x27;朋友&#x27;, &#x27;表达&#x27;, &#x27;深切&#x27;, &#x27;政变&#x27;, &#x27;亲朋好友&#x27;, &#x27;日子&#x27;, &#x27;误会&#x27;, &#x27;段时间&#x27;, &#x27;交流&#x27;]发现 [&#x27;Chef&#x27;, &#x27;found&#x27;, &#x27;in&#x27;, &#x27;that&#x27;, &#x27;find&#x27;, &#x27;body&#x27;, &#x27;continues&#x27;, &#x27;was&#x27;] 有一些单词甚至没有正确的对齐关系，如下：12weighing [&#x27;战术&#x27;, &#x27;理性&#x27;]验尸官[&#x27;body&#x27;, &#x27;Westfield&#x27;] 句子中的标点符号能够作为一种噪声存在，它对齐了很多非标点符号的(子)词，如下：12.[&#x27;。&#x27;, &#x27;时&#x27;, &#x27;，&#x27;, &#x27;”&#x27;, &#x27;都&#x27;, &#x27;食物&#x27;, &#x27;没想到&#x27;, &#x27;了&#x27;, &#x27;方式&#x27;, &#x27;；&#x27;, &#x27;的&#x27;, &#x27;这个&#x27;, &#x27;直播&#x27;, &#x27;保存&#x27;, &#x27;女士&#x27;, &#x27;用品&#x27;, &#x27;故&#x27;, &#x27;货品&#x27;, &#x27;非常&#x27;, &#x27;就&#x27;, &#x27;才&#x27;, &#x27;岁&#x27;, &#x27;地&#x27;, &#x27;跟&#x27;, &#x27;无小事&#x27;, &#x27;我&#x27;, &#x27;,&#x27;, &#x27;不&#x27;, &#x27;外地&#x27;, &#x27;防晒&#x27;, &#x27;常见&#x27;, &#x27;交工&#x27;, &#x27;特点&#x27;, &#x27;几句&#x27;, &#x27;事情&#x27;, &#x27;能力&#x27;, &#x27;职位&#x27;, &#x27;扮&#x27;, &#x27;很&#x27;],[&#x27;，&#x27;, &#x27;为&#x27;, &#x27;美国广播公司&#x27;, &#x27;KGO&#x27;, &#x27;波士顿&#x27;, &#x27;同时&#x27;, &#x27;在&#x27;, &#x27;烹饪&#x27;, &#x27;主席&#x27;, &#x27;一贯&#x27;, &#x27;感觉&#x27;, &#x27;的&#x27;, &#x27;政府&#x27;, &#x27;举行&#x27;, &#x27;。&#x27;, &#x27;顿&#x27;, &#x27;网球&#x27;, &#x27;例如&#x27;, &#x27;、&#x27;, &#x27;Co&#x27;, &#x27;您&#x27;, &#x27;已&#x27;, &#x27;这些&#x27;, &#x27;3&#x27;, &#x27;道&#x27;, &#x27;其中&#x27;, &#x27;比赛&#x27;, &#x27;真正&#x27;, &#x27;还是&#x27;, &#x27;年&#x27;, &#x27;表现&#x27;, &#x27;前&#x27;, &#x27;·&#x27;, &#x27;岁&#x27;, &#x27;后&#x27;, &#x27;了&#x27;, &#x27;（&#x27;, &#x27;）&#x27;, &#x27;请求&#x27;, &#x27;是&#x27;, &#x27;(&#x27;, &#x27;重新&#x27;, &#x27;他&#x27;, &#x27;之后&#x27;, &#x27;进行&#x27;, &#x27;表演&#x27;, &#x27;还有&#x27;, &#x27;分别&#x27;, &#x27;事实&#x27;, &#x27;但&#x27;, &#x27;腹部&#x27;, &#x27;部位&#x27;, &#x27;被&#x27;, &#x27;从&#x27;, &#x27;其次&#x27;, &#x27;里面&#x27;, &#x27;包含&#x27;, &#x27;犹太教&#x27;, &#x27;天主教&#x27;, &#x27;科&#x27;, &#x27;精彩绝伦&#x27;, &#x27;主要&#x27;, &#x27;差旅&#x27;, &#x27;服装&#x27;, &#x27;包括&#x27;, &#x27;教练&#x27;, &#x27;这&#x27;, &#x27;影响&#x27;, &#x27;中&#x27;, &#x27;15&#x27;, &#x27;丽埃勒及&#x27;, &#x27;参与&#x27;, &#x27;成本&#x27;, &#x27;课程&#x27;, &#x27;提纲&#x27;, &#x27;认知&#x27;, &#x27;进修&#x27;, &#x27;性别&#x27;, &#x27;重视&#x27;, &#x27;时间&#x27;, &#x27;罹患&#x27;, &#x27;评估&#x27;, &#x27;教授&#x27;, &#x27;但是&#x27;, &#x27;任&#x27;, &#x27;少数&#x27;, &#x27;族裔&#x27;, &#x27;移民&#x27;, &#x27;然而&#x27;, &#x27;从小&#x27;, &#x27;很&#x27;, &#x27;体操选手&#x27;, &#x27;安娜&#x27;, &#x27;Jennifer&#x27;, &#x27;Carpenter&#x27;, &#x27;盖&#x27;, &#x27;开始&#x27;, &#x27;现年&#x27;, &#x27;分&#x27;, &#x27;57&#x27;, &#x27;这种&#x27;, &#x27;镇&#x27;, &#x27;说&#x27;, &#x27;枪手&#x27;, &#x27;当&#x27;, &#x27;内&#x27;, &#x27;棍&#x27;, &#x27;要&#x27;, &#x27;人们&#x27;, &#x27;其&#x27;, &#x27;在内&#x27;, &#x27;理发&#x27;, &quot;&#x27;&quot;, &#x27;斯&#x27;, &#x27;目前&#x27;, &#x27;日前&#x27;, &#x27;自然&#x27;, &#x27;,&#x27;, &#x27;带来&#x27;, &#x27;其实&#x27;, &#x27;发微博&#x27;, &#x27;玩游戏&#x27;, &#x27;逛&#x27;, &#x27;需要&#x27;, &#x27;：&#x27;, &#x27;种植&#x27;, &#x27;个&#x27;, &#x27;股份&#x27;, &#x27;锦州市&#x27;, &#x27;大有&#x27;, &#x27;滨海公路&#x27;, &#x27;段&#x27;, &#x27;1&#x27;, &#x27;号&#x27;, &#x27;11&#x27;, &#x27;有&#x27;, &#x27;都&#x27;, &#x27;不久&#x27;, &#x27;打出&#x27;, &#x27;加上&#x27;, &#x27;无论是&#x27;, &#x27;见面&#x27;, &#x27;自己&#x27;, &#x27;家中&#x27;, &#x27;成立&#x27;, &#x27;保障&#x27;, &#x27;握&#x27;, &#x27;手心&#x27;, &#x27;轻飘飘&#x27;, &#x27;一小撮&#x27;, &#x27;百万元&#x27;, &#x27;海关&#x27;, &#x27;较&#x27;, &#x27;决定&#x27;, &#x27;部&#x27;, &#x27;总导演&#x27;, &#x27;一声令下&#x27;, &#x27;武警&#x27;, &#x27;和田&#x27;, &#x27;阿克苏&#x27;, &#x27;三地&#x27;, &#x27;部队&#x27;, &#x27;导调&#x27;, &#x27;指令&#x27;, &#x27;环绕&#x27;, &#x27;所以&#x27;, &#x27;质量&#x27;, &#x27;稀薄&#x27;, &#x27;车&#x27;, &#x27;总体&#x27;, &#x27;不断&#x27;, &#x27;行政院长&#x27;, &#x27;昨天上午&#x27;, &#x27;刚好&#x27;, &#x27;真的&#x27;, &#x27;一切&#x27;, &#x27;金牌&#x27;, &#x27;其间&#x27;, &#x27;比如说&#x27;, &#x27;多个&#x27;, &#x27;转而&#x27;, &#x27;运动员&#x27;, &#x27;)&#x27;, &#x27;监狱&#x27;, &#x27;时&#x27;, &#x27;地区&#x27;, &#x27;支队长&#x27;, &#x27;马琪&#x27;, &#x27;新闻&#x27;, &#x27;指标&#x27;, &#x27;情况&#x27;, &#x27;所在&#x27;, &#x27;39&#x27;, &#x27;格雷&#x27;, &#x27;如今&#x27;, &#x27;中部&#x27;, &#x27;厘米&#x27;, &#x27;等&#x27;, &#x27;随着&#x27;, &#x27;日&#x27;, &#x27;收到&#x27;, &#x27;仍&#x27;, &#x27;次&#x27;, &#x27;品类&#x27;, &#x27;涵盖&#x27;, &#x27;建材&#x27;, &#x27;轻工产品&#x27;, &#x27;出台&#x27;, &#x27;加大&#x27;, &#x27;引进&#x27;, &#x27;220&#x27;, &#x27;近期&#x27;, &#x27;方面&#x27;, &#x27;之声&#x27;, &#x27;基础&#x27;, &#x27;喀山&#x27;, &#x27;世锦赛&#x27;, &#x27;和&#x27;, &#x27;各种&#x27;, &#x27;宁泽涛&#x27;, &#x27;奠定&#x27;, &#x27;田亮&#x27;, &#x27;去&#x27;, &#x27;而&#x27;, &#x27;伦敦&#x27;, &#x27;功成身退&#x27;, &#x27;甚至&#x27;, &#x27;比如&#x27;, &#x27;快&#x27;, &#x27;回复&#x27;, &#x27;却&#x27;, &#x27;查&#x27;, &#x27;车辆&#x27;, &#x27;整治&#x27;, &#x27;看病&#x27;, &#x27;监管&#x27;, &#x27;存在&#x27;, &#x27;于&#x27;, &#x27;采访&#x27;, &#x27;四川&#x27;, &#x27;绵阳&#x27;, &#x27;往下沉&#x27;, &#x27;小时候&#x27;, &#x27;学过&#x27;, &#x27;坚持&#x27;, &#x27;足球&#x27;, &#x27;篮球&#x27;, &#x27;排球&#x27;, &#x27;自行车&#x27;, &#x27;摔跤&#x27;, &#x27;武术&#x27;, &#x27;市场运作&#x27;, &#x27;节俭办&#x27;, &#x27;同年&#x27;, &#x27;时许&#x27;, &#x27;房间内&#x27;, &#x27;研究&#x27;, &#x27;保持&#x27;, &#x27;8&#x27;, &#x27;再&#x27;, &#x27;主导&#x27;, &#x27;分析&#x27;, &#x27;把&#x27;, &#x27;强&#x27;, &#x27;科技&#x27;, &#x27;还&#x27;, &#x27;增加&#x27;, &#x27;设有&#x27;, &#x27;节日&#x27;, &#x27;布置&#x27;, &#x27;７&#x27;, &#x27;三亚市&#x27;, &#x27;对&#x27;, &#x27;他会&#x27;, &#x27;欧文&#x27;, &#x27;操刀&#x27;, &#x27;助攻&#x27;, &#x27;拦截&#x27;, &#x27;意甲&#x27;, &#x27;法甲&#x27;, &#x27;王国&#x27;, &#x27;抢断&#x27;, &#x27;配合&#x27;, &#x27;跑动&#x27;, &#x27;无懈可击&#x27;, &#x27;现在&#x27;, &#x27;担任&#x27;, &#x27;训练&#x27;, &#x27;这周&#x27;, &#x27;不好&#x27;, &#x27;功能&#x27;, &#x27;针对&#x27;, &#x27;照片&#x27;, &#x27;推特&#x27;, &#x27;各色&#x27;, &#x27;人种&#x27;, &#x27;表示&#x27;, &#x27;负责&#x27;, &#x27;战争&#x27;, &#x27;国防&#x27;, &#x27;位于&#x27;, &#x27;错误&#x27;, &#x27;结果&#x27;, &#x27;伊拉克&#x27;, &#x27;维和部队&#x27;, &#x27;降低&#x27;, &#x27;格雷戈里&#x27;, &#x27;撰写&#x27;, &#x27;一篇&#x27;, &#x27;文章&#x27;, &#x27;文中&#x27;, &#x27;布什&#x27;, &#x27;不顾&#x27;, &#x27;未尝&#x27;, &#x27;败绩&#x27;, &#x27;巴姆&#x27;, &#x27;上&#x27;, &#x27;作为&#x27;, &#x27;菜&#x27;, &#x27;共同&#x27;, &#x27;本书&#x27;, &#x27;三明治&#x27;, &#x27;们&#x27;, &#x27;并&#x27;, &#x27;至&#x27;, &#x27;发布&#x27;, &#x27;奶油&#x27;, &#x27;补充&#x27;, &#x27;果脯&#x27;, &#x27;赞加&#x27;, &#x27;沉得&#x27;, &#x27;下心&#x27;, &#x27;公司&#x27;, &#x27;形式&#x27;, &#x27;在线&#x27;, &#x27;学习&#x27;, &#x27;不仅&#x27;, &#x27;而且&#x27;, &#x27;如遇&#x27;, &#x27;9&#x27;, &#x27;章&#x27;, &#x27;法制&#x27;, &#x27;层面&#x27;, &#x27;框架&#x27;, &#x27;军力&#x27;, &#x27;文化公园&#x27;, &#x27;金额&#x27;, &#x27;奥马哈&#x27;, &#x27;巴克&#x27;, &#x27;所有&#x27;, &#x27;尤其&#x27;, &#x27;特&#x27;, &#x27;女子组&#x27;, &#x27;洛特&#x27;, &#x27;乔&#x27;, &#x27;埃莉诺&#x27;, &#x27;孩子&#x27;, &#x27;培养&#x27;, &#x27;朋友&#x27;, &#x27;总需求&#x27;, &#x27;减弱&#x27;, &#x27;海南省&#x27;, &#x27;路&#x27;, &#x27;走&#x27;, &#x27;天津&#x27;, &#x27;甘肃&#x27;, &#x27;省市&#x27;, &#x27;实施细则&#x27;, &#x27;即&#x27;, &#x27;经查&#x27;, &#x27;迎战&#x27;, &#x27;得到&#x27;, &#x27;拍照&#x27;, &#x27;通过&#x27;, &#x27;人中&#x27;, &#x27;法律咨询&#x27;, &#x27;代理服务&#x27;, &#x27;拒&#x27;, &#x27;巨额&#x27;, &#x27;债务&#x27;, &#x27;分崩离析&#x27;, &#x27;这位&#x27;, &#x27;名叫&#x27;, &#x27;预算&#x27;, &#x27;T5&#x27;, &#x27;由&#x27;, &#x27;来&#x27;, &#x27;无水&#x27;, &#x27;激情&#x27;, &#x27;过&#x27;, &#x27;太阳系&#x27;, &#x27;诸多方面&#x27;] 3. GIZA++1. 实验过程参考 Align Tools 即可。 2. 错误解决 align_sym.py 文件无法正常运行 需要使用 python2 的环境来运行，使用 conda 即可创建 python2 的虚拟环境 align_plot.py 文件无法正常运行 使用之前创建好的 python2 环境 下载依赖包 numpy 和 matplotlib 下载 sans-serif 字体，否则中文将无法正常显示 下载 simhei 字体 从网站 fontpalace 下载 SimHei 字体文件 找到存放字体文件的目录, 大约为~/matplotlib/mpl-data/fonts/ttf，可通过下列代码找到：12import matplotlibprint(matplotlib.matplotlib_fname()) 将文件 simhei.ttf 存放入 ~/fonts/ttf 目录下 将如下代码添加到文件 matplotlibrc 中去123font.family : sans-seriffont.sans-serif : SimHeiaxes.unicode_minus : False 删除缓存中已生成的 matplotlib 目录，大约为 ~/.cache/matplotlib 使用下述命令重启虚拟环境12conda deactivateconda activate python2 3. 结果分析 许多文件的不明所以，完全没有任何提示信息，对新手极其不友好，如下:12345678en2ch.a3.final:1 1 2 100 11 2 2 100 12 3 2 100 12 4 2 100 11 1 3 100 0.7773383 1 3 100 0.2226621 2 3 100 0.443757 en2ch.perp 文件表示的是困惑度，不难发现虽然困惑度呈现逐步下降趋势，但是最终的困惑度仍然很高，而且 HMM 的困惑度很接近于 Model 4 的困惑度；无独有偶，在之前的大数据集上 HMM 的困惑度甚至低于 Model 4 的困惑度，这一点是令我不解的，如下：123456789101112131415161718192021#trnsz tstsz iter model trn-pp test-pp trn-vit-pp tst-vit-pp2001 0 0 Model1 11770.9 N/A inf N/A2001 0 1 Model1 112.831 N/A 745.31 N/A2001 0 2 Model1 81.2514 N/A 413.337 N/A2001 0 3 Model1 67.9365 N/A 271.741 N/A2001 0 4 Model1 61.7402 N/A 211.242 N/A2001 0 5 HMM 58.0343 N/A 180.88 N/A2001 0 6 HMM 52.6243 N/A 91.7463 N/A2001 0 7 HMM 38.5041 N/A 54.2244 N/A2001 0 8 HMM 28.8229 N/A 36.4591 N/A2001 0 9 HMM 23.8012 N/A 28.4325 N/A2001 0 10 THTo3 20.6636 N/A 22.4276 N/A2001 0 11 Model3 47.6184 N/A 50.2598 N/A2001 0 12 Model3 42.8841 N/A 44.75 N/A2001 0 13 Model3 41.4157 N/A 43.0298 N/A2001 0 14 Model3 40.4792 N/A 41.9223 N/A2001 0 15 T3To4 39.7868 N/A 41.1109 N/A2001 0 16 Model4 21.8971 N/A 22.3101 N/A2001 0 17 Model4 20.2356 N/A 20.5329 N/A2001 0 18 Model4 19.7004 N/A 19.9528 N/A2001 0 19 Model4 19.3744 N/A 19.6003 N/A en2ch.a3.final 文件中的内容形式如下：$i\\ j\\ l\\ m\\ p(i/j, l, m)$，其中 $i$ 表示源语言 token 的位置；$j$ 表示目标语言 token 的位置；$l$ 表示源语言句子长度；$m$ 表示目标语言句子长度；$p(i/j, l, m)$ 表示 $i$ 位置上的源语言 token 被翻译到目标语言 $j$ 位置上 token 的概率；总的来说，这个文件应该可以解决对齐中的重排序问题(reordering)，粗略看一了下还是比较邻近的位置上的reordering概率比较大，如下：123456789101112131415161764 49 64 100 164 50 64 100 164 51 64 100 164 52 64 100 163 53 64 100 163 54 64 100 161 55 64 100 161 56 64 100 10 57 64 100 161 58 64 100 15 1 66 100 18 2 66 100 19 3 66 100 10 4 66 100 133 5 66 100 133 6 66 100 133 7 66 100 1 en2ch.n3.final 文件表示的是各个 token 的繁殖力概率，很多单词的繁殖力还是主要集中在 0、1、2 这些数字上，如下：1234562 0.790922 0.163175 0.0374083 0.00353707 0.00180963 0.00064888 0.00153155 0.000358334 0.000246704 0.000363033 3 0.967959 0.0279171 0.00146716 0.000960575 0.000645714 0.000464062 0.000311298 0.000119431 7.17008e-05 8.35965e-05 4 0.628078 0.223453 0.0939839 0.0166623 0.0138697 0.00923218 0.00555119 0.00400141 0.00302904 0.00213859 5 0.828264 0.123534 0.0269111 0.00769915 0.00517549 0.00371953 0.0024951 0.000957258 0.000574693 0.000670038 6 0.647396 0.229846 0.066597 0.017175 0.0142964 0.00951625 0.005722 0.00412453 0.00312224 0.00220439 7 0.519123 0.307894 0.0652615 0.0394383 0.0267116 0.0176144 0.0104919 0.00504176 0.00457916 0.003845 en2ch.t3.final 文件是 IBM Model 3 的翻译表，en2ch.d4.final 文件是 IBM Model 4 的翻译表，由于是 ids 间的对齐，并不直观，如下：1234567890 16 0.3829580 33 0.4460210 40 0.06683410 41 6.36139e-070 72 0.09552340 96 0.002046120 491 0.0001623880 696 6.19271e-060 737 0.00643965 en2ch.A3.final 文件是单向对齐文件，其中的数字代表了 token 在目标句中的位置，下标从 1 开始；如下：123456#Sentence pair (1) source length 12 target length 10 alignment score : 3.26715e-1428 岁 厨师 被 发现 死 于 旧金山 一家 商 NULL (&#123; &#125;) 28 (&#123; 1 &#125;) @-@ (&#123; &#125;) Year (&#123; &#125;) @-@ (&#123; &#125;) Old (&#123; &#125;) Chef (&#123; &#125;) Found (&#123; &#125;) Dead (&#123; 2 3 4 5 6 &#125;) at (&#123; &#125;) San (&#123; 8 &#125;) Francisco (&#123; 7 &#125;) Mal (&#123; 9 10 &#125;) #Sentence pair (2) source length 26 target length 20 alignment score : 3.26866e-28近日 刚 搬 至 旧金山 的 一位 28 岁 厨师 本周 被 发现 死 于 当地 一家 商场 的 楼梯间 NULL (&#123; 6 19 &#125;) a (&#123; &#125;) 28 (&#123; 8 &#125;) @-@ (&#123; &#125;) year (&#123; &#125;) @-@ (&#123; &#125;) old (&#123; 9 &#125;) chef (&#123; 10 &#125;) who (&#123; &#125;) had (&#123; &#125;) recently (&#123; &#125;) moved (&#123; &#125;) to (&#123; &#125;) San (&#123; &#125;) Francisco (&#123; 14 15 &#125;) was (&#123; &#125;) found (&#123; 13 &#125;) dead (&#123; &#125;) in (&#123; &#125;) the (&#123; &#125;) stairwell (&#123; 1 2 3 4 5 7 &#125;) of (&#123; &#125;) a (&#123; &#125;) local (&#123; 16 &#125;) mall (&#123; 12 17 18 20 &#125;) this (&#123; &#125;) week (&#123; 11 &#125;) 如上，GIZA++ 能够找出正确的对齐关系，如 28-28 week-本周 等； 如上，GIZA++ 也构造了错误的对齐关系，如 Dead-岁 stairwell-近日 等； 4. 可视化及分析如果需要可视化对齐结果的话，我们需要两个 pyhton 文件，分别是 align_sym.py 和 align_plot.py，在下面的实验结果连接中可以找到这两个文件（感谢爱心师兄改正的python文件 align_sym.py）。 首先，根据两个不同方向的单向对齐文件生成对称化文件，运行如下代码即可： 1python align_sym.py en2ch/en2ch.A3.final ch2en/ch2en.A3.final &gt; aligned.grow-diag-final-and 其次，运行如下文件绘制可视图： 1python align_plot.py test_data/en_tok.txt test_data/ch_tok.txt aligned.grow-diag-final-and &quot;example number&quot; 可视化结果如下所示： Example 9： Example 86： Example 126： Example 1511： 我随机选择了几条数据样本进行可视化，如上图所示，通过观察我发现： GIZA++ 不仅正确地建模了 One-to-One Mapping 的对齐关系，而且还正确建模了 One-to-Many Mapping 和 Many-to-One Mapping 的对齐关系； 虽然对齐中存在着 Reordering 问题，但是正确对齐的一大部分都呈现出一条斜对角线； GIZA++ 还建模了许多错误的对齐关系； 和 fast algin 一样，GIZA++ 也存在着一个词对齐着很多词的现象，而其中只有几个是正确的对齐。 4. GIZA++隐藏用法 实现方法 我们可以先删除整个 giza-pp-master 目录，在重新下载/解压； 然后，找到 GIZA++v2/Makefile 文件，将其打开； 将上述文件中第 9 行的 -DBINARY_SEARCH_FOR_TTABLE 删除； 重复之前的所有操作。 隐藏文件 en2ch.ti.final 文件是 ids 的双语对齐表，其形式为 $source\\ id\\ \\ \\ target\\ id\\ \\ \\ p(align(source\\ id, target\\ id))$，如下：123454459 2372 1.06663e-061405 6900 2.70731e-088416 162 4.56842e-068416 380 5.20437e-068416 737 8.84737e-06 en2ch.actual.ti.final 文件是 tokens 的双语对齐表, 其形式为 $source\\ token\\ \\ \\ target\\ token\\ \\ \\ p(align(source\\ token, target\\ token))$，如下：1234518 August 1.06663e-06奥运会 Athens 2.70731e-08熬夜 day 4.56842e-06熬夜 next 5.20437e-06熬夜 sports 8.84737e-06 结果分析 有很多完全错误的对齐关系，如下：12奖牌 event 0.159333产生 event 0.0866653 有些单词对齐了很多单词，但只有一个或几个是正确的，如下：123感到 worried 0.0346762比较 worried 0.0737838担心 worried 0.113913 虽然有些单词有对有错，但是正确的对齐关系概率最高，如下：12345意义 ecosystem 0.0287441具有 ecosystem 0.0234632方面 ecosystem 0.00794153生态圈 ecosystem 0.402861行业 ecosystem 0.0152548 粗略看下来，GIZA++的错误率还是很高。 5. 总结fast algin 的对齐结果直观地看起来是不如 GIZA++ 的，但是 GIZA++ 在小数据集上从搭建到运行出结果所花的时间远远大于 fast align，甚至在大数据集上花费了3天左右的时间。而在这两个对齐工具所展现的对齐结果中，我们都发现了一个单词对齐到很多单词的现象(仅有一个或几个单词是正确的对齐关系，其余皆是错误对齐)。如果我们需要提高对齐的质量以提升模型的性能，那么可以使用 GIZA++；而如果我们的对齐并不是模型提升性能的关键因素，那么或许也可以使用 fast align。关于后续的对齐学习，我将着手去学习 Connectionist Temporal Classification，该技术在 Non-Autoregressive Translation 中十分关键。","categories":[{"name":"Statistical Machine Translation","slug":"Statistical-Machine-Translation","permalink":"http://example.com/categories/Statistical-Machine-Translation/"}],"tags":[{"name":"Alignment","slug":"Alignment","permalink":"http://example.com/tags/Alignment/"}]},{"title":"End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification","slug":"End-to-End-Non-Autoregressive-Neural-Machine-Translation-with-Connectionist-Temporal-Classification","date":"2021-11-24T06:14:17.000Z","updated":"2021-11-25T08:35:20.049Z","comments":true,"path":"2021/11/24/End-to-End-Non-Autoregressive-Neural-Machine-Translation-with-Connectionist-Temporal-Classification/","link":"","permalink":"http://example.com/2021/11/24/End-to-End-Non-Autoregressive-Neural-Machine-Translation-with-Connectionist-Temporal-Classification/","excerpt":"Transformer 虽然帮助 NMT 模型摆脱了训练时的顺序执行问题，但是却没有使得其在推理时仍然保持这种并行化优势。非自回归翻译模型 (NAT) 让推理时的每个 token 都能够并行地被生成，但是付出了较大的性能衰减代价。作者提出了将序列标注问题中的 Connectionist Temporal Classification (CTC) 引入到 NAT 模型中去，从而在保持推理速度的同时提升其翻译质量。作者在 WMT En-Ro 和 WMT En-De 数据集上进行了实验，结果表明 CTC-based NAT 模型能够获得与其他非自回归模型相当的性能表现。","text":"Transformer 虽然帮助 NMT 模型摆脱了训练时的顺序执行问题，但是却没有使得其在推理时仍然保持这种并行化优势。非自回归翻译模型 (NAT) 让推理时的每个 token 都能够并行地被生成，但是付出了较大的性能衰减代价。作者提出了将序列标注问题中的 Connectionist Temporal Classification (CTC) 引入到 NAT 模型中去，从而在保持推理速度的同时提升其翻译质量。作者在 WMT En-Ro 和 WMT En-De 数据集上进行了实验，结果表明 CTC-based NAT 模型能够获得与其他非自回归模型相当的性能表现。 首先，作者的模型是基于强大的 Transformer 模型而搭建起来的，并且其编码器部分保持不变。因为无论是 AT 模型的编码器还是 NAT 模型的编码器，它们所承担的职责是完全相同的，所以作者选择将编码器部分保持不变。 鉴于 NAT 模型的强条件独立性假设，所以作者将非自回归翻译任务看作为一个序列标注问题。正如 CTC 的原始论文中所述，机器翻译中的源句和目标句长度往往是不相同的，这一点就和序列标注有所冲突，因此无法直接将序列标注器应用在机器翻译上。为此，作者采用了一种拉伸输入序列长度的方法，从而可以保证标注器的输入序列长度一定是不小于输出序列长度的。简言之，作者将编码器输出的每个隐藏状态 $h$ 映射到一个更长的隐藏表示 $W_{spl}\\cdot h$，然后再将该表示切分成 $k$ 个隐藏表示，如此便得到了长度为 $k \\cdot T_x$ 的隐藏表示序列，如下所示：$$s_{ci+b} = (W_{spl} h_c + b_{spl})_{bd:(b+1)d}$$ 既然作者已经让输入 CTC 解码器的输入序列长度大于等于目标序列的长度了，接下来就是将 CTC 正式引入了。上述增长的隐藏表示序列会被喂给解码器解码，值得注意的是作者取消了原始 Transformer 解码器自注意力网络中的 mask 机制以鼓励每个 token 的预测都依赖于其双向上下文信息。最终解码器输出的隐藏状态都会被标注成目标语言的 tokens 或者一个 null 标记，如果被标注上目标语言的 token 则表示该位置上的翻译内容，而如果被标注上 null 则表示该位置上无内容生成。由于输入输出之间没有先验的对齐关系，所以作者将所有能产生正确结果的输出序列都考虑进损失函数中。又因为直接对所有组合的指数数量进行求和是不切实际的，于是作者顺水推舟，将 CTC loss 引入模型之中。 CTC 通过使用动态规划(dynamic programming)算法来计算输出序列的负对数似然，而这个损失可以借助一个类似于训练隐马尔可夫模型的线性算法来计算得到。CTC 使用动态规划的方法计算出输出序列的所有前后缀的偏对数概率之和，而这些概率会被存储在一个预计算对数概率表格中，于是我们就可以通过结合对应前缀和后缀的对数概率来计算得到正确输出序列的部分概率了。除此之外，CTC 还为作者带来了束搜索算法，虽然这会让模型的推理时延稍微上升，但是却能够有效地提升模型性能。因为线性层上的顺序计算还是要比自回归解码快得多，但是束搜索已经被证明能够有效地提升模型翻译体质量了。 下图为作者所提出的 NAT 模型结构： 最终实验结果(如下图)表明，作者所提出的模型在 WMT16 En-Ro 和 WMT14 En-De 数据集上的表现不尽如人意，但是在 WMT15 En-De 数据集上则创造出了 NAT 模型中的 SOTA 成绩。 本论文感想：由于本文中没有详细介绍 CTC loss 的原理，所以后续还需要看回 CTC 的原始论文！","categories":[{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"}],"tags":[{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"}]},{"title":"Align Tools","slug":"Align-Tools","date":"2021-11-23T07:08:19.000Z","updated":"2021-11-29T14:07:42.725Z","comments":true,"path":"2021/11/23/Align-Tools/","link":"","permalink":"http://example.com/2021/11/23/Align-Tools/","excerpt":"$\\mathfrak{Fast\\ \\ \\ Align}$","text":"$\\mathfrak{Fast\\ \\ \\ Align}$ 1. 从 Github 上下载 fast_align fast_align 2. 通过以下命令编译 fast_align1234mkdir buildcd buildcmake ..make 3. 准备平行语料 ( 可以 tokenize , 也可以不 tokenize ) 我使用的是 tokenized WMT14 En-De 训练数据集: 英语数据样本 12I declare resumed the session of the European Parliament ad@@ jour@@ ned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant fes@@ tive period .Although , as you will have seen , the d@@ read@@ ed &amp;apos; millenn@@ ium bug &amp;apos; failed to materi@@ alise , still the people in a number of countries suffered a series of natural disasters that truly were d@@ read@@ ful . 德语数据样本 12Ich erkläre die am Freitag , dem 17. Dezember unterbro@@ ch@@ ene Sitzungsperiode des Europäischen Parlaments für wieder@@ aufgenommen , wünsche Ihnen nochmals alles Gute zum Jahres@@ wechsel und hoffe , daß Sie schöne Ferien hatten .Wie Sie feststellen konnten , ist der ge@@ für@@ chtete &amp;quot; Mill@@ en@@ ium @-@ Bu@@ g &amp;quot; nicht eingetreten . Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden . 4. 编写 python 来执行 fast_align1234567891011121314151617181920212223242526272829303132333435363738394041import osdef fast_align_preprocess(path): src_path = path + &quot;/train.src&quot; tgt_path = path + &quot;/train.tgt&quot; res_path = path + &quot;/result.txt&quot; with open(src_path, &quot;r&quot;) as src_file, open(tgt_path, &quot;r&quot;) as tgt_file: src_sentences = src_file.readlines() tgt_sentences = tgt_file.readlines() with open(res_path, &quot;w&quot;) as file: for src, tgt in zip(src_sentences, tgt_sentences): res = src[:-1] + &quot; ||| &quot; + tgt[:-1] file.write(res + &quot;\\n&quot;)def use_fast_align(path): align = r&quot;/data/wbxu/fast_align-master/build/fast_align&quot; atools = r&quot;/data/wbxu/fast_align-master/build/atools&quot; res_path = path + &quot;/result.txt&quot; save_path1 = path + &quot;/wmt14_en-de_source-target.align&quot; save_path2 = path + &quot;/wmt14_en-de_target-source.align&quot; save_path3 = path + &quot;/wmt14_en-de_symmetrized.align&quot; total_path_forward = align + &quot; -i &quot; + res_path + &quot; -d -o -v &gt; &quot; + save_path1 os.system(total_path_forward) total_path_reverse = align + &quot; -i &quot; + res_path + &quot; -d -o -v -r &gt; &quot; + save_path2 os.system(total_path_reverse) symmetrize_path = atools + &quot; -i &quot; + save_path1 + &quot; -j &quot; + save_path2 + &quot; -c grow-diag-final-and &gt; &quot; + save_path3 os.system(symmetrize_path)def main(path=&quot;/data/wbxu/data&quot;): fast_align_preprocess(path) use_fast_align(path)main() 第一个函数用于预处理输入数据, 预处理完后的数据样本如下: 12I declare resumed the session of the European Parliament ad@@ jour@@ ned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant fes@@ tive period . ||| Ich erkläre die am Freitag , dem 17. Dezember unterbro@@ ch@@ ene Sitzungsperiode des Europäischen Parlaments für wieder@@ aufgenommen , wünsche Ihnen nochmals alles Gute zum Jahres@@ wechsel und hoffe , daß Sie schöne Ferien hatten .Although , as you will have seen , the d@@ read@@ ed &amp;apos; millenn@@ ium bug &amp;apos; failed to materi@@ alise , still the people in a number of countries suffered a series of natural disasters that truly were d@@ read@@ ful . ||| Wie Sie feststellen konnten , ist der ge@@ für@@ chtete &amp;quot; Mill@@ en@@ ium @-@ Bu@@ g &amp;quot; nicht eingetreten . Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden . 原始的输出文件是下标之间的对齐, 晦涩难懂, 输出样例如下: 正向对齐 forward align 120-0 1-1 3-2 2-3 13-4 6-6 14-7 15-8 10-9 10-10 11-11 11-12 11-13 7-14 8-15 10-16 11-17 2-18 17-19 25-20 26-21 23-22 25-23 25-24 24-25 30-26 30-27 18-28 33-29 34-30 34-31 35-32 38-33 38-34 36-35 42-362-0 3-1 6-2 6-3 7-4 8-6 11-7 10-8 10-9 12-10 13-11 14-12 14-13 14-14 15-15 15-16 16-17 17-18 19-19 42-20 30-21 24-23 27-24 33-25 29-26 30-27 33-28 40-29 35-30 42-31 42-32 逆向对齐 reverse align 120-0 1-1 2-1 3-2 4-1 5-2 6-6 7-14 8-15 9-9 10-9 11-9 12-3 13-4 14-7 15-8 16-18 17-19 18-28 19-20 20-20 21-20 22-22 23-22 24-25 25-20 26-21 28-24 29-27 30-26 32-25 33-29 34-31 35-32 36-35 37-33 38-33 39-33 42-360-5 1-4 2-0 3-1 4-1 5-3 6-2 7-4 8-6 9-8 10-8 11-9 12-10 13-11 14-13 15-15 16-10 17-18 19-19 20-19 24-23 27-24 28-24 29-26 30-27 31-24 32-28 33-28 34-30 35-30 38-29 39-29 40-29 41-29 42-32 Tips: 虽然是逆向对齐, 但是输出内容还是以 ‘src_id-tgt_id’ 的形式输出的~ 5. 将输出文件中数字 ids 转换成 tokens 对齐1234567891011121314151617181920212223242526272829303132def recover(path): align_path = path + &quot;/wmt14_en-de_source-target.align&quot; src_path = path + &quot;/train.src&quot; tgt_path = path + &quot;/train.tgt&quot; save_path = path + &quot;/token_source-target.align&quot; src_tokens, tgt_tokens, ali_mapping = [], [], [] with open(align_path, &quot;r&quot;) as fa, open(src_path, &#x27;r&#x27;) as fs, open(tgt_path, &#x27;r&#x27;) as ft: src = fs.readlines() tgt = ft.readlines() ali = fa.readlines() length = len(src) for i in range(length): src_tokens.append(src[i].split()) tgt_tokens.append(tgt[i].split()) ali_mapping.append(ali[i].split()) with open(save_path, &quot;w&quot;) as f: for i in range(length): res = &quot;&quot; for k in range(len(ali_mapping[i])): src_index, tgt_index = ali_mapping[i][k].split(&#x27;-&#x27;) res += src_tokens[i][int(src_index)] + &quot;-&quot; + tgt_tokens[i][int(tgt_index)] + &quot; &quot; res += &quot;\\n&quot; f.write(res)recover(path=&quot;/data/wbxu/data&quot;) 1234567891011121314151617181920212223242526272829303132def recover(path): align_path = path + &quot;/wmt14_en-de_target-source.align&quot; src_path = path + &quot;/train.src&quot; tgt_path = path + &quot;/train.tgt&quot; save_path = path + &quot;/token_target-source.align&quot; src_tokens, tgt_tokens, ali_mapping = [], [], [] with open(align_path, &quot;r&quot;) as fa, open(src_path, &#x27;r&#x27;) as fs, open(tgt_path, &#x27;r&#x27;) as ft: src = fs.readlines() tgt = ft.readlines() ali = fa.readlines() length = len(src) for i in range(length): src_tokens.append(src[i].split()) tgt_tokens.append(tgt[i].split()) ali_mapping.append(ali[i].split()) with open(save_path, &quot;w&quot;) as f: for i in range(length): res = &quot;&quot; for k in range(len(ali_mapping[i])): src_index, tgt_index = ali_mapping[i][k].split(&#x27;-&#x27;) res += tgt_tokens[i][int(tgt_index)] + &quot;-&quot; + src_tokens[i][int(src_index)] + &quot; &quot; res += &quot;\\n&quot; f.write(res)recover(path=&quot;/data/wbxu/data&quot;) 转换成 tokens 对齐后, 子词间的对齐关系更加直观可见, 输出样例如下: 正向对齐 forward align 12I-Ich declare-erkläre the-die resumed-am Friday-Freitag the-dem 17-17. December-Dezember jour@@-unterbro@@ jour@@-ch@@ ned-ene ned-Sitzungsperiode ned-des European-Europäischen Parliament-Parlaments jour@@-für ned-wieder@@ resumed-aufgenommen ,-, wish-wünsche you-Ihnen again-nochmals wish-alles wish-Gute to-zum year-Jahres@@ year-wechsel and-und hope-hoffe that-, that-daß you-Sie pleasant-schöne pleasant-Ferien enjoyed-hatten .-. as-Wie you-Sie seen-feststellen seen-konnten ,-, the-der ed-ge@@ read@@-für@@ read@@-chtete &amp;apos;-&amp;quot; millenn@@-Mill@@ ium-en@@ ium-ium ium-@-@ bug-Bu@@ bug-g &amp;apos;-&amp;quot; failed-nicht materi@@-eingetreten .-. suffered-Doch people-Bürger number-einiger of-unserer countries-Mitgliedstaaten suffered-Opfer of-von read@@-schrecklichen disasters-Naturkatastrophen .-geworden .-. 逆向对齐 reverse align 12Ich-I erkläre-declare erkläre-resumed die-the erkläre-session die-of dem-the Europäischen-European Parlaments-Parliament unterbro@@-ad@@ unterbro@@-jour@@ unterbro@@-ned am-on Freitag-Friday 17.-17 Dezember-December aufgenommen-1999 ,-, und-and wünsche-I wünsche-would wünsche-like nochmals-once nochmals-again zum-to wünsche-wish Ihnen-you Gute-happy wechsel-new Jahres@@-year zum-the hoffe-hope daß-that Sie-you hatten-enjoyed schöne-a schöne-pleasant schöne-fes@@ .-. ist-Although ,-, Wie-as Sie-you Sie-will konnten-have feststellen-seen ,-, der-the für@@-d@@ für@@-read@@ chtete-ed &amp;quot;-&amp;apos; Mill@@-millenn@@ ium-ium Bu@@-bug &amp;quot;-&amp;apos; nicht-failed eingetreten-materi@@ eingetreten-alise Bürger-people einiger-number einiger-of Mitgliedstaaten-countries Opfer-suffered einiger-a von-series von-of Naturkatastrophen-natural Naturkatastrophen-disasters schrecklichen-were schrecklichen-d@@ schrecklichen-read@@ schrecklichen-ful .-. $\\mathfrak{GIZA++}$1. 从 Github 上下载 GIZA++ giza-pp 2. 通过以下命令编译 GIZA++12cd giza-pp-mastermake 编译完成之后, 我们所需要的就是以下4个文件: /data1/wbxu/giza-pp-master/GIZA++-v2/plain2snt.out /data1/wbxu/giza-pp-master/GIZA++-v2/snt2cooc.out /data1/wbxu/giza-pp-master/GIZA++-v2/GIZA++ /data1/wbxu/giza-pp-master/mkcls-v2/mkcls 3. 准备平行语料 我选择了没有 tokenize 的 WMT14 En-De 训练数据集： 英文样本如下： 12I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .Although , as you will have seen , the dreaded &amp;apos; millennium bug &amp;apos; failed to materialise , still the people in a number of countries suffered a series of natural disasters that truly were dreadful . 德语样本如下： 12Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .Wie Sie feststellen konnten , ist der gefürchtete &amp;quot; Millenium @-@ Bug &amp;quot; nicht eingetreten . Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden . 4. 为句子和单词进行编号1GIZA++-v2/plain2snt.out /data1/wbxu/data/en.txt /data1/wbxu/data/de.txt 该命令会生成4个文件：2个以vcb为后缀；2个以snt为后缀 Vcb 文件, 其形式为 id : token : count 英语词典1234567892 I 595964 3 declare 2140 4 resumed 1401 5 the 5940442 6 session 6792 7 of 3104652 8 European 301463 9 Parliament 117592 10 adjourned 582 德语词典1234567892 Ich 2251133 erkläre 8504 die 28863525 am 1166296 Freitag 27257 , 59130698 dem 4568219 17. 285610 Dezember 10338 Tip: 该文件为对应语言的词典文件, 其中 count 统计了对应单词的出现次数 Snt 文件, 其形式为 count $Enter$ ids $Enter$ ids $Enter$ 英德句对12345612 3 4 5 6 7 5 8 9 10 11 12 13 14 15 16 17 2 18 19 20 21 22 23 24 25 26 27 28 29 5 30 31 24 32 25 33 34 35 36 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 7 18 19 20 21 22 23 24 25 26 7 27 28 29 30 31 32 137 16 38 24 39 40 41 16 5 42 43 44 45 43 46 22 47 16 48 5 49 29 25 50 7 51 52 25 53 7 54 55 31 56 57 58 36 33 28 34 35 7 36 37 38 39 40 41 42 39 43 44 32 45 46 47 48 49 50 51 52 53 54 55 32 德英句对1234561 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 7 18 19 20 21 22 23 24 25 26 7 27 28 29 30 31 32 2 3 4 5 6 7 5 8 9 10 11 12 13 14 15 16 17 2 18 19 20 21 22 23 24 25 26 27 28 29 5 30 31 24 32 25 33 34 35 36 1 33 28 34 35 7 36 37 38 39 40 41 42 39 43 44 32 45 46 47 48 49 50 51 52 53 54 55 32 37 16 38 24 39 40 41 16 5 42 43 44 45 43 46 22 47 16 48 5 49 29 25 50 7 51 52 25 53 7 54 55 31 56 57 58 36 Tip: 该文件为 ids 化的句对文件, 第一行为对应句对的出现次数, 第二行为源语言句子, 第三行为目标语言句子 5. 生成共现文件12GIZA++-v2/snt2cooc.out /data1/wbxu/data/en.vcb /data1/wbxu/data/de.vcb /data1/wbxu/data/en_de.snt &gt; en_de.coocGIZA++-v2/snt2cooc.out /data1/wbxu/data/de.vcb /data1/wbxu/data/en.vcb /data1/wbxu/data/de_en.snt &gt; de_en.cooc 该命令会生成两个后缀为cooc的文件 (不清楚什么意思, 好像是一样的)： en_de.cooc 文件如下： 123456780 20 30 40 50 60 70 80 9 de_en.cooc 123456780 20 30 40 50 60 70 80 9 6. 生成词类12mkcls-v2/mkcls -p/data1/wbxu/data/en.txt -V/data1/wbxu/data/en.vcb.classes optmkcls-v2/mkcls -p/data1/wbxu/data/de.txt -V/data1/wbxu/data/de.vcb.classes opt 该命令的参数如下： -c 词类数目 -n 优化次数，默认是1，越大越好 -p 输入文件 -V 输出文件 opt 优化输出 该命令会生成4个文件, 2个以classes为后缀, 2个以cats为后缀： 英语词类： classes 定义单词到类别编号的映射12345678910&amp;apos;ALGAR 64 &amp;apos;ALSACE 64 &amp;apos;ALT 66 &amp;apos;ALZINA 64 &amp;apos;AMBRE 66 &amp;apos;AMORE 64 &amp;apos;AMOUR 66 &amp;apos;AN 66 &amp;apos;ANGELY 66 &amp;apos;ANNONCE 64 cats 定义类别编号到单词的映射1230:$, 1: 2:-2º,host,.ccf,0.01sein,0.6mm,04835,07541,09599,09pol,1,0l,1.033.856.359,1.037.162,1.062,1.114.112,1.116.513,1.150.000,1.199,1.215.000,1.465,1.536,1.685,1.711m,10,47,10,970,10.250,10.450,100,50,100ps,10ns,10x106,11,978,11.850,116,554,135,698,12,48,12.900,12353,1236ha,125gr,12621,128.000,12K,12º,13,05,13,60,13.250,13347,133º,134,15,13445,14,60,14,74,14,900,14.280,14.500.000,14fachem,15.267,150MHz,15m2,15tes,16,221,166.442,1694.,17,500,17.238,17.317.774,17m-,18,11,18.----in,185,000,187,888,187.000.000,19.220,191.400.000,1965.The,1986.,1KM,2.300m2,2.5mm,2.754,2.855,2.Hand.Das,20,04,20,67,20.026,20.685,200,000,000,2005im,20459,205,366,880,20KHz,20hektar,217m2,22.000km,22jähriger,22º,23.909,24ps,24x36mm,25,378km,25.500,25.700,256fachen,256x256px,25dB,25jährigem,2620m,27.445,27jähriger,28.400,2809,28219,285.782,295km,29sten,2Mbit,2ter,3.000.000.000,3.6Mbps,30,10,30.70,306,6,309.684,30M,31,72,3253,3299m,32MByte,32x16,32º,333.488,33428,335er,341.624,35390,36,500,000,38,4,390,000,39031,39035,39039,399,99,3DVIA,3Loch,3RA6,3x3x3m,4,664,4.694,40.972,400ml,40549,40jährigem,41.917,41qm,43.07079,43.700 德语词类： classes 定义单词到类别编号的映射123456789101112131415---BEMERKUNG 73---WICHTIG--- 20---en 94---too 30--AMDRY 32--Asma 48--Audio 26--Brasilianische 99--COMING 49--CZK 31--Der 76--Die 76--Diese 76--EINFÜGEN-- 32--Feld 91 cats 定义类别编号到单词的映射1230:$,1:2:-bresaola,-effectively,-may,-members,-nor,-oac,-overwhelming,-personal,-solutions,-to,-would,.will,10a.m.We,1152x864,125K,16.10.1996,18000th,1986s,1un,2.2rev4.1,250ms,251,66,300or,3Dembossments,3Sis,3beds,3pt,4.95m,61,870,7.1.3.3,7020.021,7040.021,7040.030,7041.060,81g,8HP,93.55.xxx.xxx,ALLREADY,AMSTeX,AVPDOS32,Abatec,Abraser,Acherkogl,Adiv,Aenor,Affinati,Aier,Alcar,Algarvios,America--can,Anting,Aow,ApplicationsTo,Aquadynamic,Aqualog,ArccOS,Arilines,Atheatos,Aufeis,Autobild.de,BCTCS,Baantai,Babae,Backgammonboard,Balabranip,Bananera,BaoLong,Bartimaeus,Basothen 7. 运行GIZA++12/data1/wbxu/giza-pp-master/GIZA++-v2/GIZA++ -S /data1/wbxu/data/en.vcb -T /data1/wbxu/data/de.vcb -C /data1/wbxu/data/en_de.snt -CoocurrenceFile /data1/wbxu/giza-pp-master/en_de.cooc -o en2de -OutputPath en2de/data1/wbxu/giza-pp-master/GIZA++-v2/GIZA++ -S /data1/wbxu/data/de.vcb -T /data1/wbxu/data/en.vcb -C /data1/wbxu/data/de_en.snt -CoocurrenceFile /data1/wbxu/giza-pp-master/de_en.cooc -o de2en -OutputPath de2en 该命令的参数如下： -S 源语言文件 -T 目标语言文件 -C src_tgt.snt文件 -CoocurrenceFile src_tgt.cooc文件 -o 输出文件的前缀 -OutputPath 输出文件的目录 该命令执行时间较长, 须待良久, 最终会生成如下文件： 输出日志文件 1234567891011==========================================================writing Final tables to Disk Writing PERPLEXITY report to: de2en/de2en.perpWriting source vocabulary list to : de2en/de2en.trn.src.vcbWriting source vocabulary list to : de2en/de2en.trn.trg.vcbWriting source vocabulary list to : de2en/de2en.tst.src.vcbWriting source vocabulary list to : de2en/de2en.tst.trg.vcbwriting decoder configuration file to de2en/de2en.Decoder.configEntire Training took: 99500 secondsProgram Finished at: Mon Nov 22 23:45:49 2021========================================================== 困惑度文件 ~.perp 123456789101112131415161718192021#trnsz tstsz iter model trn-pp test-pp trn-vit-pp tst-vit-pp3995262 0 0 Model1 1.63537e+06 N/A inf N/A3995262 0 1 Model1 567.258 N/A 3215.22 N/A3995262 0 2 Model1 242.121 N/A 860.049 N/A3995262 0 3 Model1 195.809 N/A 544.603 N/A3995262 0 4 Model1 183.124 N/A 443.58 N/A3995262 0 5 HMM 156.719 N/A inf N/A3995262 0 6 HMM 110.575 N/A 169.095 N/A3995262 0 7 HMM 78.7195 N/A 101.809 N/A3995262 0 8 HMM 68.6674 N/A 83.8846 N/A3995262 0 9 HMM 65.3209 N/A 77.9282 N/A3995262 0 10 THTo3 74.0646 N/A 80.3015 N/A3995262 0 11 Model3 184.549 N/A 198.633 N/A3995262 0 12 Model3 164.136 N/A 175.439 N/A3995262 0 13 Model3 157.36 N/A 167.888 N/A3995262 0 14 Model3 154.949 N/A 165.288 N/A3995262 0 15 T3To4 153.757 N/A 163.999 N/A3995262 0 16 Model4 101.187 N/A 106.708 N/A3995262 0 17 Model4 92.3756 N/A 97.1254 N/A3995262 0 18 Model4 89.1882 N/A 93.4744 N/A3995262 0 19 Model4 87.5094 N/A 91.4966 N/A ~d3.final 文件 (不知所云) 123456789101 1 100 1 11 2 100 1 11 3 100 1 11 4 100 1 11 5 100 1 11 6 100 1 11 7 100 1 11 0 100 2 0.7535832 0 100 2 0.2464171 1 100 2 0.948539 n3.final 文件表示的是某个单词所对应的繁殖力大小的概率 (从左到右，繁殖力从0到9)，其形式为 $source\\ id$ $p(fertility=0)$ $p(fertility=1)$ $\\dots$ $p(fertility=9)$ 123456789102 0.0835292 0.881598 0.0270334 0.00658778 0.00100327 0.000200634 4.27065e-05 2.70696e-07 4.87095e-06 1.14867e-07 3 0.178227 0.790122 0.0275572 0.00175264 0.0018591 0.000106401 8.37043e-05 6.1348e-05 3.65072e-05 0.000194441 4 0.0758428 0.201973 0.253675 0.0130023 0.452613 0.000283031 0.000939822 0.00132266 5.494e-05 0.000292524 5 0.401716 0.594518 0.000245277 0.00334565 0.000124138 2.99709e-05 3.1172e-06 1.49017e-05 0 3.0277e-06 6 0.328716 0.648779 0.0206393 0.00136076 0.000203803 0.000180117 2.69242e-05 1.97331e-05 1.17428e-05 6.25434e-05 7 0.729189 0.269479 0.000124212 0.00117522 2.06356e-05 1.28205e-06 9.66674e-06 7.59257e-07 0 0 8 0.105242 0.894326 0.000120709 0.000260014 3.93617e-05 7.70443e-06 7.42444e-07 5.79232e-07 3.33635e-07 1.80787e-06 9 0.0415899 0.883176 0.0729691 0.00195852 0.000284182 3.15497e-06 2.54355e-06 9.94552e-06 9.9275e-07 5.42298e-06 10 0.0534802 0.455451 0.436919 0.0427366 0.00281034 0.000736073 0.000359615 0.00348065 0.00315132 0.000875467 11 0.518013 0.481123 0.000105029 0.000745583 9.45342e-06 1.92139e-06 1.87431e-06 0 0 0 ~t3.final 文件(翻译表)表示的是对齐概率，其形式为 $source\\ id$ $target\\ id$ $p(target\\ id|source\\ id)$ 1234567891011121314&gt;0 2 8.71379e-070 4 0.07727310 5 0.001774540 6 1.23743e-070 7 0.2158340 8 0.009932770 10 1.4587e-070 13 0.01017450 14 3.58152e-060 15 2.81992e-060 16 0.01415340 18 6.23962e-070 19 0.0005449380 20 1.49871e-06 ~A3.final 文件表示单向对齐文件，数字代表了单词在句中的位置 123456789#Sentence pair (1) source length 40 target length 33 alignment score : 2.17771e-70Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten . NULL (&#123; 3 6 7 15 17 23 25 27 &#125;) I (&#123; 1 &#125;) declare (&#123; 2 &#125;) resumed (&#123; 16 &#125;) the (&#123; &#125;) session (&#123; 11 &#125;) of (&#123; 12 &#125;) the (&#123; &#125;) European (&#123; 13 &#125;) Parliament (&#123; 14 &#125;) adjourned (&#123; 10 &#125;) on (&#123; 4 &#125;) Friday (&#123; 5 &#125;) 17 (&#123; 8 &#125;) December (&#123; 9 &#125;) 1999 (&#123; &#125;) , (&#123; &#125;) and (&#123; &#125;) I (&#123; &#125;) would (&#123; &#125;) like (&#123; &#125;) once (&#123; &#125;) again (&#123; 20 &#125;) to (&#123; &#125;) wish (&#123; 18 21 22 &#125;) you (&#123; 19 &#125;) a (&#123; &#125;) happy (&#123; &#125;) new (&#123; &#125;) year (&#123; 24 &#125;) in (&#123; &#125;) the (&#123; &#125;) hope (&#123; 26 &#125;) that (&#123; 28 &#125;) you (&#123; 29 &#125;) enjoyed (&#123; 32 &#125;) a (&#123; &#125;) pleasant (&#123; 30 &#125;) festive (&#123; 31 &#125;) period (&#123; &#125;) . (&#123; 33 &#125;) #Sentence pair (2) source length 37 target length 28 alignment score : 2.3304e-64Wie Sie feststellen konnten , ist der gefürchtete &amp;quot; Millenium @-@ Bug &amp;quot; nicht eingetreten . Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden . NULL (&#123; 6 11 16 18 24 &#125;) Although (&#123; 1 &#125;) , (&#123; &#125;) as (&#123; &#125;) you (&#123; 2 &#125;) will (&#123; &#125;) have (&#123; 4 &#125;) seen (&#123; 3 &#125;) , (&#123; 5 &#125;) the (&#123; 7 &#125;) dreaded (&#123; 8 &#125;) &amp;apos; (&#123; 9 &#125;) millennium (&#123; 10 &#125;) bug (&#123; 12 &#125;) &amp;apos; (&#123; 13 &#125;) failed (&#123; 14 &#125;) to (&#123; &#125;) materialise (&#123; 15 17 &#125;) , (&#123; &#125;) still (&#123; &#125;) the (&#123; &#125;) people (&#123; 19 &#125;) in (&#123; &#125;) a (&#123; &#125;) number (&#123; 20 &#125;) of (&#123; 21 &#125;) countries (&#123; 22 &#125;) suffered (&#123; 23 27 &#125;) a (&#123; &#125;) series (&#123; &#125;) of (&#123; &#125;) natural (&#123; &#125;) disasters (&#123; 26 &#125;) that (&#123; &#125;) truly (&#123; &#125;) were (&#123; &#125;) dreadful (&#123; 25 &#125;) . (&#123; 28 &#125;) #Sentence pair (3) source length 23 target length 17 alignment score : 7.25555e-37Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen . NULL (&#123; 3 4 6 &#125;) You (&#123; 1 &#125;) have (&#123; &#125;) requested (&#123; 2 5 &#125;) a (&#123; 7 &#125;) debate (&#123; 8 &#125;) on (&#123; 9 &#125;) this (&#123; &#125;) subject (&#123; &#125;) in (&#123; 13 &#125;) the (&#123; &#125;) course (&#123; &#125;) of (&#123; &#125;) the (&#123; 14 &#125;) next (&#123; 15 &#125;) few (&#123; &#125;) days (&#123; 16 &#125;) , (&#123; &#125;) during (&#123; 10 &#125;) this (&#123; 11 &#125;) part (&#123; &#125;) @-@ (&#123; &#125;) session (&#123; 12 &#125;) . (&#123; 17 &#125;) ~d4.final 文件是 IBM Model 4 翻译表 1234567891011#Translation tables for Model 4 .#Table for head of cept.F: 69 E: 0 SUM: 3109.6 1 2840.242 106.6663 31.57454 11.46715 13.09116 8.40917 7.46996 ~gizacfg 文件是 GIZA++ 的配置文件，包括一些超参数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192adbackoff 0c /data1/wbxu/data/en_de.sntcompactadtable 1compactalignmentformat 0coocurrencefile /data1/wbxu/giza-pp-master/en_de.cooccorpusfile /data1/wbxu/data/en_de.sntcountcutoff 1e-06countcutoffal 1e-05countincreasecutoff 1e-06countincreasecutoffal 1e-05d deficientdistortionforemptyword 0depm4 76depm5 68dictionary dopeggingyn 0emalignmentdependencies 2emalsmooth 0.2emprobforempty 0.4emsmoothhmm 2hmmdumpfrequency 0hmmiterations 5l en2de/2021-11-20.130008.wbxu.loglog 0logfile en2de/2021-11-20.130008.wbxu.logm1 5m2 0m3 5m4 5m5 0m5p0 -1m6 0manlexfactor1 0manlexfactor2 0manlexmaxmultiplicity 20maxfertility 10maxsentencelength 101mh 5mincountincrease 1e-07ml 101model1dumpfrequency 0model1iterations 5model23smoothfactor 0model2dumpfrequency 0model2iterations 0model345dumpfrequency 0model3dumpfrequency 0model3iterations 5model4iterations 5model4smoothfactor 0.2model5iterations 0model5smoothfactor 0.1model6iterations 0nbestalignments 0nodumps 0nofiledumpsyn 0noiterationsmodel1 5noiterationsmodel2 0noiterationsmodel3 5noiterationsmodel4 5noiterationsmodel5 0noiterationsmodel6 0nsmooth 64nsmoothgeneral 0numberofiterationsforhmmalignmentmodel 5o en2de/en2deonlyaldumps 0outputfileprefix en2de/en2deoutputpath en2de/p 0p0 -1peggedcutoff 0.03pegging 0probcutoff 1e-07probsmooth 1e-07readtableprefix s /data1/wbxu/data/en.vcbsourcevocabularyfile /data1/wbxu/data/en.vcbt /data1/wbxu/data/de.vcbt1 0t2 0t2to3 0t3 0t345 0targetvocabularyfile /data1/wbxu/data/de.vcbtc testcorpusfile th 0transferdumpfrequency 0v 0verbose 0verbosesentence -10 ~Decoder.config 文件用于 ISI Rewrite Decoder 1234567891011TTable = en2de.t3.finalInverseTTable = en2de.ti.finalNTable = en2de.n3.finalD3Table = en2de.d3.finalD4Table = en2de.D4.finalPZero = en2de.p0_3.finalSource.vcb = /data1/wbxu/data/en.vcbTarget.vcb = /data1/wbxu/data/de.vcbSource.classes = /data1/wbxu/data/en.vcb.classesTarget.classes = /data1/wbxu/data/de.vcb.classesFZeroWords = en2de.fe0_3.final $\\mathfrak{Thought}$1. fast align1.1 检查句内对齐关系 Source-Target 对齐样例 123456I-Ich declare-erkläre the-die resumed-am Friday-Freitag the-dem 17-17. December-Dezember jour@@-unterbro@@ jour@@-ch@@ ned-ene ned-Sitzungsperiode ned-des European-Europäischen Parliament-Parlaments jour@@-für ned-wieder@@ resumed-aufgenommen ,-, wish-wünsche you-Ihnen again-nochmals wish-alles wish-Gute to-zum year-Jahres@@ year-wechsel and-und hope-hoffe that-, that-daß you-Sie pleasant-schöne pleasant-Ferien enjoyed-hatten .-. as-Wie you-Sie seen-feststellen seen-konnten ,-, the-der ed-ge@@ read@@-für@@ read@@-chtete &amp;apos;-&amp;quot; millenn@@-Mill@@ ium-en@@ ium-ium ium-@-@ bug-Bu@@ bug-g &amp;apos;-&amp;quot; failed-nicht materi@@-eingetreten .-. suffered-Doch people-Bürger number-einiger of-unserer countries-Mitgliedstaaten suffered-Opfer of-von read@@-schrecklichen disasters-Naturkatastrophen .-geworden .-. You-Im requested-Parlament a-besteht on-der requested-Wunsch subject-nach a-einer debate-Aussprache in-im course-Verlauf this-dieser session-Sitzungsperiode during-in during-den next-nächsten days-Tagen .-. In-Heute like-möchte I-ich like-Sie like-bitten ,-- the-das a-ist as-auch of-der requested-Wunsch number-einiger Members-Kolleginnen Members-Kollegen ,-, all-allen victims-Opfern of-der stor@@-St@@ stor@@-ür@@ stor@@-me ,-, particularly-insbesondere in-in the-den various-verschiedenen countries-Ländern of-der European-Europäischen Union-Union ,-, in-in of-einer silence-Schwei@@ silence-minute victims-denken .-. Please-Ich Please-bitte Please-Sie ,-, ,-sich for-zu minute-einer silence-Schwei@@ silence-ge@@ silence-minute s-zu silence-erheben .-. (-( The-Das House-Parlament rose-erhebt rose-sich observed-zu a-einer silence-Schwei@@ silence-ge@@ silence-minute )-. )-) Target-Source 对齐样例 123456Ich-I erkläre-declare erkläre-resumed die-the erkläre-session die-of dem-the Europäischen-European Parlaments-Parliament unterbro@@-ad@@ unterbro@@-jour@@ unterbro@@-ned am-on Freitag-Friday 17.-17 Dezember-December aufgenommen-1999 ,-, und-and wünsche-I wünsche-would wünsche-like nochmals-once nochmals-again zum-to wünsche-wish Ihnen-you Gute-happy wechsel-new Jahres@@-year zum-the hoffe-hope daß-that Sie-you hatten-enjoyed schöne-a schöne-pleasant schöne-fes@@ .-. ist-Although ,-, Wie-as Sie-you Sie-will konnten-have feststellen-seen ,-, der-the für@@-d@@ für@@-read@@ chtete-ed &amp;quot;-&amp;apos; Mill@@-millenn@@ ium-ium Bu@@-bug &amp;quot;-&amp;apos; nicht-failed eingetreten-materi@@ eingetreten-alise Bürger-people einiger-number einiger-of Mitgliedstaaten-countries Opfer-suffered einiger-a von-series von-of Naturkatastrophen-natural Naturkatastrophen-disasters schrecklichen-were schrecklichen-d@@ schrecklichen-read@@ schrecklichen-ful .-. Im-You Parlament-have Wunsch-requested einer-a Aussprache-debate der-on dieser-this nach-subject im-in im-the Verlauf-course im-of im-the nächsten-next Tagen-few Tagen-days in-, Verlauf-during dieser-this Sitzungsperiode-part Sitzungsperiode-@-@ Sitzungsperiode-session .-. Heute-In das-the ich-meantime Heute-, ich-I möchte-should möchte-like bitten-to minute-observe ist-a minute-minute der-&amp;apos; der-s Schwei@@-silence Kolleginnen-, auch-as einiger-number der-of Kollegen-Members ,-have Wunsch-requested ,-, der-on Opfern-behalf der-of allen-all den-the Opfern-victims insbesondere-concerned ,-, insbesondere-particularly Ländern-those der-of der-the Opfern-terrible ür@@-stor@@ me-ms ,-, in-in der-the verschiedenen-various Ländern-countries einer-of zu-the Europäischen-European Union-Union .-. bitte-Please erheben-rise ,-, ,-then ,-, zu-for zu-this minute-minute minute-&amp;apos; minute-s minute-silence .-. (-( Das-The Parlament-House erhebt-rose sich-and erhebt-observed einer-a minute-minute minute-&amp;apos; minute-s minute-silence )-) 接下来，我有如下的几个发现: fast align 能够区分单词的大小写变化ich-I &amp; Ich-I ...; fast align 构造出的大部分都是一对一的对齐映射关系I-Ich declare-erkläre ...; fast align 是具备了一定的处理 tokenized 数据间对齐的能力silence-Schwei@@ &amp; silence-ge@@ &amp; silence-minute ...; fast align 所支持的正逆向对齐关系之间无法完全对应起来对应: I-Ich &amp; Ich-I ..., 不对应: rose-sich &amp; sich-and ...; fast align 处理 BPE 化的数据时, 仍然存在着问题quota-Quot@@ &amp; quota-en@@ ...; 1.2 检查语料库上的对齐关系编写如下程序： 12345678910111213141516171819202122232425262728293031def mapping(file, output): text = [] with open(file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f: content = f.readlines() length = len(content) for i in range(length): text.append(content[i].split()) mappings = &#123;&#125; for i in range(length): for j in range(len(text[i])): for k in range(len(text[i][j])): if text[i][j][k] == &quot;-&quot;: key = text[i][j][:k] value = text[i][j][k+1:] break if key not in mappings.keys(): mappings[key] = [value, ] elif value not in mappings[key]: mappings[key].append(value) with open(output, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f: for key, value in mappings.items(): f.write(str(key) + str(value) + &quot;\\n&quot;)mapping(file=&quot;token_source-target.align&quot;, output=&quot;token_source-target.txt&quot;)mapping(file=&quot;token_target-source.align&quot;, output=&quot;token_target-source.txt&quot;) 英语单词的对齐样例 12345I[&#x27;Ich&#x27;, &#x27;ich&#x27;, &#x27;mir&#x27;, &#x27;bin&#x27;, &#x27;Ihnen&#x27;, &#x27;Das&#x27;, &#x27;meiner&#x27;, &#x27;mich&#x27;, &#x27;Mir&#x27;, &#x27;halte&#x27;, &#x27;will&#x27;, &#x27;wünsche&#x27;, &#x27;habe&#x27;, &#x27;betrachte&#x27;, &#x27;daß&#x27;, &#x27;meine&#x27;, &#x27;meinen&#x27;, &#x27;Wie&#x27;, &#x27;möchte&#x27;, &#x27;tze&#x27;, &#x27;gehe&#x27;, &#x27;war&#x27;, &#x27;scheint&#x27;, &#x27;meinem&#x27;, &#x27;Dafürhalten&#x27;, &#x27;verstehe&#x27;, &#x27;hiermit&#x27;, &#x27;e&#x27;, &#x27;Hier&#x27;, &#x27;Den&#x27;, &#x27;br@@&#x27;, &#x27;inge&#x27;, &#x27;mache&#x27;, &#x27;stelle&#x27;, &#x27;darf&#x27;, &#x27;mein&#x27;, &#x27;Auch&#x27;, &#x27;sehe&#x27;, &#x27;Deshalb&#x27;, &#x27;Und&#x27;, &#x27;spreche&#x27;, &#x27;erlau@@&#x27;, &#x27;be&#x27;, &#x27;Außerdem&#x27;, &#x27;Was&#x27;, &#x27;In&#x27;, &#x27;kann&#x27;, &#x27;begrüße&#x27;, &#x27;Punkt&#x27;, &#x27;Darin&#x27;, &#x27;FR&#x27;, &#x27;sagen&#x27;, &#x27;Im&#x27;, &#x27;Da&#x27;, &#x27;Lassen&#x27;, &#x27;Meine&#x27;, &#x27;Damen&#x27;, &#x27;werde&#x27;, &#x27;komme&#x27;, &#x27;bitte&#x27;, &#x27;Gestatten&#x27;, &#x27;bedaure&#x27;, &#x27;stimme&#x27;, &#x27;lege&#x27;, &#x27;,&#x27;, &#x27;meines&#x27;, &#x27;Auf&#x27;, &#x27;nehme&#x27;, &#x27;finde&#x27;, &#x27;Dazu&#x27;, &#x27;Dabei&#x27;, &#x27;vertrete&#x27;, &#x27;Hier@@&#x27;, &#x27;danke&#x27;, &#x27;Ansicht&#x27;, &#x27;Darauf&#x27;, &#x27;aber&#x27;, &#x27;wollte&#x27;, &#x27;weise&#x27;, &#x27;Für&#x27;, &#x27;Mit&#x27;, &#x27;Meinung&#x27;, &#x27;Noch&#x27;, &#x27;Plenum&#x27;, &#x27;Nichts@@&#x27;, &#x27;diesem&#x27;, &#x27;jetzt&#x27;, &#x27;deswegen&#x27;, &#x27;Dies&#x27;, &#x27;befürworte&#x27;, &#x27;Leider&#x27;, &#x27;erkläre&#x27;, &#x27;Deswegen&#x27;, &#x27;hier&#x27;, &#x27;Ganz&#x27;, &#x27;muß&#x27;, &#x27;Als&#x27;, &#x27;I&#x27;, &#x27;Es&#x27;, &#x27;Meiner&#x27;, &#x27;gestatten&#x27;, &#x27;betone&#x27;, &#x27;erinnere&#x27;, &#x27;Herr&#x27;, &#x27;Herrn&#x27;, &#x27;teile&#x27;, &#x27;Meines&#x27;, &#x27;Erachtens&#x27;, &#x27;Damit&#x27;, &#x27;Darüber&#x27;, &#x27;Sitzung&#x27;, &#x27;Mehr&#x27;, &#x27;Präsident&#x27;, &#x27;sage&#x27;, &#x27;grei@@&#x27;, &#x27;schließe&#x27;, &#x27;M@@&#x27;, &#x27;freue&#x27;, &#x27;An&#x27;, &#x27;diesen&#x27;, &#x27;erwäh@@&#x27;, &#x27;glaube&#x27;, &#x27;Sehr&#x27;, &#x27;Vor&#x27;, &#x27;folgendes&#x27;, &#x27;gebe&#x27;, &#x27;Gerade&#x27;, &#x27;Doch&#x27;, &#x27;Mein&#x27;, &#x27;nochmals&#x27;, &#x27;einmal&#x27;, &#x27;liebe&#x27;, &#x27;unterstütze&#x27;, &#x27;Von&#x27;, &#x27;Zwar&#x27;, &#x27;zitiere&#x27;, &#x27;Wenn&#x27;, &#x27;Zu&#x27;, &#x27;Dies@@&#x27;, &#x27;hoffe&#x27;, &#x27;wage&#x27;, &#x27;stehe&#x27;, &#x27;hinweisen&#x27;, &#x27;Dennoch&#x27;, &#x27;Ausführungen&#x27;, &#x27;würde&#x27;, &#x27;denke&#x27;, &#x27;Obwohl&#x27;, &#x27;bekräf@@&#x27;, &#x27;verur@@&#x27;, &#x27;EN&#x27;, &#x27;Eigentlich&#x27;, &#x27;übrigen&#x27;, &#x27;Bei&#x27;, &#x27;Diese&#x27;, &#x27;sehr&#x27;, &#x27;Davon&#x27;, &#x27;Abgeordneter&#x27;, &#x27;wiederhole&#x27;, &#x27;Frau&#x27;, &#x27;daher&#x27;, &#x27;davon&#x27;, &#x27;mitteilen&#x27;, &#x27;persönlich&#x27;, &#x27;weiß&#x27;, &#x27;Aus&#x27;, &#x27;Zur&#x27;, &#x27;Gelegenheit&#x27;, &#x27;höre&#x27;, &#x27;trage&#x27;, &#x27;achte&#x27;, &#x27;fühle&#x27;, &#x27;aufmerksam&#x27;, &#x27;äuß@@&#x27;, &#x27;Insbesondere&#x27;, &#x27;Abgeordnete&#x27;, &#x27;wende&#x27;, &#x27;Aber&#x27;, &#x27;Zum&#x27;, &#x27;frage&#x27;, &#x27;Warum&#x27;, &#x27;ausdrücklich&#x27;, &#x27;verwende&#x27;, &#x27;unterstreichen&#x27;, &#x27;erkt&#x27;, &#x27;Wohl&#x27;, &#x27;fordere&#x27;, &#x27;erwarte&#x27;, &#x27;Ferner&#x27;, &#x27;schätze&#x27;, &#x27;suche&#x27;, &#x27;Äußerungen&#x27;, &#x27;hinzufügen&#x27;, &#x27;Weiterhin&#x27;, &#x27;gern&#x27;, &#x27;Angesichts&#x27;, &#x27;Soweit&#x27;, &#x27;beziehe&#x27;, &#x27;bewußt&#x27;, &#x27;Nun&#x27;, &#x27;fürchte&#x27;, &#x27;Bemerkung&#x27;, &#x27;Überzeugung&#x27;, &#x27;soeben&#x27;, &#x27;Daher&#x27;, &#x27;hätte&#x27;, &#x27;betrifft&#x27;, &#x27;darüber&#x27;, &#x27;soviel&#x27;, &#x27;Standpunkt&#x27;, &#x27;Abschließend&#x27;, &#x27;Des@@&#x27;, &#x27;vertrau@@&#x27;, &#x27;hatte&#x27;, &#x27;Zuerst&#x27;, &#x27;Daß&#x27;, &#x27;Zusammenhang&#x27;, &#x27;Persön@@&#x27;, &#x27;darauf&#x27;, &#x27;Damals&#x27;, &#x27;schlage&#x27;, &#x27;dankbar&#x27;, &#x27;erstaunt&#x27;, &#x27;Eine&#x27;, &#x27;Also&#x27;, &#x27;gehöre&#x27;, &#x27;kurz&#x27;, &#x27;Präsidentin&#x27;, &#x27;beg@@&#x27;, &#x27;inne&#x27;, &#x27;Vorredner&#x27;, &#x27;Worte&#x27;, &#x27;versichern&#x27;, &#x27;weshalb&#x27;, &#x27;Kollege&#x27;, &#x27;wohl&#x27;, &#x27;hne&#x27;, &#x27;Natürlich&#x27;, &#x27;fe&#x27;, &#x27;Dem&#x27;, &#x27;Sicht&#x27;, &#x27;Obgleich&#x27;, &#x27;Entschließung&#x27;, &#x27;Kollegen&#x27;, &#x27;Parlaments@@&#x27;, &#x27;Wissens&#x27;, &#x27;ßte&#x27;, &#x27;Nach&#x27;, &#x27;Zunächst&#x27;, &#x27;ha@@&#x27;, &#x27;Diesen&#x27;, &#x27;Auffassung&#x27;, &#x27;Dann&#x27;, &#x27;Wort@@&#x27;, &#x27;bean@@&#x27;, &#x27;Sicherlich&#x27;, &#x27;Ja&#x27;, &#x27;lasse&#x27;, &#x27;gesagt&#x27;, &#x27;Zudem&#x27;, &#x27;Herren&#x27;, &#x27;enne&#x27;, &#x27;Besonders&#x27;, &#x27;So@@&#x27;, &#x27;Hierzu&#x27;, &#x27;Dafür&#x27;, &#x27;Da@@&#x27;, &#x27;jedenfalls&#x27;, &#x27;Insofern&#x27;, &#x27;tu@@&#x27;, &#x27;anerk@@&#x27;, &#x27;anmerken&#x27;, &#x27;bemerken&#x27;, &#x27;schon&#x27;, &#x27;meldung&#x27;, &#x27;bedauere&#x27;, &#x27;einerseits&#x27;, &#x27;ere&#x27;, &#x27;brau@@&#x27;, &#x27;Allerdings&#x27;, &#x27;Frage&#x27;, &#x27;kenne&#x27;, &#x27;geehrte&#x27;, &#x27;Einer&#x27;, &#x27;abschließend&#x27;, &#x27;betonen&#x27;, &#x27;Ausgehend&#x27;, &#x27;Vielleicht&#x27;, &#x27;einverstanden&#x27;, &#x27;Rede@@&#x27;, &#x27;äußern&#x27;, &#x27;Vielen&#x27;, &#x27;Stelle&#x27;, &#x27;gestimmt&#x27;, &#x27;Des&#x27;, &#x27;rede&#x27;, &#x27;Eines&#x27;, &#x27;dachte&#x27;, &#x27;deshalb&#x27;, &#x27;Daran&#x27;, &#x27;pflich@@&#x27;, &#x27;Trotzdem&#x27;, &#x27;Änderungsanträgen&#x27;, &#x27;Kommissarin&#x27;, &#x27;erle@@&#x27;, &#x27;Vorbehalte&#x27;, &#x27;Bevor&#x27;, &#x27;hoffentlich&#x27;, &#x27;beglückwünsche&#x27;, &#x27;Schließlich&#x27;, &#x27;Zweitens&#x27;, &#x27;spare&#x27;, &#x27;Kommissionsmitglied&#x27;, &#x27;Abgeordneten&#x27;, &#x27;mag&#x27;, &#x27;aufrichtig&#x27;, &#x27;Kolleginnen&#x27;, &#x27;Sitzungs@@&#x27;, &#x27;Bedenken&#x27;, &#x27;hervorheben&#x27;, &#x27;erläu@@&#x27;, &#x27;Verständnis&#x27;, &#x27;he@@&#x27;, &#x27;feststellen&#x27;, &#x27;Hierbei&#x27;, &#x27;hierher&#x27;, &#x27;Um&#x27;, &#x27;Einen&#x27;, &#x27;mußte&#x27;, &#x27;verfol@@&#x27;, &#x27;Hinsichtlich&#x27;, &#x27;Genugtuung&#x27;, &#x27;wofür&#x27;, &#x27;danken&#x27;, &#x27;iere&#x27;, &#x27;bemerkt&#x27;, &#x27;behaup@@&#x27;, &#x27;diesbezüglich&#x27;, &#x27;Gleichwohl&#x27;, &#x27;gerne&#x27;, &#x27;muss&#x27;, &#x27;Weiteren&#x27;, &#x27;Bemerkungen&#x27;, &#x27;dass&#x27;, &#x27;betei@@&#x27;, &#x27;Einschätzung&#x27;, &#x27;Jetzt&#x27;, &#x27;Wort&#x27;, &#x27;Über&#x27;, &#x27;Wahrschein@@&#x27;, &#x27;Zugleich&#x27;, &#x27;respe@@&#x27;, &#x27;bekomm@@&#x27;, &#x27;Darum&#x27;, &#x27;ergrei@@&#x27;, &#x27;So&#x27;, &#x27;bestä@@&#x27;, &#x27;eingehen&#x27;, &#x27;Antwort&#x27;, &#x27;appelliere&#x27;, &#x27;lehne&#x27;, &#x27;akzep@@&#x27;, &#x27;tiere&#x27;, &#x27;Übrigen&#x27;, &#x27;beitrag&#x27;, &#x27;Dieser&#x27;, &#x27;Kön@@&#x27;, &#x27;Argumente&#x27;, &#x27;Dass&#x27;, &#x27;Je@@&#x27;, &#x27;denfalls&#x27;, &#x27;zweif@@&#x27;, &#x27;musste&#x27;, &#x27;letztes&#x27;, &#x27;Nein&#x27;, &#x27;empfehle&#x27;, &#x27;arbeit@@&#x27;, &#x27;erwähnen&#x27;, &#x27;erfahr@@&#x27;, &#x27;Ratspräsident&#x27;, &#x27;konnte&#x27;, &#x27;Ebenso&#x27;, &#x27;!&#x27;, &#x27;bezüglich&#x27;, &#x27;Heute&#x27;, &#x27;Bericht&#x27;, &#x27;anwesend&#x27;, &#x27;sagte&#x27;, &#x27;wiederholen&#x27;, &#x27;zunächst&#x27;, &#x27;beur@@&#x27;, &#x27;Folgendes&#x27;, &#x27;blei@@&#x27;, &#x27;außerordentlich&#x27;, &#x27;Kommissar&#x27;, &#x27;Dar@@&#x27;, &#x27;zäh@@&#x27;, &#x27;Berichterstatters&#x27;, &#x27;pfe&#x27;, &#x27;SV&#x27;, &#x27;Hoff@@&#x27;, &#x27;verehrte&#x27;, &#x27;einzugehen&#x27;, &#x27;Dieses&#x27;, &#x27;be@@&#x27;, &#x27;Welche&#x27;, &#x27;Dank&#x27;, &#x27;da&#x27;, &#x27;Andererseits&#x27;, &#x27;NL&#x27;, &#x27;Kann&#x27;, &#x27;beobach@@&#x27;, &#x27;gratuliere&#x27;, &#x27;rufe&#x27;, &#x27;Rat&#x27;, &#x27;vermutlich&#x27;, &#x27;Mitteilung&#x27;, &#x27;durfte&#x27;, &#x27;ausdrücken&#x27;, &#x27;Alles&#x27;, &#x27;vorliegenden&#x27;, &#x27;nachdrücklich&#x27;, &#x27;amtierenden&#x27;, &#x27;Ratspräsidenten&#x27;, &#x27;Hin@@&#x27;, &#x27;Zustimmung&#x27;, &#x27;übrigens&#x27;, &#x27;befür@@&#x27;, &#x27;chte&#x27;, &#x27;Ansonsten&#x27;, &#x27;erfreut&#x27;, &#x27;Ausdruck&#x27;, &#x27;Hinweis&#x27;, &#x27;Wertsch@@&#x27;, &#x27;klarstellen&#x27;, &#x27;rechn@@&#x27;, &#x27;-&#x27;, &#x27;kriti@@&#x27;, &#x27;Erwä@@&#x27;, &#x27;Kenntnis&#x27;, &#x27;Sicher&#x27;, &#x27;dies&#x27;, &#x27;Verheugen&#x27;, &#x27;chmals&#x27;, &#x27;Bitte&#x27;, &#x27;erinnern&#x27;, &#x27;anbelangt&#x27;, &#x27;daran&#x27;, &#x27;Selbstverständlich&#x27;, &#x27;anführen&#x27;, &#x27;Berichterstatter&#x27;, &#x27;Aussage&#x27;, &#x27;Ratspräsidentin&#x27;, &#x27;Ehr@@&#x27;, &#x27;Denn&#x27;, &#x27;wirklich&#x27;, &#x27;Präsidium&#x27;, &#x27;Unabhängi@@&#x27;, &#x27;Berichterstatterin&#x27;, &#x27;Nicht&#x27;, &#x27;Letz@@&#x27;, &#x27;Äuß@@&#x27;, &#x27;Anmerkung&#x27;, &#x27;Weil&#x27;, &#x27;erläutern&#x27;, &#x27;Liikanen&#x27;, &#x27;zutiefst&#x27;, &#x27;fas@@&#x27;, &#x27;itze&#x27;, &#x27;Punkte&#x27;, &#x27;versuche&#x27;, &#x27;füge&#x27;, &#x27;entschei@@&#x27;, &#x27;doch&#x27;, &#x27;noch&#x27;, &#x27;Aussprache&#x27;, &#x27;reiche&#x27;, &#x27;eigentlich&#x27;, &#x27;Anmerkungen&#x27;, &#x27;Moment&#x27;, &#x27;fand&#x27;, &#x27;drücklich&#x27;, &#x27;Vorschlag&#x27;, &#x27;hre&#x27;, &#x27;Worten&#x27;, &#x27;Lob&#x27;, &#x27;zuversichtlich&#x27;, &#x27;Zig@@&#x27;, &#x27;hole&#x27;, &#x27;antwor@@&#x27;, &#x27;liere&#x27;, &#x27;wissen&#x27;, &#x27;Appell&#x27;, &#x27;plä@@&#x27;, &#x27;bereits&#x27;, &#x27;überzeugt&#x27;, &#x27;schriftlich&#x27;, &#x27;Ein&#x27;, &#x27;entschuldigen&#x27;, &#x27;tre@@&#x27;, &#x27;Mitglied&#x27;, &#x27;Thema&#x27;, &#x27;ne&#x27;, &#x27;Verstän@@&#x27;, &#x27;verehrten&#x27;, &#x27;vermis@@&#x27;, &#x27;kunde&#x27;, &#x27;Barón&#x27;, &#x27;worten&#x27;, &#x27;froh&#x27;, &#x27;gefällt&#x27;, &#x27;auch&#x27;, &#x27;Bedauerlicherweise&#x27;, &#x27;Hohen&#x27;, &#x27;Vielmehr&#x27;, &#x27;bemü@@&#x27;, &#x27;wäre&#x27;, &#x27;benutz@@&#x27;, &#x27;schrei@@&#x27;, &#x27;vermu@@&#x27;, &#x27;Rede&#x27;, &#x27;ehrlich&#x27;, &#x27;Mein@@&#x27;, &#x27;messe&#x27;, &#x27;herz@@&#x27;, &#x27;Sinne&#x27;, &#x27;ganz&#x27;, &#x27;wünschte&#x27;, &#x27;Erstens&#x27;, &#x27;Beim&#x27;, &#x27;DA&#x27;, &#x27;Bezüglich&#x27;, &#x27;etwas&#x27;, &#x27;durchaus&#x27;, &#x27;verweisen&#x27;, &#x27;Fraktion&#x27;, &#x27;gangs&#x27;, &#x27;bitten&#x27;, &#x27;Aufgrund&#x27;, &#x27;Weiter&#x27;, &#x27;Gemeinsam&#x27;, &#x27;Gestern&#x27;, &#x27;Ausschusses&#x27;, &#x27;Gleichzeitig&#x27;, &#x27;Kommissars&#x27;, &#x27;zieh@@&#x27;, &#x27;so&#x27;, &#x27;letzt&#x27;, &#x27;te&#x27;, &#x27;Fraktionen&#x27;, &#x27;amtierender&#x27;, &#x27;fest&#x27;, &#x27;was&#x27;, &#x27;Hoffnung&#x27;, &#x27;weil&#x27;, &#x27;Eindruck&#x27;, &#x27;einige&#x27;, &#x27;antworten&#x27;, &#x27;dringlich&#x27;, &#x27;freut&#x27;, &#x27;berichten&#x27;, &#x27;Frattini&#x27;, &#x27;emp@@&#x27;, &#x27;Nur&#x27;, &#x27;Ist&#x27;, &#x27;Einwanderungspolitik&#x27;, &#x27;entlich&#x27;, &#x27;wus@@&#x27;, &#x27;beglückwünschen&#x27;, &#x27;Liebe&#x27;, &#x27;allerdings&#x27;, &#x27;beitra@@&#x27;, &#x27;Hab@@&#x27;, &#x27;soweit&#x27;, &#x27;damals&#x27;, &#x27;Moment@@&#x27;, &#x27;angeht&#x27;, &#x27;vorhin&#x27;, &#x27;verstanden&#x27;, &#x27;gefallen&#x27;, &#x27;Mandelson&#x27;, &#x27;Wor@@&#x27;, &#x27;Era@@&#x27;, &#x27;grü@@&#x27;, &#x27;Grunde&#x27;, &#x27;Einwände&#x27;, &#x27;Vertreterin&#x27;, &#x27;erklären&#x27;, &#x27;Werde&#x27;, &#x27;überein&#x27;, &#x27;erwähnte&#x27;, &#x27;Schluss&#x27;, &#x27;spiele&#x27;, &#x27;geehrter&#x27;, &#x27;sicher&#x27;, &#x27;zustimmen&#x27;, &#x27;Debatte&#x27;, &#x27;worüber&#x27;, &#x27;ermaßen&#x27;, &#x27;das&#x27;, &#x27;Übri@@&#x27;, &#x27;Vizepräsident&#x27;, &#x27;Kommission&#x27;, &#x27;benö@@&#x27;, &#x27;Ihre&#x27;, &#x27;gelesen&#x27;, &#x27;es&#x27;, &#x27;Insgesamt&#x27;, &#x27;Kollegin&#x27;, &#x27;Wo&#x27;, &#x27;PT&#x27;, &#x27;ermu@@&#x27;, &#x27;diesmal&#x27;, &#x27;Et@@&#x27;, &#x27;ansprechen&#x27;, &#x27;Glückwunsch&#x27;, &#x27;Leid&#x27;, &#x27;drücke&#x27;, &#x27;Ob&#x27;, &#x27;Zweifel@@&#x27;, &#x27;Lange&#x27;, &#x27;Man&#x27;, &#x27;zeige&#x27;, &#x27;gemeldet&#x27;, &#x27;plane&#x27;, &#x27;Redezeit&#x27;, &#x27;dieser&#x27;, &#x27;Plenar@@&#x27;, &#x27;üs@@&#x27;, &#x27;Genau&#x27;, &#x27;möge&#x27;, &#x27;spür@@&#x27;, &#x27;tim@@&#x27;, &#x27;zurückkommen&#x27;, &#x27;nu@@&#x27;, &#x27;fragte&#x27;, &#x27;Dür@@&#x27;, &#x27;Gedanken&#x27;, &#x27;dear&#x27;, &#x27;All&#x27;, &#x27;Stellungnahme&#x27;, &#x27;Umweltfragen&#x27;, &#x27;Verfasser&#x27;, &#x27;Überlegungen&#x27;, &#x27;nun&#x27;, &#x27;zol@@&#x27;, &#x27;Vorsitz&#x27;, &#x27;Schattenberichterstat@@&#x27;, &#x27;geäußer@@&#x27;, &#x27;anschließen&#x27;, &#x27;blicke&#x27;, &#x27;selber&#x27;, &#x27;Abschluß&#x27;, &#x27;stimmt&#x27;, &#x27;protesti@@&#x27;, &#x27;tut&#x27;, &#x27;Schluß&#x27;, &#x27;Bedauern&#x27;, &#x27;Parlaments&#x27;, &#x27;keines@@&#x27;, &#x27;Geschäftsordnung&#x27;, &#x27;wü@@&#x27;, &#x27;Entschuldigung&#x27;, &#x27;Parlament&#x27;, &#x27;nach&#x27;, &#x27;Änderungsanträge&#x27;, &#x27;aufgreifen&#x27;, &#x27;jedoch&#x27;, &#x27;führ@@&#x27;, &#x27;Viertens&#x27;, &#x27;Glück@@&#x27;, &#x27;hierzu&#x27;, &#x27;Plen@@&#x27;, &#x27;Angelegenheit&#x27;, &#x27;Empfehlung&#x27;, &#x27;Anbetracht&#x27;, &#x27;Befür@@&#x27;, &#x27;Präsidentschaft&#x27;, &#x27;festhalten&#x27;, &#x27;chmal&#x27;, &#x27;läßlich&#x27;, &#x27;Schon&#x27;, &#x27;ihn&#x27;, &#x27;Gegen&#x27;, &#x27;Nachdem&#x27;, &#x27;insofern&#x27;, &#x27;Feststellung&#x27;, &#x27;Freude&#x27;, &#x27;Befürwor@@&#x27;, &#x27;guilty&#x27;, &#x27;Möglicherweise&#x27;, &#x27;recht&#x27;, &#x27;schick@@&#x27;, &#x27;zufügen&#x27;, &#x27;allererst&#x27;, &#x27;Beitrag&#x27;, &#x27;vorgetragen&#x27;, &#x27;Mulder&#x27;, &#x27;Sache&#x27;, &#x27;We&#x27;, &#x27;talking&#x27;, &#x27;proo@@&#x27;, &#x27;fing&#x27;, &#x27;Anwesenheit&#x27;, &#x27;gave&#x27;, &#x27;think&#x27;, &#x27;receiving&#x27;, &#x27;downloaded&#x27;, &#x27;ordered&#x27;, &#x27;bought&#x27;, &#x27;Some&#x27;, &#x27;Manchmal&#x27;, &#x27;When&#x27;, &#x27;Beauti@@&#x27;, &#x27;ebenso&#x27;, &#x27;received&#x27;, &#x27;This&#x27;, &#x27;Thank&#x27;, &#x27;nice&#x27;, &#x27;really&#x27;, &#x27;gladly&#x27;, &#x27;ouldn&#x27;, &#x27;che&#x27;, &#x27;After&#x27;, &#x27;irgendwie&#x27;, &#x27;seems&#x27;, &#x27;It&#x27;, &#x27;allo&#x27;, &#x27;Which&#x27;, &#x27;Ebenfalls&#x27;, &#x27;kümm@@&#x27;, &#x27;remembered&#x27;, &#x27;The&#x27;, &#x27;Gute&#x27;, &#x27;someone&#x27;, &#x27;probably&#x27;, &#x27;Glei@@&#x27;, &#x27;My&#x27;, &#x27;promptly&#x27;, &#x27;How&#x27;, &#x27;always&#x27;, &#x27;love&#x27;, &#x27;Will&#x27;, &#x27;ately&#x27;, &#x27;dafür&#x27;, &#x27;Everything&#x27;, &#x27;Just&#x27;, &#x27;Maybe&#x27;, &#x27;actually&#x27;, &#x27;caught&#x27;, &#x27;father&#x27;, &#x27;Hi&#x27;, &#x27;ICH&#x27;, &#x27;toll&#x27;, &#x27;worse&#x27;, &#x27;le&#x27;, &#x27;Anyone&#x27;, &#x27;just&#x27;, &#x27;supposed&#x27;, &#x27;since&#x27;, &#x27;Obviously&#x27;, &#x27;wasn&#x27;, &#x27;Then&#x27;, &#x27;ese&#x27;, &#x27;Why&#x27;, &#x27;myself&#x27;, &#x27;Well&#x27;, &#x27;wrote&#x27;, &#x27;knew&#x27;, &#x27;Have&#x27;, &#x27;me&#x27;, &#x27;gerade&#x27;, &#x27;But&#x27;, &#x27;Any&#x27;, &#x27;went&#x27;, &#x27;nennen&#x27;, &#x27;could&#x27;, &#x27;Thanks&#x27;, &#x27;yesterday&#x27;, &#x27;There&#x27;, &#x27;Vergan@@&#x27;, &#x27;Good&#x27;, &#x27;tried&#x27;, &#x27;shame&#x27;, &#x27;stayed&#x27;, &#x27;You&#x27;, &#x27;Nice&#x27;, &#x27;happened&#x27;, &#x27;denn&#x27;, &#x27;Aussagen&#x27;, &#x27;While&#x27;, &#x27;have&#x27;, &#x27;Our&#x27;, &#x27;agree&#x27;, &#x27;besorgt&#x27;, &#x27;sah&#x27;, &#x27;my&#x27;, &#x27;importantly&#x27;, &#x27;bedanken&#x27;, &#x27;angesprochenen&#x27;, &#x27;beschäf@@&#x27;, &#x27;Muss&#x27;, &#x27;enttäuscht&#x27;, &#x27;Help&#x27;, &#x27;gestern&#x27;, &#x27;richtig&#x27;, &#x27;Behauptung&#x27;, &#x27;Hohe&#x27;, &#x27;bekam&#x27;, &#x27;Folgen@@&#x27;, &#x27;Glücklicherweise&#x27;, &#x27;I.&#x27;, &#x27;trotzdem&#x27;, &#x27;Nehmen&#x27;, &#x27;Schließ@@&#x27;, &#x27;hope&#x27;, &#x27;Folglich&#x27;, &#x27;Vorsitzender&#x27;, &#x27;Although&#x27;, &#x27;would&#x27;, &#x27;Not&#x27;, &#x27;very&#x27;, &#x27;euch&#x27;, &#x27;schau@@&#x27;, &#x27;gesprochen&#x27;, &#x27;vorstellen&#x27;, &#x27;beabsichti@@&#x27;, &#x27;mal&#x27;]resumed[&#x27;am&#x27;, &#x27;aufgenommen&#x27;, &#x27;15.00&#x27;, &#x27;wieder@@&#x27;, &#x27;21.@@&#x27;, &#x27;Donnerstag&#x27;, &#x27;unterbrochen&#x27;, &#x27;um&#x27;, &#x27;wieder&#x27;, &#x27;Wiederaufnahme&#x27;, &#x27;00&#x27;, &#x27;fortgesetzt&#x27;, &#x27;wurden&#x27;, &#x27;Uhr&#x27;, &#x27;15.@@&#x27;, &#x27;Aussprache&#x27;, &#x27;15.&#x27;, &#x27;nahm&#x27;, &#x27;resumed&#x27;, &#x27;è&#x27;, &#x27;pres@@&#x27;, &#x27;Freitag&#x27;, &#x27;13.&#x27;, &#x27;September&#x27;, &#x27;2005&#x27;]Friday[&#x27;Freitag&#x27;, &#x27;Am&#x27;, &#x27;am&#x27;, &#x27;Woche&#x27;, &#x27;:&#x27;, &#x27;Wochenende&#x27;, &#x27;bis&#x27;, &#x27;stag&#x27;, &#x27;fre@@&#x27;, &#x27;it@@&#x27;, &#x27;ags&#x27;, &#x27;Freit@@&#x27;, &#x27;ag@@&#x27;, &#x27;s@@&#x27;, &#x27;stattfinden&#x27;, &#x27;unseren&#x27;, &#x27;sitzung&#x27;, &#x27;vergangenen&#x27;, &#x27;Sitzung&#x27;, &#x27;kommenden&#x27;, &#x27;stattfindet&#x27;, &#x27;Sitz@@&#x27;, &#x27;stag@@&#x27;, &#x27;agen&#x27;, &#x27;andt&#x27;, &#x27;ag&#x27;, &#x27;jeweils&#x27;, &#x27;Samstag&#x27;, &#x27;Montag&#x27;, &#x27;vom&#x27;, &#x27;statt&#x27;, &#x27;gangenen&#x27;, &#x27;vor@@&#x27;, &#x27;abend&#x27;, &#x27;mittag&#x27;, &#x27;sam@@&#x27;, &#x27;Wochen@@&#x27;, &#x27;end@@&#x27;, &#x27;zwungen&#x27;, &#x27;Tag&#x27;, &#x27;tagen&#x27;, &#x27;sitz@@&#x27;, &#x27;Zum&#x27;, &#x27;tage&#x27;, &#x27;Sitzungen&#x27;, &#x27;Protokoll&#x27;, &#x27;aufgenommen&#x27;, &#x27;anwesend&#x27;, &#x27;itage&#x27;, &#x27;Der&#x27;, &#x27;Friday&#x27;, &#x27;täglich&#x27;, &#x27;findet&#x27;, &#x27;Sowohl&#x27;, &#x27;Juli&#x27;, &#x27;True&#x27;, &#x27;24.@@&#x27;, &#x27;7.@@&#x27;, &#x27;ven@@&#x27;, &#x27;dre@@&#x27;, &#x27;21.@@&#x27;, &#x27;Jeden&#x27;, &#x27;Uhr&#x27;, &#x27;Donnerstag&#x27;, &#x27;M@@&#x27;, &#x27;tags&#x27;, &#x27;2008&#x27;, &#x27;12.&#x27;, &#x27;2007&#x27;, &#x27;15.00&#x27;, &#x27;mitt@@&#x27;, &#x27;EST&#x27;, &#x27;10&#x27;, &#x27;di&#x27;, &#x27;jeden&#x27;, &#x27;besetzt&#x27;, &#x27;r.&#x27;, &#x27;Mon@@&#x27;, &#x27;März&#x27;, &#x27;Besu@@&#x27;, &#x27;Fei@@&#x27;, &#x27;geöffnet&#x27;, &#x27;tes@@&#x27;, &#x27;serviert&#x27;, &#x27;30&#x27;, &#x27;Januar&#x27;, &#x27;Sunday&#x27;, &#x27;00&#x27;, &#x27;Straßburg&#x27;, &#x27;03&#x27;, &#x27;Kar@@&#x27;, &#x27;09&#x27;, &#x27;pm&#x27;, &#x27;Tuesday&#x27;, &#x27;.00&#x27;, &#x27;mentary&#x27;, &#x27;Dienstag&#x27;, &#x27;26&#x27;, &#x27;ì&#x27;, &#x27;Mittag@@&#x27;, &#x27;Abend&#x27;, &#x27;fuhren&#x27;, &#x27;Shop&#x27;, &#x27;Mittwoch&#x27;, &#x27;zeiten&#x27;, &#x27;morgen&#x27;, &#x27;3&#x27;, &#x27;max&#x27;, &#x27;Sonntag&#x27;, &#x27;13.&#x27;, &#x27;September&#x27;, &#x27;Juni&#x27;, &#x27;Diesel@@&#x27;, &#x27;wertung&#x27;, &#x27;Von&#x27;, &#x27;Bern&#x27;, &#x27;morgens&#x27;]17[&#x27;17.&#x27;, &#x27;17&#x27;, &#x27;vom&#x27;, &#x27;:&#x27;, &#x27;sieb@@&#x27;, &#x27;zehn&#x27;, &#x27;17@@&#x27;, &#x27;;&#x27;, &#x27;@-@&#x27;, &#x27;17.@@&#x27;, &#x27;Sie@@&#x27;, &#x27;bis&#x27;, &#x27;insgesamt&#x27;, &#x27;.-@@&#x27;, &#x27;Jahren&#x27;, &#x27;Nr.&#x27;, &#x27;16.&#x27;, &#x27;19&#x27;, &#x27;am&#x27;, &#x27;jährigen&#x27;, &#x27;(&#x27;, &#x27;)&#x27;, &#x27;9&#x27;, &#x27;1@@&#x27;, &#x27;25&#x27;, &#x27;21&#x27;, &#x27;27&#x27;, &#x27;10&#x27;, &#x27;knapp&#x27;, &#x27;18&#x27;, &#x27;1&#x27;, &#x27;zehn@@&#x27;, &#x27;14&#x27;, &#x27;h&#x27;, &#x27;20&#x27;, &#x27;45&#x27;, &#x27;Uhr&#x27;, &#x27;06&#x27;, &#x27;Erdbeben&#x27;, &#x27;18.&#x27;, &#x27;28&#x27;, &#x27;93&#x27;, &#x27;7&#x27;, &#x27;32&#x27;, &#x27;16&#x27;, &#x27;Februar&#x27;, &#x27;Januar&#x27;, &#x27;15&#x27;, &#x27;33&#x27;, &#x27;37&#x27;, &#x27;09&#x27;, &#x27;00&#x27;, &#x27;30&#x27;, &#x27;11&#x27;, &#x27;Euro&#x27;, &#x27;77&#x27;, &#x27;92&#x27;, &#x27;November&#x27;, &#x27;2@@&#x27;, &#x27;September&#x27;, &#x27;12@@&#x27;, &#x27;112&#x27;, &#x27;12&#x27;, &#x27;41&#x27;, &#x27;2&#x27;, &#x27;31&#x27;, &#x27;23&#x27;, &#x27;6&#x27;, &#x27;08&#x27;, &#x27;22&#x27;, &#x27;35&#x27;, &#x27;24&#x27;, &#x27;um&#x27;, &#x27;44&#x27;, &#x27;5&#x27;, &#x27;07&#x27;, &#x27;Ju@@&#x27;, &#x27;8&#x27;, &#x27;/&#x27;, &#x27;Mai&#x27;, &#x27;2003&#x27;, &#x27;Jahr&#x27;, &#x27;04&#x27;, &#x27;Paris&#x27;, &#x27;v&#x27;, &#x27;last&#x27;, &#x27;statt&#x27;, &#x27;Juni&#x27;, &#x27;3@@&#x27;, &#x27;Alter&#x27;, &#x27;Air&#x27;, &#x27;34&#x27;, &#x27;stehung&#x27;, &#x27;August&#x27;, &#x27;15@@&#x27;, &#x27;term&#x27;, &#x27;Jahres&#x27;, &#x27;nan@@&#x27;, &#x27;Oktober&#x27;, &#x27;18@@&#x27;, &#x27;4@@&#x27;, &#x27;6@@&#x27;, &#x27;Juli&#x27;, &#x27;Dezember&#x27;, &#x27;13&#x27;, &#x27;19@@&#x27;, &#x27;20.&#x27;, &#x27;2006&#x27;, &#x27;47&#x27;, &#x27;blatt&#x27;, &#x27;December&#x27;, &#x27;27@@&#x27;, &#x27;Vereinbarung&#x27;, &#x27;95&#x27;, &#x27;54&#x27;, &#x27;00@@&#x27;, &#x27;03&#x27;, &#x27;9.&#x27;, &#x27;page&#x27;, &#x27;2009&#x27;, &#x27;3&#x27;, &#x27;mbol&#x27;, &#x27;June&#x27;, &#x27;43&#x27;, &#x27;*&#x27;, &#x27;000&#x27;]December[&#x27;Dezember&#x27;, &#x27;vom&#x27;, &#x27;am&#x27;, &#x27;letzten&#x27;, &#x27;Jahres&#x27;, &#x27;Dez@@&#x27;, &#x27;em@@&#x27;, &#x27;ber@@&#x27;, &#x27;12.@@&#x27;, &#x27;vergangenen&#x27;, &#x27;im&#x27;, &#x27;12.&#x27;, &#x27;hat&#x27;, &#x27;bis&#x27;, &#x27;soll&#x27;, &#x27;zum&#x27;, &#x27;1999&#x27;, &#x27;unterzeichnet&#x27;, &#x27;Anfang&#x27;, &#x27;vorgelegt&#x27;, &#x27;hatte&#x27;, &#x27;legte&#x27;, &#x27;wurde&#x27;, &#x27;stattgefunden&#x27;, &#x27;des@@&#x27;, &#x27;Monat&#x27;, &#x27;Einigung&#x27;, &#x27;ember&#x27;, &#x27;Im&#x27;, &#x27;aufgenommen&#x27;, &#x27;ab&#x27;, &#x27;abgeschlossen&#x27;, &#x27;verabschiedeten&#x27;, &#x27;Kraft&#x27;, &#x27;laufen&#x27;, &#x27;2001&#x27;, &#x27;verabschiedet&#x27;, &#x27;Am&#x27;, &#x27;September&#x27;, &#x27;statt&#x27;, &#x27;December&#x27;, &#x27;trat&#x27;, &#x27;nete&#x27;, &#x27;z&#x27;, &#x27;2002&#x27;, &#x27;begrü@@&#x27;, &#x27;kam&#x27;, &#x27;worden&#x27;, &#x27;2000&#x27;, &#x27;De@@&#x27;, &#x27;nahm&#x27;, &#x27;15.&#x27;, &#x27;übergeben&#x27;, &#x27;beschlossen&#x27;, &#x27;tag@@&#x27;, &#x27;angenommen&#x27;, &#x27;vorzulegen&#x27;, &#x27;getreten&#x27;, &#x27;seiner&#x27;, &#x27;Tagung&#x27;, &#x27;waren&#x27;, &#x27;gebilligt&#x27;, &#x27;unterbreit@@&#x27;, &#x27;ete&#x27;, &#x27;stattfand&#x27;, &#x27;stattfindet&#x27;, &#x27;3.&#x27;, &#x27;Staats-&#x27;, &#x27;Regierungschefs&#x27;, &#x27;2003&#x27;, &#x27;erneut&#x27;, &#x27;dieses&#x27;, &#x27;verkün@@&#x27;, &#x27;dete&#x27;, &#x27;2004&#x27;, &#x27;vorlegen&#x27;, &#x27;erreichte&#x27;, &#x27;2005&#x27;, &#x27;Gültigkeit&#x27;, &#x27;5.&#x27;, &#x27;Rates&#x27;, &#x27;2006&#x27;, &#x27;bestätigt&#x27;, &#x27;13.&#x27;, &#x27;2007&#x27;, &#x27;veröffentlicht&#x27;, &#x27;1.&#x27;, &#x27;19.&#x27;, &#x27;31.&#x27;, &#x27;angekün@@&#x27;, &#x27;embers&#x27;, &#x27;angenommene&#x27;, &#x27;unterzeichnen&#x27;, &#x27;kommenden&#x27;, &#x27;mussten&#x27;, &#x27;erfolgte&#x27;, &#x27;Ab&#x27;, &#x27;stattfinden&#x27;, &#x27;Sitzung&#x27;, &#x27;rat&#x27;, &#x27;endete&#x27;, &#x27;2010&#x27;, &#x27;festgelegt&#x27;, &#x27;1995&#x27;, &#x27;1996&#x27;, &#x27;datum&#x27;, &#x27;7.&#x27;, &#x27;gültig&#x27;, &#x27;1997&#x27;, &#x27;Ende&#x27;, &#x27;eingereicht&#x27;, &#x27;8.&#x27;, &#x27;selben&#x27;, &#x27;ECOFIN&#x27;, &#x27;Oktober&#x27;, &#x27;17.&#x27;, &#x27;Monats&#x27;, &#x27;November&#x27;, &#x27;Erklärung&#x27;, &#x27;1998&#x27;, &#x27;Forderungen&#x27;, &#x27;verabschie@@&#x27;, &#x27;spätestens&#x27;, &#x27;Inneres&#x27;, &#x27;konferenz&#x27;, &#x27;Betrieb&#x27;, &#x27;16.&#x27;, &#x27;29.&#x27;, &#x27;23.&#x27;, &#x27;27.&#x27;, &#x27;bre&#x27;, &#x27;ami&#x27;, &#x27;9.&#x27;, &#x27;4.&#x27;, &#x27;ey&#x27;, &#x27;26.&#x27;, &#x27;2.&#x27;, &#x27;hatten&#x27;, &#x27;14.&#x27;, &#x27;10.&#x27;, &#x27;US&#x27;, &#x27;Heinrich&#x27;, &#x27;2008&#x27;, &#x27;beschäf@@&#x27;, &#x27;August&#x27;, &#x27;22.&#x27;, &#x27;erklärte&#x27;, &#x27;April&#x27;, &#x27;stag&#x27;, &#x27;3@@&#x27;, &#x27;bunal&#x27;, &#x27;nachdem&#x27;, &#x27;Kriegs@@&#x27;, &#x27;seit&#x27;, &#x27;2012&#x27;, &#x27;igte&#x27;, &#x27;2009&#x27;, &#x27;race&#x27;, &#x27;Weihnachts@@&#x27;, &#x27;Verlei@@&#x27;, &#x27;hung&#x27;, &#x27;1.@@&#x27;, &#x27;Monaten&#x27;, &#x27;20.&#x27;, &#x27;Dé@@&#x27;, &#x27;geleistet&#x27;, &#x27;Weihnachten&#x27;, &#x27;Januar&#x27;, &#x27;25.&#x27;, &#x27;Februar&#x27;, &#x27;Bis&#x27;, &#x27;Vom&#x27;, &#x27;18.&#x27;, &#x27;Der&#x27;, &#x27;abgehalten&#x27;, &#x27;01&#x27;, &#x27;12&#x27;, &#x27;Vorbereitungen&#x27;, &#x27;farbe&#x27;, &#x27;fassten&#x27;, &#x27;stimm@@&#x27;] 德语单词的对齐样例 12Ich[&#x27;I&#x27;, &#x27;It&#x27;, &#x27;like&#x27;, &#x27;Madam&#x27;, &#x27;My&#x27;, &#x27;saying&#x27;, &#x27;should&#x27;, &#x27;In&#x27;, &#x27;As&#x27;, &#x27;But&#x27;, &#x27;would&#x27;, &#x27;want&#x27;, &#x27;Let&#x27;, &#x27;wish&#x27;, &#x27;At&#x27;, &#x27;shall&#x27;, &#x27;There&#x27;, &#x27;am&#x27;, &#x27;This&#x27;, &#x27;On&#x27;, &#x27;see&#x27;, &#x27;Having&#x27;, &#x27;Commissioner&#x27;, &#x27;will&#x27;, &#x27;that&#x27;, &#x27;What&#x27;, &#x27;me&#x27;, &#x27;do&#x27;, &#x27;have&#x27;, &#x27;Like&#x27;, &#x27;honourable&#x27;, &#x27;Well&#x27;, &#x27;Allow&#x27;, &#x27;agree&#x27;, &#x27;myself&#x27;, &#x27;When&#x27;, &#x27;So&#x27;, &#x27;And&#x27;, &#x27;hope&#x27;, &#x27;also&#x27;, &#x27;believe&#x27;, &#x27;think&#x27;, &#x27;For&#x27;, &#x27;my&#x27;, &#x27;SV&#x27;, &#x27;Mr&#x27;, &#x27;President&#x27;, &#x27;May&#x27;, &#x27;note&#x27;, &#x27;sympathy&#x27;, &#x27;urge&#x27;, &#x27;DE&#x27;, &#x27;However&#x27;, &#x27;With&#x27;, &#x27;wholeheartedly&#x27;, &#x27;Indeed&#x27;, &#x27;remind&#x27;, &#x27;welcome&#x27;, &#x27;To&#x27;, &#x27;commend&#x27;, &#x27;Although&#x27;, &#x27;Moreover&#x27;, &#x27;wonder&#x27;, &#x27;(&#x27;, &#x27;FR&#x27;, &#x27;mention&#x27;, &#x27;gentlemen&#x27;, &#x27;PT&#x27;, &#x27;Without&#x27;, &#x27;Perhaps&#x27;, &#x27;All&#x27;, &#x27;Ladies&#x27;, &#x27;Can&#x27;, &#x27;Please&#x27;, &#x27;give&#x27;, &#x27;Here&#x27;, &#x27;must&#x27;, &#x27;Could&#x27;, &#x27;say&#x27;, &#x27;Together&#x27;, &#x27;Now&#x27;, &#x27;Yet&#x27;, &#x27;While&#x27;, &#x27;feel&#x27;, &#x27;If&#x27;, &#x27;suggest&#x27;, &#x27;Just&#x27;, &#x27;ladies&#x27;, &#x27;Further&#x27;, &#x27;certainly&#x27;, &#x27;That&#x27;, &#x27;Certainly&#x27;, &#x27;Spe@@&#x27;, &#x27;aking&#x27;, &#x27;Next&#x27;, &#x27;personally&#x27;, &#x27;accept&#x27;, &#x27;really&#x27;, &#x27;thoughts&#x27;, &#x27;understand&#x27;, &#x27;refer&#x27;, &#x27;Yes&#x27;, &#x27;Instead&#x27;, &#x27;therefore&#x27;, &#x27;Another&#x27;, &#x27;just&#x27;, &#x27;repeat&#x27;, &#x27;speak&#x27;, &#x27;favour&#x27;, &#x27;very&#x27;, &#x27;One&#x27;, &#x27;strongly&#x27;, &#x27;By&#x27;, &#x27;NL&#x27;, &#x27;Along&#x27;, &#x27;Furthermore&#x27;, &#x27;appreciate&#x27;, &#x27;Will&#x27;, &#x27;applau@@&#x27;, &#x27;know&#x27;, &#x27;hear&#x27;, &#x27;regret&#x27;, &#x27;Whilst&#x27;, &#x27;argue&#x27;, &#x27;Finally&#x27;, &#x27;suspect&#x27;, &#x27;Firstly&#x27;, &#x27;Of&#x27;, &#x27;emphasise&#x27;, &#x27;Once&#x27;, &#x27;support&#x27;, &#x27;grateful&#x27;, &#x27;call&#x27;, &#x27;Following&#x27;, &#x27;sincerely&#x27;, &#x27;Person@@&#x27;, &#x27;Naturally&#x27;, &#x27;Tur@@&#x27;, &#x27;Mrs&#x27;, &#x27;point&#x27;, &#x27;From&#x27;, &#x27;Thank&#x27;, &#x27;Since&#x27;, &#x27;Be&#x27;, &#x27;Quite&#x27;, &#x27;opinion&#x27;, &#x27;ally&#x27;, &#x27;fact&#x27;, &#x27;gratul@@&#x27;, &#x27;Very&#x27;, &#x27;tell&#x27;, &#x27;cannot&#x27;, &#x27;ask&#x27;, &#x27;Neither&#x27;, &#x27;pleased&#x27;, &#x27;Above&#x27;, &#x27;conclude&#x27;, &#x27;endorse&#x27;, &#x27;Therefore&#x27;, &#x27;Nevertheless&#x27;, &#x27;Some&#x27;, &#x27;listened&#x27;, &#x27;thank&#x27;, &#x27;Rather&#x27;, &#x27;delighted&#x27;, &#x27;acknowledge&#x27;, &#x27;recognise&#x27;, &#x27;quote&#x27;, &#x27;share&#x27;, &#x27;firmly&#x27;, &#x27;trust&#x27;, &#x27;Lastly&#x27;, &#x27;owe&#x27;, &#x27;Regarding&#x27;, &#x27;Unlike&#x27;, &#x27;No&#x27;, &#x27;reiterate&#x27;, &#x27;look&#x27;, &#x27;imagine&#x27;, &#x27;writing&#x27;, &#x27;happy&#x27;, &#x27;glad&#x27;, &#x27;wanted&#x27;, &#x27;invite&#x27;, &#x27;Where&#x27;, &#x27;referring&#x27;, &#x27;Any@@&#x27;, &#x27;ij&#x27;, &#x27;Consequently&#x27;, &#x27;express&#x27;, &#x27;Not&#x27;, &#x27;pay&#x27;, &#x27;draw&#x27;, &#x27;Being&#x27;, &#x27;Again&#x27;, &#x27;declare&#x27;, &#x27;After&#x27;, &#x27;recall&#x27;, &#x27;Then&#x27;, &#x27;sal@@&#x27;, &#x27;Today&#x27;, &#x27;gladly&#x27;, &#x27;Would&#x27;, &#x27;reply&#x27;, &#x27;Nor&#x27;, &#x27;Do&#x27;, &#x27;thinking&#x27;, &#x27;confirm&#x27;, &#x27;stress&#x27;, &#x27;IT&#x27;, &#x27;said&#x27;, &#x27;apolog@@&#x27;, &#x27;explain&#x27;, &#x27;genuinely&#x27;, &#x27;Taking&#x27;, &#x27;First&#x27;, &#x27;Nonetheless&#x27;, &#x27;sorry&#x27;, &#x27;attention&#x27;, &#x27;voted&#x27;, &#x27;hereby&#x27;, &#x27;Even&#x27;, &#x27;Surely&#x27;, &#x27;hear@@&#x27;, &#x27;How&#x27;, &#x27;comments&#x27;, &#x27;.-&#x27;, &#x27;Ich&#x27;, &#x27;During&#x27;, &#x27;convinced&#x27;, &#x27;pointing&#x27;, &#x27;My@@&#x27;, &#x27;Commission&#x27;, &#x27;this&#x27;, &#x27;&amp;apos;m&#x27;, &#x27;simply&#x27;, &#x27;Obviously&#x27;, &#x27;realise&#x27;, &#x27;Thanks&#x27;, &#x27;behalf&#x27;, &#x27;objection&#x27;, &#x27;assure&#x27;, &#x27;emph@@&#x27;, &#x27;dare&#x27;, &#x27;Wallström&#x27;, &#x27;seems&#x27;, &#x27;Defin@@&#x27;, &#x27;SK&#x27;, &#x27;PL&#x27;, &#x27;ALDE&#x27;, &#x27;EL&#x27;, &#x27;Member&#x27;, &#x27;rapporteur&#x27;, &#x27;Verts&#x27;, &#x27;inform&#x27;, &#x27;HU&#x27;, &#x27;approve&#x27;, &#x27;ES&#x27;, &#x27;fully&#x27;, &#x27;intend&#x27;, &#x27;to&#x27;, &#x27;Have&#x27;, &#x27;honest@@&#x27;, &#x27;depl@@&#x27;, &#x27;convey&#x27;, &#x27;Me&#x27;, &#x27;congratulate&#x27;, &#x27;abstained&#x27;, &#x27;Hence&#x27;, &#x27;More&#x27;, &#x27;add&#x27;, &#x27;Frank@@&#x27;, &#x27;emphasize&#x27;, &#x27;Take&#x27;, &#x27;comment&#x27;, &#x27;I.&#x27;, &#x27;disagree&#x27;, &#x27;consider&#x27;, &#x27;conf@@&#x27;, &#x27;Amendment&#x27;, &#x27;Did&#x27;, &#x27;congratulations&#x27;, &#x27;speaking&#x27;, &#x27;echo&#x27;, &#x27;prefer&#x27;, &#x27;She&#x27;, &#x27;Hon@@&#x27;, &#x27;&amp;apos;d&#x27;, &#x27;)&#x27;, &#x27;don&#x27;, &#x27;ego&#x27;, &#x27;&amp;apos;ve&#x27;, &#x27;Und&#x27;, &#x27;Happ@@&#x27;, &#x27;Excellent&#x27;, &#x27;thought&#x27;, &#x27;Wir&#x27;, &#x27;Hi&#x27;, &#x27;Kann&#x27;, &#x27;Hop@@&#x27;, &#x27;Je&#x27;, &#x27;propose&#x27;, &#x27;Sor@@&#x27;, &#x27;&amp;apos;ll&#x27;, &#x27;keen&#x27;, &#x27;Danke&#x27;, &#x27;Still&#x27;, &#x27;sehe&#x27;]erkläre[&#x27;declare&#x27;, &#x27;resumed&#x27;, &#x27;session&#x27;, &#x27;explain&#x27;, &#x27;ad@@&#x27;, &#x27;state&#x27;, &#x27;say&#x27;, &#x27;announce&#x27;, &#x27;confirm&#x27;, &#x27;hereby&#x27;, &#x27;open&#x27;, &#x27;shall&#x27;, &#x27;declar@@&#x27;, &#x27;I&#x27;, &#x27;jour@@&#x27;, &#x27;ned&#x27;, &#x27;inge&#x27;, &#x27;closed&#x27;, &#x27;affir@@&#x27;, &#x27;concluded&#x27;, &#x27;agree&#x27;, &#x27;explains&#x27;, &#x27;the&#x27;, &#x27;196&#x27;, &#x27;03&#x27;] 接下来，我有如下的几个发现: fast align 是可以建模出正确的对齐的I→Ich erkläre→declare ...; fast align 在大规模数据集上制造了太多的不正确对齐关系I→nice really gladly ...。 1.3 个人想法总的来说，fast align 能够起到对齐的作用，但它的对齐准确率显然不高。所以，我觉得如果只是那 fast align 去指导一个完全没有中间结构来辅助的翻译模型的话，该模型的翻译质量能够得到有效的提升；但是，如果想在 NAT 方面促成一个 SOTA 成绩的非自回归翻译模型的话，恐怕是心有余而力不足了。 2 GIZA++2.1 检查词对齐文本数据从之前展示的一些文件内容来看，我发现 一些文件en2de.A3.final en2de.n3.final ...的含义明显要比 fast align 所生成的对齐文件内容要丰富很多，如en2de.n3.final文件可以更好地观察单词的繁殖力、en2de.A3.final文件可以更好地观察整个句子的对齐情况； 虽然随着迭代次数的增加, 我们可以发现en2de.perp对齐的困惑度有在减小，但是最终的困惑度仍然很高(≈90)，而且我们还可以发现隐马尔可夫模型的困惑度是最低的，而不是最终迭代的IBM Model 4； 有些文件en2de.d3.final en2de.a3.final ...的内容对于新手而言根本看不懂是什么意思，过于晦涩； en2de.A3.final文件中I (&#123; 1 &#125;) declare (&#123; 2 &#125;)说明模型能够建模正确的对齐关系(‘I’对齐’Ich’, ‘declare’对齐’erkläre’)，NULL (&#123; 3 6 7 15 17 23 25 27 &#125;) the (&#123; &#125;)说明模型还是会有漏掉对齐的情况存在(‘die’表示’这’，’the’应该是对齐’die’的)，I (&#123; 1 &#125;) would (&#123; &#125;)说明模型存在错误对齐的情况(‘Könnten’可以与’would’对齐，但是模型却将’I’对齐到’Könnten’)； 2.2 个人想法GIZA++的对齐能力明显优于fast align，但这也意味着更多的时间开销，并且GIZA++的对齐中也有不正确的情况存在。总的来说，如果用GIZA++就需要在获取对齐时花费更多的时间，但是能够在性能上取得一定的提升。","categories":[{"name":"Statistical Machine Translation","slug":"Statistical-Machine-Translation","permalink":"http://example.com/categories/Statistical-Machine-Translation/"}],"tags":[{"name":"Alignment","slug":"Alignment","permalink":"http://example.com/tags/Alignment/"}]}],"categories":[{"name":"Automated Essay Scoring","slug":"Automated-Essay-Scoring","permalink":"http://example.com/categories/Automated-Essay-Scoring/"},{"name":"Combinatorial Mathematices","slug":"Combinatorial-Mathematices","permalink":"http://example.com/categories/Combinatorial-Mathematices/"},{"name":"Non-Autoregressive Translation","slug":"Non-Autoregressive-Translation","permalink":"http://example.com/categories/Non-Autoregressive-Translation/"},{"name":"Neural Machine Translation","slug":"Neural-Machine-Translation","permalink":"http://example.com/categories/Neural-Machine-Translation/"},{"name":"Speech Recognition","slug":"Speech-Recognition","permalink":"http://example.com/categories/Speech-Recognition/"},{"name":"Statistical Machine Translation","slug":"Statistical-Machine-Translation","permalink":"http://example.com/categories/Statistical-Machine-Translation/"}],"tags":[{"name":"AES","slug":"AES","permalink":"http://example.com/tags/AES/"},{"name":"Combinatorial Mathematices","slug":"Combinatorial-Mathematices","permalink":"http://example.com/tags/Combinatorial-Mathematices/"},{"name":"NAT","slug":"NAT","permalink":"http://example.com/tags/NAT/"},{"name":"NMT","slug":"NMT","permalink":"http://example.com/tags/NMT/"},{"name":"CTC","slug":"CTC","permalink":"http://example.com/tags/CTC/"},{"name":"Alignment","slug":"Alignment","permalink":"http://example.com/tags/Alignment/"}]}